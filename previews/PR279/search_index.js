var documenterSearchIndex = {"docs":
[{"location":"creating_custom_plots/#Creating-custom-plots","page":"Creating custom plots","title":"Creating custom plots","text":"","category":"section"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"While ArviZ includes many plotting functions for visualizing the data stored in InferenceData objects, you will often need to construct custom plots, or you may want to tweak some of our plots in your favorite plotting package.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"In this tutorial, we will show you a few useful techniques you can use to construct these plots using Julia’s plotting packages. For demonstration purposes, we’ll use Makie.jl and AlgebraOfGraphics.jl, which can consume Dataset objects since they implement the Tables interface. However, we could just as easily have used StatsPlots.jl.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"using ArviZ, DimensionalData, DataFrames, Statistics, AlgebraOfGraphics, CairoMakie\nusing AlgebraOfGraphics: density\nset_aog_theme!()","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"/home/runner/.julia/conda/3/x86_64/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"We’ll start by loading some draws from an implementation of the non-centered parameterization of the 8 schools model. In this parameterization, the model has some sampling issues.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"idata = load_example_data(\"centered_eight\")","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"<div>InferenceData<details>\n<summary>posterior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n&#10;with metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"</code></pre>\n</details>\n<details>\n<summary>posterior_predictive</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n&#10;with metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"</code></pre>\n</details>\n<details>\n<summary>log_likelihood</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n&#10;with metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"</code></pre>\n</details>\n<details>\n<summary>sample_stats</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (500×4)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n&#10;with metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"</code></pre>\n</details>\n<details>\n<summary>prior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n&#10;with metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"</code></pre>\n</details>\n<details>\n<summary>prior_predictive</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n&#10;with metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"</code></pre>\n</details>\n<details>\n<summary>observed_data</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n&#10;with metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"</code></pre>\n</details>\n<details>\n<summary>constant_data</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n&#10;with metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"</code></pre>\n</details>\n</div>","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"idata.posterior","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\"","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"The plotting functions we’ll be using interact with a tabular view of a Dataset. Let’s see what that view looks like for a Dataset:","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"df = DataFrame(idata.posterior)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"The tabular view includes dimensions and variables as columns.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"When variables with different dimensions are flattened into a tabular form, there’s always some duplication of values. As a simple case, note that chain, draw, and school all have repeated values in the above table.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"In this case, theta has the school dimension, but tau doesn’t, so the values of tau will be repeated in the table for each value of school.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"df[df.school .== Ref(\"Choate\"), :].tau == df[df.school .== Ref(\"Deerfield\"), :].tau","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"true","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"In our first example, this will be important.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"Here, let’s construct a trace plot. Besides idata, all functions and types in the following cell are defined in AlgebraOfGraphics or Makie:","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"data(...) indicates that the wrapped object implements the Tables interface\nmapping indicates how the data should be used. The symbols are all column names in the table, which for us are our variable names and dimensions.\nvisual specifies how the data should be converted to a plot.\nLines is a plot type defined in Makie.\ndraw takes this combination and plots it.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"draw(\n    data(idata.posterior.mu) *\n    mapping(:draw, :mu; color=:chain => nonnumeric) *\n    visual(Lines; alpha=0.8),\n)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"(Image: )","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"Note the line idata.posterior.mu. If we had just used idata.posterior, the plot would have looked more-or-less the same, but there would be artifacts due to mu being copied many times. By selecting mu directly, all other dimensions are discarded, so each value of mu appears in the plot exactly once.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"When examining an MCMC trace plot, we want to see a “fuzzy caterpillar”. Instead we see a few places where the Markov chains froze. We can do the same for theta as well, but it’s more useful here to separate these draws by school.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"draw(\n    data(idata.posterior) *\n    mapping(:draw, :theta; layout=:school, color=:chain => nonnumeric) *\n    visual(Lines; alpha=0.8),\n)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"(Image: )","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"Suppose we want to compare tau with theta for two different schools. To do so, we use InferenceDatas indexing syntax to subset the data.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"draw(\n    data(idata[:posterior, school=At([\"Choate\", \"Deerfield\"])]) *\n    mapping(:theta, :tau; color=:school) *\n    density() *\n    visual(Contour; levels=10),\n)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"┌ Warning: (Dim{:school},) dims were not found in object\n└ @ DimensionalData.Dimensions ~/.julia/packages/DimensionalData/pS9IE/src/Dimensions/primitives.jl:659\n┌ Warning: (Dim{:school},) dims were not found in object\n└ @ DimensionalData.Dimensions ~/.julia/packages/DimensionalData/pS9IE/src/Dimensions/primitives.jl:659","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"(Image: )","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"We can also compare the density plots constructed from each chain for different schools.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"draw(\n    data(idata.posterior) *\n    mapping(:theta; layout=:school, color=:chain => nonnumeric) *\n    density(),\n)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"(Image: )","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"If we want to compare many schools in a single plot, an ECDF plot is more convenient.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"draw(\n    data(idata.posterior) * mapping(:theta; color=:school => nonnumeric) * visual(ECDFPlot);\n    axis=(; ylabel=\"probability\"),\n)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"(Image: )","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"So far we’ve just plotted data from one group, but we often want to combine data from multiple groups in one plot. The simplest way to do this is to create the plot out of multiple layers. Here we use this approach to plot the observations over the posterior predictive distribution.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"draw(\n    (data(idata.posterior_predictive) * mapping(:obs; layout=:school) * density()) +\n    (data(idata.observed_data) * mapping(:obs, :obs => zero => \"\"; layout=:school)),\n)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"(Image: )","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"Another option is to combine the groups into a single dataset.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"Here we compare the prior and posterior. Since the prior has 1 chain and the posterior has 4 chains, if we were to combine them into a table, the structure would need to be ragged. This is not currently supported.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"We can then either plot the two distributions separately as we did before, or we can compare a single chain from each group. This is what we’ll do here. To concatenate the two groups, we introduce a new named dimension using DimensionalData.Dim.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"draw(\n    data(\n        cat(\n            idata.posterior[chain=[1]], idata.prior; dims=Dim{:group}([:posterior, :prior])\n        )[:mu],\n    ) *\n    mapping(:mu; color=:group) *\n    histogram(; bins=20) *\n    visual(; alpha=0.8);\n    axis=(; ylabel=\"probability\"),\n)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"(Image: )","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"From the trace plots, we suspected the geometry of this posterior was bad. Let’s highlight divergent transitions. To do so, we merge posterior and samplestats, which can do with merge since they share no common variable names.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"draw(\n    data(merge(idata.posterior, idata.sample_stats)) * mapping(\n        :theta,\n        :tau;\n        layout=:school,\n        color=:diverging,\n        markersize=:diverging => (d -> d ? 5 : 2),\n    ),\n)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"(Image: )","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"When we try building more complex plots, we may need to build new Datasets from our existing ones.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"One example of this is the corner plot. To build this plot, we need to make a copy of theta with a copy of the school dimension.","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"theta = idata.posterior.theta[school=1:4]\ntheta2 = rebuild(set(theta; school=:school2); name=:theta2)\nplot_data = Dataset(theta, theta2, idata.sample_stats.diverging)\ndraw(\n    data(plot_data) * mapping(\n        :theta,\n        :theta2 => \"theta\";\n        col=:school,\n        row=:school2,\n        color=:diverging,\n        markersize=:diverging => (d -> d ? 3 : 1),\n    );\n    figure=(; figsize=(5, 5)),\n    axis=(; aspect=1),\n)","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"(Image: )","category":"page"},{"location":"creating_custom_plots/#Environment","page":"Creating custom plots","title":"Environment","text":"","category":"section"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"using Pkg\nPkg.status()","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"Status `~/work/ArviZ.jl/ArviZ.jl/docs/Project.toml`\n  [cbdf2221] AlgebraOfGraphics v0.6.16\n  [131c737c] ArviZ v0.9.0-DEV `~/work/ArviZ.jl/ArviZ.jl`\n  [13f3f980] CairoMakie v0.10.6\n  [992eb4ea] CondaPkg v0.2.18\n  [a93c6f00] DataFrames v1.5.0\n  [0703355e] DimensionalData v0.24.12\n  [31c24e10] Distributions v0.25.98\n  [e30172f5] Documenter v0.27.25\n⌅ [f6006082] EvoTrees v0.14.11\n  [7073ff75] IJulia v1.24.2\n  [c7f686f2] MCMCChains v6.0.3\n  [be115224] MCMCDiagnosticTools v0.3.4\n  [a7f614a8] MLJBase v0.21.11\n  [614be32b] MLJIteration v0.5.1\n  [438e738f] PyCall v1.96.1\n  [d330b81b] PyPlot v2.11.1\n  [754583d1] SampleChains v0.5.1\n  [c1514b29] StanSample v7.4.1\n⌅ [fce5fe82] Turing v0.24.4\n  [f43a241f] Downloads v1.6.0\n  [37e2e46d] LinearAlgebra\n  [10745b16] Statistics v1.9.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"versioninfo()","category":"page"},{"location":"creating_custom_plots/","page":"Creating custom plots","title":"Creating custom plots","text":"Julia Version 1.9.2\nCommit e4ee485e909 (2023-07-05 09:39 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 2 × Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, skylake-avx512)\n  Threads: 3 on 2 virtual cores\nEnvironment:\n  JULIA_CMDSTAN_HOME = /home/runner/work/ArviZ.jl/ArviZ.jl/.cmdstan//cmdstan-2.25.0/\n  JULIA_IMAGE_THREADS = 1\n  JULIA_NUM_THREADS = 2","category":"page"},{"location":"api/dataset/#dataset-api","page":"Dataset","title":"Dataset","text":"","category":"section"},{"location":"api/dataset/","page":"Dataset","title":"Dataset","text":"Pages = [\"dataset.md\"]","category":"page"},{"location":"api/dataset/#Type-definition","page":"Dataset","title":"Type definition","text":"","category":"section"},{"location":"api/dataset/","page":"Dataset","title":"Dataset","text":"Dataset","category":"page"},{"location":"api/dataset/#InferenceObjects.Dataset","page":"Dataset","title":"InferenceObjects.Dataset","text":"Dataset{L} <: DimensionalData.AbstractDimStack{L}\n\nContainer of dimensional arrays sharing some dimensions.\n\nThis type is an DimensionalData.AbstractDimStack that implements the same interface as DimensionalData.DimStack and has identical usage.\n\nWhen a Dataset is passed to Python, it is converted to an xarray.Dataset without copying the data. That is, the Python object shares the same memory as the Julia object. However, if an xarray.Dataset is passed to Julia, its data must be copied.\n\nConstructors\n\nDataset(data::DimensionalData.AbstractDimArray...)\nDataset(data::Tuple{Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(data::NamedTuple{Keys,Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(\n    data::NamedTuple,\n    dims::Tuple{Vararg{DimensionalData.Dimension}};\n    metadata=DimensionalData.NoMetadata(),\n)\n\nIn most cases, use convert_to_dataset to create a Dataset instead of directly using a constructor.\n\n\n\n\n\n","category":"type"},{"location":"api/dataset/#General-conversion","page":"Dataset","title":"General conversion","text":"","category":"section"},{"location":"api/dataset/","page":"Dataset","title":"Dataset","text":"convert_to_dataset\nnamedtuple_to_dataset","category":"page"},{"location":"api/dataset/#InferenceObjects.convert_to_dataset","page":"Dataset","title":"InferenceObjects.convert_to_dataset","text":"convert_to_dataset(obj; group = :posterior, kwargs...) -> Dataset\n\nConvert a supported object to a Dataset.\n\nIn most cases, this function calls convert_to_inference_data and returns the corresponding group.\n\n\n\n\n\n","category":"function"},{"location":"api/dataset/#InferenceObjects.namedtuple_to_dataset","page":"Dataset","title":"InferenceObjects.namedtuple_to_dataset","text":"namedtuple_to_dataset(data; kwargs...) -> Dataset\n\nConvert NamedTuple mapping variable names to arrays to a Dataset.\n\nAny non-array values will be converted to a 0-dimensional array.\n\nKeywords\n\nattrs::AbstractDict{<:AbstractString}: a collection of metadata to attach to the dataset, in addition to defaults. Values should be JSON serializable.\nlibrary::Union{String,Module}: library used for performing inference. Will be attached to the attrs metadata.\ndims: a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are:\nSymbol: dimension name\nType{<:DimensionsionalData.Dimension}: dimension type\nDimensionsionalData.Dimension: dimension, potentially with indices\nNothing: no dimension name provided, dimension name is automatically generated\ncoords: a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in dims are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated.\n\n\n\n\n\n","category":"function"},{"location":"api/dataset/#DimensionalData","page":"Dataset","title":"DimensionalData","text":"","category":"section"},{"location":"api/dataset/","page":"Dataset","title":"Dataset","text":"As a DimensionalData.AbstractDimStack, Dataset also implements the AbstractDimStack API and can be used like a DimStack. See DimensionalData's documentation for example usage.","category":"page"},{"location":"api/dataset/#Tables-inteface","page":"Dataset","title":"Tables inteface","text":"","category":"section"},{"location":"api/dataset/","page":"Dataset","title":"Dataset","text":"Dataset implements the Tables interface. This allows Datasets to be used as sources for any function that can accept a table. For example, it's straightforward to:","category":"page"},{"location":"api/dataset/","page":"Dataset","title":"Dataset","text":"write to CSV with CSV.jl\nflatten to a DataFrame with DataFrames.jl\nplot with StatsPlots.jl\nplot with AlgebraOfGraphics.jl","category":"page"},{"location":"api/plots/#plots-api","page":"Plots","title":"Plots","text":"","category":"section"},{"location":"api/plots/","page":"Plots","title":"Plots","text":"Pages = [\"plots.md\"]","category":"page"},{"location":"api/plots/#Reference","page":"Plots","title":"Reference","text":"","category":"section"},{"location":"api/plots/","page":"Plots","title":"Plots","text":"Modules = [ArviZ]\nPages   = [\"plots.jl\"]\nPrivate = false","category":"page"},{"location":"api/plots/#ArviZ.plot_autocorr","page":"Plots","title":"ArviZ.plot_autocorr","text":"Bar plot of the autocorrelation function (ACF) for a sequence of data.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_autocorr. The docstring of that function is included below.\n\n\n    The ACF plots are helpful as a convergence diagnostic for posteriors from MCMC\n    samples which display autocorrelation.\n\n    Parameters\n    ----------\n    data : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names : list of str, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot. See :ref:`this section <common_var_names>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    max_lag : int, optional\n        Maximum lag to calculate autocorrelation. By Default, the plot displays the\n        first 100 lag or the total number of draws, whichever is smaller.\n    combined : bool, default False\n        Flag for combining multiple chains into a single chain. If False, chains will be\n        plotted separately.\n    grid : tuple, optional\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n        Note this is not used if `ax` is supplied.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on `figsize`.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_config : dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in ``rcParams``.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    autocov : Compute autocovariance estimates for every lag for the input array.\n    autocorr : Compute autocorrelation using FFT for every lag for the input array.\n\n    Examples\n    --------\n    Plot default autocorrelation\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_autocorr(data)\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'] )\n\n\n    Combine chains by variable and select variables by excluding some with partial naming\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['~thet'], filter_vars=\"like\", combined=True)\n\n\n    Specify maximum lag (x axis bound)\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'], max_lag=200, combined=True)\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_bpv","page":"Plots","title":"ArviZ.plot_bpv","text":"Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_bpv. The docstring of that function is included below.\n\n\n    Parameters\n    ----------\n    data : InferenceData\n        :class:`arviz.InferenceData` object containing the observed and\n        posterior/prior predictive data.\n    kind : {\"u_value\", \"p_value\", \"t_stat\"}, default \"u_value\"\n        Specify the kind of plot:\n\n        * The ``kind=\"p_value\"`` computes :math:`p := p(y* \\leq y | y)`.\n          This is the probability of the data y being larger or equal than the predicted data y*.\n          The ideal value is 0.5 (half the predictions below and half above the data).\n        * The ``kind=\"u_value\"`` argument computes :math:`p_i := p(y_i* \\leq y_i | y)`.\n          i.e. like a p_value but per observation :math:`y_i`. This is also known as marginal\n          p_value. The ideal distribution is uniform. This is similar to the LOO-PIT\n          calculation/plot, the difference is than in LOO-pit plot we compute\n          :math:`pi = p(y_i* r \\leq y_i | y_{-i} )`, where :math:`y_{-i}`,\n          is all other data except :math:`y_i`.\n        * The ``kind=\"t_stat\"`` argument computes :math:`:= p(T(y)* \\leq T(y) | y)`\n          where T is any test statistic. See ``t_stat`` argument below for details\n          of available options.\n\n    t_stat : str, float, or callable, default \"median\"\n        Test statistics to compute from the observations and predictive distributions.\n        Allowed strings are “mean”, “median” or “std”. Alternative a quantile can be passed\n        as a float (or str) in the interval (0, 1). Finally a user defined function is also\n        acepted, see examples section for details.\n    bpv : bool, default True\n        If True add the Bayesian p_value to the legend when ``kind = t_stat``.\n    plot_mean : bool, default True\n        Whether or not to plot the mean test statistic.\n    reference : {\"analytical\", \"samples\", None}, default \"analytical\"\n        How to compute the distributions used as reference for ``kind=u_values``\n        or ``kind=p_values``. Use `None` to not plot any reference.\n    mse : bool, default False\n        Show scaled mean square error between uniform distribution and marginal p_value\n        distribution.\n    n_ref : int, default 100\n        Number of reference distributions to sample when ``reference=samples``.\n    hdi_prob : float, optional\n        Probability for the highest density interval for the analytical reference distribution when\n        ``kind=u_values``. Should be in the interval (0, 1]. Defaults to the\n        rcParam ``stats.hdi_prob``. See :ref:`this section <common_hdi_prob>` for usage examples.\n    color : str, optional\n        Matplotlib color\n    grid : tuple, optional\n        Number of rows and columns. By default, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on `figsize`.\n    data_pairs : dict, optional\n        Dictionary containing relations between observed data and posterior/prior predictive data.\n        Dictionary structure:\n\n        - key = data var_name\n        - value = posterior/prior predictive var_name\n\n        For example, ``data_pairs = {'y' : 'y_hat'}``\n        If None, it will assume that the observed data and the posterior/prior\n        predictive data have the same variable name.\n    Labeller : Labeller, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    var_names : list of str, optional\n        Variables to be plotted. If `None` all variable are plotted. Prefix the variables by ``~``\n        when you want to exclude them from the plot. See the :ref:`this section <common_var_names>`\n        for usage examples. See :ref:`this section <common_var_names>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    coords : dict, optional\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. Defaults to including all coordinates for all\n        dimensions if None. See :ref:`this section <common_coords>` for usage examples.\n    flatten : list, optional\n        List of dimensions to flatten in observed_data. Only flattens across the coordinates\n        specified in the coords argument. Defaults to flattening all of the dimensions.\n    flatten_pp : list, optional\n        List of dimensions to flatten in posterior_predictive/prior_predictive. Only flattens\n        across the coordinates specified in the coords argument. Defaults to flattening all\n        of the dimensions. Dimensions should match flatten excluding dimensions for data_pairs\n        parameters. If `flatten` is defined and `flatten_pp` is None, then ``flatten_pp=flatten``.\n    legend : bool, default True\n        Add legend to figure.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend : str, optional\n        Select plotting backend {\"matplotlib\", \"bokeh\"}. Default \"matplotlib\".\n    plot_ref_kwargs :  dict, optional\n        Extra keyword arguments to control how reference is represented.\n        Passed to :meth:`matplotlib.axes.Axes.plot` or\n        :meth:`matplotlib.axes.Axes.axhspan` (when ``kind=u_value``\n        and ``reference=analytical``).\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    group : {\"posterior\", \"prior\"}, default \"posterior\"\n        Specifies which InferenceData group should be plotted. If \"posterior\", then the values\n        in `posterior_predictive` group are compared to the ones in `observed_data`, if \"prior\" then\n        the same comparison happens, but with the values in `prior_predictive` group.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : 2D ndarray of matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_ppc : Plot for posterior/prior predictive checks.\n    plot_loo_pit : Plot Leave-One-Out probability integral transformation (PIT) predictive checks.\n    plot_dist_comparison : Plot to compare fitted and unfitted distributions.\n\n    References\n    ----------\n    * Gelman et al. (2013) see http://www.stat.columbia.edu/~gelman/book/ pages 151-153 for details\n\n    Examples\n    --------\n    Plot Bayesian p_values.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data(\"regression1d\")\n        >>> az.plot_bpv(data, kind=\"p_value\")\n\n    Plot custom test statistic comparison.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data(\"regression1d\")\n        >>> az.plot_bpv(data, kind=\"t_stat\", t_stat=lambda x:np.percentile(x, q=50, axis=-1))\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_compare","page":"Plots","title":"ArviZ.plot_compare","text":"Summary plot for model comparison.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_compare. The docstring of that function is included below.\n\n\n    Models are compared based on their expected log pointwise predictive density (ELPD).\n    This plot is in the style of the one used in [2]_. Chapter 6 in the first edition\n    or 7 in the second.\n\n    Notes\n    -----\n    The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out\n    cross-validation (LOO) or using the widely applicable information criterion (WAIC).\n    We recommend LOO in line with the work presented by [1]_.\n\n    Parameters\n    ----------\n    comp_df : pandas.DataFrame\n        Result of the :func:`arviz.compare` method.\n    insample_dev : bool, default False\n        Plot in-sample ELPD, that is the value of the information criteria without the\n        penalization given by the effective number of parameters (p_loo or p_waic).\n    plot_standard_error : bool, default True\n        Plot the standard error of the ELPD.\n    plot_ic_diff : bool, default True\n        Plot standard error of the difference in ELPD between each model\n        and the top-ranked model.\n    order_by_rank : bool, default True\n        If True ensure the best model is used as reference.\n    legend : bool, default True\n        Add legend to figure.\n    figsize : (float, float), optional\n        If `None`, size is (6, num of models) inches.\n    title : bool, default True\n        Show a tittle with a description of how to interpret the plot.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    plot_kwargs : dict, optional\n        Optional arguments for plot elements. Currently accepts 'color_ic',\n        'marker_ic', 'color_insample_dev', 'marker_insample_dev', 'color_dse',\n        'marker_dse', 'ls_min_ic' 'color_ls_min_ic',  'fontsize'\n    ax : matplotlib_axes or bokeh_figure, optional\n        Matplotlib axes or bokeh figure.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_elpd : Plot pointwise elpd differences between two or more models.\n    compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation.\n    loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n    waic : Compute the widely applicable information criterion.\n\n    References\n    ----------\n    .. [1] Vehtari et al. (2016). Practical Bayesian model evaluation using leave-one-out\n    cross-validation and WAIC https://arxiv.org/abs/1507.04544\n\n    .. [2] McElreath R. (2022). Statistical Rethinking A Bayesian Course with Examples in\n    R and Stan, Second edition, CRC Press.\n\n    Examples\n    --------\n    Show default compare plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> model_compare = az.compare({'Centered 8 schools': az.load_arviz_data('centered_eight'),\n        >>>                  'Non-centered 8 schools': az.load_arviz_data('non_centered_eight')})\n        >>> az.plot_compare(model_compare)\n\n    Include the in-sample ELDP\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_compare(model_compare, insample_dev=True)\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_density","page":"Plots","title":"ArviZ.plot_density","text":"Generate KDE plots for continuous variables and histograms for discrete ones.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_density. The docstring of that function is included below.\n\n\n    Plots are truncated at their 100*(1-alpha)% highest density intervals. Plots are grouped per\n    variable and colors assigned to models.\n\n    Parameters\n    ----------\n    data : InferenceData or iterable of InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object, or an Iterator\n        returning a sequence of such objects.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    group : {\"posterior\", \"prior\"}, default \"posterior\"\n        Specifies which InferenceData group should be plotted. If \"posterior\", then the values\n        in `posterior_predictive` group are compared to the ones in `observed_data`, if \"prior\" then\n        the same comparison happens, but with the values in `prior_predictive` group.\n    data_labels : list of str, default None\n        List with names for the datasets passed as \"data.\" Useful when plotting more than one\n        dataset.  Must be the same shape as the data parameter.\n    var_names : list of str, optional\n        List of variables to plot. If multiple datasets are supplied and `var_names` is not None,\n        will print the same set of variables for each dataset. Defaults to None, which results in\n        all the variables being plotted.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See :ref:`this section <common_combine_dims>` for usage examples.\n    transform : callable\n        Function to transform data (defaults to `None` i.e. the identity function).\n    hdi_prob : float, default 0.94\n        Probability for the highest density interval. Should be in the interval (0, 1].\n        See :ref:`this section <common_hdi_prob>` for usage examples.\n    point_estimate : str, optional\n        Plot point estimate per variable. Values should be 'mean', 'median', 'mode' or None.\n        Defaults to 'auto' i.e. it falls back to default set in ``rcParams``.\n    colors : str or list of str, optional\n        List with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically choose a color per model from matplotlib's\n        cycle. If a single color is passed, e.g. 'k', 'C2' or 'red' this color will be used for all\n        models. Defaults to `cycle`.\n    outline : bool, default True\n        Use a line to draw KDEs and histograms.\n    hdi_markers : str\n        A valid `matplotlib.markers` like 'v', used to indicate the limits of the highest density\n        interval. Defaults to empty string (no marker).\n    shade : float, default 0\n        Alpha blending value for the shaded area under the curve, between 0 (no shade) and 1\n        (opaque).\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when `circular` is False\n        and \"taylor\" (for now) when `circular` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    circular : bool, default False\n        If True, it interprets the values passed are from a circular variable measured in radians\n        and a circular KDE is used. Only valid for 1D KDE.\n    grid : tuple, optional\n        Number of rows and columns. Defaults to ``None``, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : 2D ndarray of matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n\n    Examples\n    --------\n    Plot default density plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> az.plot_density([centered, non_centered])\n\n    Plot variables in a 4x5 grid\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], grid=(4, 5))\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"])\n\n    Plot a specific `az.InferenceData` group\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], group=\"prior\")\n\n    Specify highest density interval\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], hdi_prob=.5)\n\n    Shade plots and/or remove outlines\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], outline=False, shade=.8)\n\n    Specify binwidth for kernel density estimation\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], bw=.9)\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_dist","page":"Plots","title":"ArviZ.plot_dist","text":"Plot distribution as histogram or kernel density estimates.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_dist. The docstring of that function is included below.\n\n\n    By default continuous variables are plotted using KDEs and discrete ones using histograms\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot from an unknown continuous or discrete distribution.\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE or a hexbin will be estimated.\n    color : string\n        valid matplotlib color.\n    kind : string, default \"auto\"\n        By default (\"auto\") continuous variables will use the kind defined by rcParam\n        ``plot.density_kind`` and discrete ones will use histograms.\n        To override this use \"hist\" to plot histograms and \"kde\" for KDEs.\n    cumulative : bool, default False\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE.\n    label : string\n        Text to include as part of the legend.\n    rotated : bool, default False\n        Whether to rotate the 1D KDE plot 90 degrees.\n    rug : bool, default False\n        Add a `rug plot <https://en.wikipedia.org/wiki/Rug_plot>`_ for a specific subset\n        of values. Ignored for 2D KDE.\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``is_circular`` is False\n        and \"taylor\" (for now) when ``is_circular`` is True.\n        Defaults to \"experimental\" when variable is not circular and \"taylor\" when it is.\n    quantiles : list, optional\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n    contour : bool, default True\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE.\n    fill_last : bool, default True\n        If True fill the last contour of the 2D KDE plot.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`. Not implemented for bokeh backend.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE. Passed to :func:`arviz.plot_kde` as\n        ``plot_kwargs``.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use fill_kwargs={'alpha': 0} to disable fill).\n        Ignored for 2D KDE. Passed to :func:`arviz.plot_kde` as ``fill_kwargs``.\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if ``rug=False`` or for 2D KDE\n        Use ``space`` keyword (float) to control the position of the rugplot.\n        The larger this number the lower the rugplot. Passed to\n        :func:`arviz.plot_kde` as ``rug_kwargs``.\n    contour_kwargs : dict\n        Keywords passed to the contourplot. Ignored for 1D KDE.\n    contourf_kwargs : dict\n        Keywords passed to :meth:`matplotlib.axes.Axes.contourf`. Ignored for 1D KDE.\n    pcolormesh_kwargs : dict\n        Keywords passed to :meth:`matplotlib.axes.Axes.pcolormesh`. Ignored for 1D KDE.\n    hist_kwargs : dict\n        Keyword arguments used to customize the histogram. Ignored when plotting a KDE.\n        They are passed to :meth:`matplotlib.axes.Axes.hist` if using matplotlib,\n        or to :meth:`bokeh.plotting.figure.quad` if using bokeh. In bokeh case,\n        the following extra keywords are also supported:\n\n        * ``color``: replaces the ``fill_color`` and ``line_color`` of the ``quad`` method\n        * ``bins``: taken from ``hist_kwargs`` and passed to :func:`numpy.histogram` instead\n        * ``density``: normalize histogram to represent a probability density function,\n          Defaults to ``True``\n\n        * ``cumulative``: plot the cumulative counts. Defaults to ``False``.\n\n    is_circular : {False, True, \"radians\", \"degrees\"}, default False\n        Select input type {\"radians\", \"degrees\"} for circular histogram or KDE plot. If True,\n        default input type is \"radians\". When this argument is present, it interprets the\n        values passed are from a circular variable measured in radians and a circular KDE is\n        used. Inputs in \"degrees\" will undergo an internal conversion to radians. Only valid\n        for 1D KDE.\n    ax : matplotlib_axes or bokeh_figure, optional\n        Matplotlib or bokeh targets on which to plot. If not supplied, Arviz will create\n        its own plot area (and return it).\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs :dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figure\n\n    See Also\n    --------\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    plot_kde : 1D or 2D KDE plot taking into account boundary conditions.\n\n    Examples\n    --------\n    Plot an integer distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> a = np.random.poisson(4, 1000)\n        >>> az.plot_dist(a)\n\n    Plot a continuous distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> b = np.random.normal(0, 1, 1000)\n        >>> az.plot_dist(b)\n\n    Add a rug under the Gaussian distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True)\n\n    Segment into quantiles\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True, quantiles=[.25, .5, .75])\n\n    Plot as the cumulative distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True, quantiles=[.25, .5, .75], cumulative=True)\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_dist_comparison","page":"Plots","title":"ArviZ.plot_dist_comparison","text":"Plot to compare fitted and unfitted distributions.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_dist_comparison. The docstring of that function is included below.\n\n\n    The resulting plots will show the compared distributions both on\n    separate axes (particularly useful when one of them is substantially tighter\n    than another), and plotted together, displaying a grid of three plots per\n    distribution.\n\n    Parameters\n    ----------\n    data : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        containing the posterior/prior data. Refer to documentation of\n        :func:`arviz.convert_to_dataset` for details.\n    kind : {\"latent\", \"observed\"}, default \"latent\"\n        kind of plot to display The \"latent\" option includes {\"prior\", \"posterior\"},\n        and the \"observed\" option includes\n        {\"observed_data\", \"prior_predictive\", \"posterior_predictive\"}.\n    figsize : (float, float), optional\n        Figure size. If ``None`` it will be defined automatically.\n    textsize : float\n        Text size scaling factor for labels, titles and lines. If ``None`` it will be\n        autoscaled based on `figsize`.\n    var_names : str, list, list of lists, optional\n        if str, plot the variable. if list, plot all the variables in list\n        of all groups. if list of lists, plot the vars of groups in respective lists.\n        See :ref:`this section <common_var_names>` for usage examples.\n    coords : dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. See :ref:`this section <common_coords>` for usage examples.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See :ref:`this section <common_combine_dims>` for usage examples.\n    transform : callable\n        Function to transform data (defaults to `None` i.e. the identity function).\n    legend : bool\n        Add legend to figure. By default True.\n    labeller : Labeller, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : (nvars, 3) array-like of matplotlib_axes, optional\n        Matplotlib axes: The ax argument should have shape (nvars, 3), where the\n        last column is for the combined before/after plots and columns 0 and 1 are\n        for the before and after plots, respectively.\n    prior_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for prior/predictive groups.\n    posterior_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for posterior/predictive groups.\n    observed_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for observed_data group.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : 2D ndarray of matplotlib_axes\n        Returned object will have shape (nvars, 3),\n        where the last column is the combined plot and the first columns are the single plots.\n\n    See Also\n    --------\n    plot_bpv : Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n\n    Examples\n    --------\n    Plot the prior/posterior plot for specified vars and coords.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('rugby')\n        >>> az.plot_dist_comparison(data, var_names=[\"defs\"], coords={\"team\" : [\"Italy\"]})\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_elpd","page":"Plots","title":"ArviZ.plot_elpd","text":"Plot pointwise elpd differences between two or more models.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_elpd. The docstring of that function is included below.\n\n\n    Pointwise model comparison based on their expected log pointwise predictive density (ELPD).\n\n    Notes\n    -----\n    The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out\n    cross-validation (LOO) or using the widely applicable information criterion (WAIC).\n    We recommend LOO in line with the work presented by [1]_.\n\n    Parameters\n    ----------\n    compare_dict : mapping of {str : ELPDData or InferenceData}\n        A dictionary mapping the model name to the object containing inference data or the result\n        of :func:`arviz.loo` or :func:`arviz.waic` functions.\n        Refer to :func:`arviz.convert_to_inference_data` for details on possible dict items.\n    color : str or array_like, default \"C0\"\n        Colors of the scatter plot. If color is a str all dots will have the same color.\n        If it is the size of the observations, each dot will have the specified color.\n        Otherwise, it will be interpreted as a list of the dims to be used for the color code.\n    xlabels : bool, default False\n        Use coords as xticklabels.\n    figsize : (float, float), optional\n        If `None`, size is (8 + numvars, 8 + numvars).\n    textsize : float, optional\n        Text size for labels. If `None` it will be autoscaled based on `figsize`.\n    coords : mapping, optional\n        Coordinates of points to plot. **All** values are used for computation, but only a\n        subset can be plotted for convenience. See :ref:`this section <common_coords>`\n        for usage examples.\n    legend : bool, default False\n        Include a legend to the plot. Only taken into account when color argument is a dim name.\n    threshold : float, optional\n        If some elpd difference is larger than ``threshold * elpd.std()``, show its label. If\n        `None`, no observations will be highlighted.\n    ic : str, optional\n        Information Criterion (\"loo\" for PSIS-LOO, \"waic\" for WAIC) used to compare models.\n        Defaults to ``rcParams[\"stats.information_criterion\"]``.\n        Only taken into account when input is :class:`arviz.InferenceData`.\n    scale : str, optional\n        Scale argument passed to :func:`arviz.loo` or :func:`arviz.waic`, see their docs for\n        details. Only taken into account when values in ``compare_dict`` are\n        :class:`arviz.InferenceData`.\n    var_name : str, optional\n        Argument passed to to :func:`arviz.loo` or :func:`arviz.waic`, see their docs for\n        details. Only taken into account when values in ``compare_dict`` are\n        :class:`arviz.InferenceData`.\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter`.\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_compare : Summary plot for model comparison.\n    loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n    waic : Compute the widely applicable information criterion.\n\n    References\n    ----------\n    .. [1] Vehtari et al. (2016). Practical Bayesian model evaluation using leave-one-out\n    cross-validation and WAIC https://arxiv.org/abs/1507.04544\n\n    Examples\n    --------\n    Compare pointwise PSIS-LOO for centered and non centered models of the 8-schools problem\n    using matplotlib.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata1 = az.load_arviz_data(\"centered_eight\")\n        >>> idata2 = az.load_arviz_data(\"non_centered_eight\")\n        >>> az.plot_elpd(\n        >>>     {\"centered model\": idata1, \"non centered model\": idata2},\n        >>>     xlabels=True\n        >>> )\n\n    .. bokeh-plot::\n        :source-position: above\n\n        import arviz as az\n        idata1 = az.load_arviz_data(\"centered_eight\")\n        idata2 = az.load_arviz_data(\"non_centered_eight\")\n        az.plot_elpd(\n            {\"centered model\": idata1, \"non centered model\": idata2},\n            backend=\"bokeh\"\n        )\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_energy","page":"Plots","title":"ArviZ.plot_energy","text":"Plot energy transition distribution and marginal energy distribution in HMC algorithms.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_energy. The docstring of that function is included below.\n\n\n    This may help to diagnose poor exploration by gradient-based algorithms like HMC or NUTS.\n    The energy function in HMC can identify posteriors with heavy tailed distributions, that\n    in practice are challenging for sampling.\n\n    This plot is in the style of the one used in [1]_.\n\n    Parameters\n    ----------\n    data : obj\n        :class:`xarray.Dataset`, or any object that can be converted (must represent\n        ``sample_stats`` and have an ``energy`` variable).\n    kind : str, optional\n        Type of plot to display (\"kde\", \"hist\").\n    bfmi : bool, default True\n        If True add to the plot the value of the estimated Bayesian fraction of missing\n        information.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    legend : bool, default True\n        Flag for plotting legend.\n    fill_alpha : tuple, default (1, 0.75)\n        Alpha blending value for the shaded area under the curve, between 0\n        (no shade) and 1 (opaque).\n    fill_color : tuple of valid matplotlib color, default ('C0', 'C5')\n        Color for Marginal energy distribution and Energy transition distribution.\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\". Defaults to \"experimental\".\n        Only works if ``kind='kde'``.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled\n        based on `figsize`.\n    fill_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_kde` (to control the shade).\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_kde` or :func:`matplotlib.pyplot.hist`\n        (if ``type='hist'``).\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    bfmi : Calculate the estimated Bayesian fraction of missing information (BFMI).\n\n    References\n    ----------\n    .. [1] Betancourt (2016). Diagnosing Suboptimal Cotangent Disintegrations in\n    Hamiltonian Monte Carlo https://arxiv.org/abs/1604.00695\n\n    Examples\n    --------\n    Plot a default energy plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_energy(data)\n\n    Represent energy plot via histograms\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_energy(data, kind='hist')\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_ess","page":"Plots","title":"ArviZ.plot_ess","text":"Generate quantile, local, or evolution ESS plots.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_ess. The docstring of that function is included below.\n\n\n    The local and the quantile ESS plots are recommended for checking\n    that there are enough samples for all the explored regions of the\n    parameter space. Checking local and quantile ESS is particularly\n    relevant when working with HDI intervals as opposed to ESS bulk,\n    which is suitable for point estimates.\n\n    Parameters\n    ----------\n    idata : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    var_names : list of str, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot. See :ref:`this section <common_var_names>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    kind : {\"local\", \"quantile\", \"evolution\"}, default \"local\"\n        Specify the kind of plot:\n\n        * The ``kind=\"local\"`` argument generates the ESS' local efficiency for\n          estimating quantiles of a desired posterior.\n        * The ``kind=\"quantile\"`` argument generates the ESS' local efficiency\n          for estimating small-interval probability of a desired posterior.\n        * The ``kind=\"evolution\"`` argument generates the estimated ESS'\n          with incrised number of iterations of a desired posterior.\n\n    relative : bool, default False\n        Show relative ess in plot ``ress = ess / N``.\n    coords : dict, optional\n        Coordinates of `var_names` to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n        See :ref:`this section <common_coords>` for usage examples.\n    grid : tuple, optional\n        Number of rows and columns. By default, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If ``None`` it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If ``None`` it will be autoscaled\n        based on `figsize`.\n    rug : bool, default False\n        Add a `rug plot <https://en.wikipedia.org/wiki/Rug_plot>`_ for a specific subset of values.\n    rug_kind : str, default \"diverging\"\n        Variable in sample stats to use as rug mask. Must be a boolean variable.\n    n_points : int, default 20\n        Number of points for which to plot their quantile/local ess or number of subsets\n        in the evolution plot.\n    extra_methods : bool, default False\n        Plot mean and sd ESS as horizontal lines. Not taken into account if ``kind = 'evolution'``.\n    min_ess : int, default 400\n        Minimum number of ESS desired. If ``relative=True`` the line is plotted at\n        ``min_ess / n_samples`` for local and quantile kinds and as a curve following\n        the ``min_ess / n`` dependency in evolution kind.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    extra_kwargs : dict, optional\n        If evolution plot, `extra_kwargs` is used to plot ess tail and differentiate it\n        from ess bulk. Otherwise, passed to extra methods lines.\n    text_kwargs : dict, optional\n        Only taken into account when ``extra_methods=True``. kwargs passed to ax.annotate\n        for extra methods lines labels. It accepts the additional\n        key ``x`` to set ``xy=(text_kwargs[\"x\"], mcse)``\n    hline_kwargs : dict, optional\n        kwargs passed to :func:`~matplotlib.axes.Axes.axhline` or to :class:`~bokeh.models.Span`\n        depending on the backend for the horizontal minimum ESS line.\n        For relative ess evolution plots the kwargs are passed to\n        :func:`~matplotlib.axes.Axes.plot` or to :class:`~bokeh.plotting.figure.line`\n    rug_kwargs : dict\n        kwargs passed to rug plot.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :meth:`mpl:matplotlib.axes.Axes.hist` or\n        :meth:`mpl:matplotlib.axes.Axes.plot` function depending on the\n        value of `kind`.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    ess : Calculate estimate of the effective sample size.\n\n    References\n    ----------\n    .. [1] Vehtari et al. (2019). Rank-normalization, folding, and\n        localization: An improved Rhat for assessing convergence of\n        MCMC https://arxiv.org/abs/1903.08008\n\n    Examples\n    --------\n    Plot local ESS.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"centered_eight\")\n        >>> coords = {\"school\": [\"Choate\", \"Lawrenceville\"]}\n        >>> az.plot_ess(\n        ...     idata, kind=\"local\", var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    Plot ESS evolution as the number of samples increase. When the model is converging properly,\n    both lines in this plot should be roughly linear.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ess(\n        ...     idata, kind=\"evolution\", var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    Customize local ESS plot to look like reference paper.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ess(\n        ...     idata, kind=\"local\", var_names=[\"mu\"], drawstyle=\"steps-mid\", color=\"k\",\n        ...     linestyle=\"-\", marker=None, rug=True, rug_kwargs={\"color\": \"r\"}\n        ... )\n\n    Customize ESS evolution plot to look like reference paper.\n\n    .. plot::\n        :context: close-figs\n\n        >>> extra_kwargs = {\"color\": \"lightsteelblue\"}\n        >>> az.plot_ess(\n        ...     idata, kind=\"evolution\", var_names=[\"mu\"],\n        ...     color=\"royalblue\", extra_kwargs=extra_kwargs\n        ... )\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_forest","page":"Plots","title":"ArviZ.plot_forest","text":"Forest plot to compare HDI intervals from a number of distributions.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_forest. The docstring of that function is included below.\n\n\n    Generate forest or ridge plots to compare distributions from a model or list of models.\n    Additionally, the function can display effective sample sizes (ess) and Rhats to visualize\n    convergence diagnostics alongside the distributions.\n\n    Parameters\n    ----------\n    data : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    kind : {\"foresplot\", \"ridgeplot\"}, default \"forestplot\"\n        Specify the kind of plot:\n\n        * The ``kind=\"forestplot\"`` generates credible intervals, where the central points are the\n          estimated posterior means, the thick lines are the central quartiles, and the thin lines\n          represent the :math:`100\\times`(`hdi_prob`)% highest density intervals.\n        * The ``kind=\"ridgeplot\"`` option generates density plots (kernel density estimate or\n          histograms) in the same graph. Ridge plots can be configured to have different overlap,\n          truncation bounds and quantile markers.\n\n    model_names : list of str, optional\n        List with names for the models in the list of data. Useful when plotting more that one\n        dataset.\n    var_names : list of str, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot. See :ref:`this section <common_var_names>` for usage examples.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See :ref:`this section <common_combine_dims>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    transform : callable, optional\n        Function to transform data (defaults to None i.e.the identity function).\n    coords : dict, optional\n        Coordinates of ``var_names`` to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n        See :ref:`this section <common_coords>` for usage examples.\n    combined : bool, default False\n        Flag for combining multiple chains into a single chain. If False, chains will\n        be plotted separately. See :ref:`this section <common_combine>` for usage examples.\n    hdi_prob : float, default 0.94\n        Plots highest posterior density interval for chosen percentage of density.\n        See :ref:`this section <common_ hdi_prob>` for usage examples.\n    rope : tuple or dictionary of tuples\n        Lower and upper values of the Region of Practical Equivalence. If a list with one interval\n        only is provided, the ROPE will be displayed across the y-axis. If more than one\n        interval is provided the length of the list should match the number of variables.\n    quartiles : bool, default True\n        Flag for plotting the interquartile range, in addition to the ``hdi_prob`` intervals.\n    r_hat : bool, default False\n        Flag for plotting Split R-hat statistics. Requires 2 or more chains.\n    ess : bool, default False\n        Flag for plotting the effective sample size.\n    colors : list or string, optional\n        list with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically chose a color per model from the matplotlibs\n        cycle. If a single color is passed, eg 'k', 'C2', 'red' this color will be used for all\n        models. Defaults to 'cycle'.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on ``figsize``.\n    linewidth : int, optional\n        Line width throughout. If `None` it will be autoscaled based on ``figsize``.\n    markersize : int, optional\n        Markersize throughout. If `None` it will be autoscaled based on ``figsize``.\n    legend : bool, optional\n        Show a legend with the color encoded model information.\n        Defaults to True, if there are multiple models.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ridgeplot_alpha: float, optional\n        Transparency for ridgeplot fill.  If ``ridgeplot_alpha=0``, border is colored by model,\n        otherwise a `black` outline is used.\n    ridgeplot_overlap : float, default 2\n        Overlap height for ridgeplots.\n    ridgeplot_kind : string, optional\n        By default (\"auto\") continuous variables are plotted using KDEs and discrete ones using\n        histograms. To override this use \"hist\" to plot histograms and \"density\" for KDEs.\n    ridgeplot_truncate : bool, default True\n        Whether to truncate densities according to the value of ``hdi_prob``.\n    ridgeplot_quantiles : list, optional\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n    figsize : (float, float), optional\n        Figure size. If `None`, it will be defined automatically.\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_config : dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in ``rcParams``.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    1D ndarray of matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    summary : Create a data frame with summary statistics.\n\n    Examples\n    --------\n    Forestplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered_data = az.load_arviz_data('non_centered_eight')\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='forestplot',\n        >>>                            var_names=[\"^the\"],\n        >>>                            filter_vars=\"regex\",\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n    Forestplot with multiple datasets\n\n    .. plot::\n        :context: close-figs\n\n        >>> centered_data = az.load_arviz_data('centered_eight')\n        >>> axes = az.plot_forest([non_centered_data, centered_data],\n        >>>                            model_names = [\"non centered eight\", \"centered eight\"],\n        >>>                            kind='forestplot',\n        >>>                            var_names=[\"^the\"],\n        >>>                            filter_vars=\"regex\",\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools models')\n\n    Forestplot with ropes\n\n    .. plot::\n        :context: close-figs\n\n        >>> rope = {'theta': [{'school': 'Choate', 'rope': (2, 4)}], 'mu': [{'rope': (-2, 2)}]}\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            rope=rope,\n        >>>                            var_names='~tau',\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n\n    Ridgeplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='ridgeplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_overlap=3,\n        >>>                            colors='white',\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n    Ridgeplot non-truncated and with quantiles\n\n    .. plot::\n        :context: close-figs\n\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='ridgeplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_truncate=False,\n        >>>                            ridgeplot_quantiles=[.25, .5, .75],\n        >>>                            ridgeplot_overlap=0.7,\n        >>>                            colors='white',\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_hdi","page":"Plots","title":"ArviZ.plot_hdi","text":"note: Note\nThis function is forwarded to Python's arviz.plot_hdi. The docstring of that function is included below.\n\n    Plot HDI intervals for regression data.\n\n    Parameters\n    ----------\n    x : array-like\n        Values to plot.\n    y : array-like, optional\n        Values from which to compute the HDI. Assumed shape ``(chain, draw, \\*shape)``.\n        Only optional if ``hdi_data`` is present.\n    hdi_data : array_like, optional\n        Precomputed HDI values to use. Assumed shape is ``(*x.shape, 2)``.\n    hdi_prob : float, optional\n        Probability for the highest density interval. Defaults to ``stats.hdi_prob`` rcParam.\n    color : str, optional\n        Color used for the limits of the HDI and fill. Should be a valid matplotlib color.\n    circular : bool, optional\n        Whether to compute the HDI taking into account ``x`` is a circular variable\n        (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n    smooth : boolean, optional\n        If True the result will be smoothed by first computing a linear interpolation of the data\n        over a regular grid and then applying the Savitzky-Golay filter to the interpolated data.\n        Defaults to True.\n    smooth_kwargs : dict, optional\n        Additional keywords modifying the Savitzky-Golay filter. See\n        :func:`scipy:scipy.signal.savgol_filter` for details.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    fill_kwargs : dict, optional\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.fill_between`\n        (use ``fill_kwargs={'alpha': 0}`` to disable fill) or to\n        :meth:`bokeh.plotting.Figure.patch`.\n    plot_kwargs : dict, optional\n        HDI limits keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh.plotting.Figure.patch`.\n    hdi_kwargs : dict, optional\n        Keyword arguments passed to :func:`~arviz.hdi`. Ignored if ``hdi_data`` is present.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    backend : {\"matplotlib\",\"bokeh\"}, optional\n        Select plotting backend.\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh.plotting.Figure.patch`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    hdi : Calculate highest density interval (HDI) of array for given probability.\n\n    Examples\n    --------\n    Plot HDI interval of simulated regression data using `y` argument:\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> x_data = np.random.normal(0, 1, 100)\n        >>> y_data = np.random.normal(2 + x_data * 0.5, 0.5, size=(2, 50, 100))\n        >>> az.plot_hdi(x_data, y_data)\n\n    ``plot_hdi`` can also be given precalculated values with the argument ``hdi_data``. This example\n    shows how to use :func:`~arviz.hdi` to precalculate the values and pass these values to\n    ``plot_hdi``. Similarly to an example in ``hdi`` we are using the ``input_core_dims``\n    argument of :func:`~arviz.wrap_xarray_ufunc` to manually define the dimensions over which\n    to calculate the HDI.\n\n    .. plot::\n        :context: close-figs\n\n        >>> hdi_data = az.hdi(y_data, input_core_dims=[[\"draw\"]])\n        >>> ax = az.plot_hdi(x_data, hdi_data=hdi_data[0], color=\"r\", fill_kwargs={\"alpha\": .2})\n        >>> az.plot_hdi(x_data, hdi_data=hdi_data[1], color=\"k\", ax=ax, fill_kwargs={\"alpha\": .2})\n\n    ``plot_hdi`` can also be used with Inference Data objects. Here we use the posterior predictive\n    to plot the HDI interval.\n\n    .. plot::\n        :context: close-figs\n\n        >>> X = np.random.normal(0,1,100)\n        >>> Y = np.random.normal(2 + X * 0.5, 0.5, size=(2,10,100))\n        >>> idata = az.from_dict(posterior={\"y\": Y}, constant_data={\"x\":X})\n        >>> x_data = idata.constant_data.x\n        >>> y_data = idata.posterior.y\n        >>> az.plot_hdi(x_data, y_data)\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_kde","page":"Plots","title":"ArviZ.plot_kde","text":"1D or 2D KDE plot taking into account boundary conditions.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_kde. The docstring of that function is included below.\n\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE will be estimated\n    cumulative : bool\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE\n    rug : bool\n        If True adds a rugplot. Defaults to False. Ignored for 2D KDE\n    label : string\n        Text to include as part of the legend\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``is_circular`` is False\n        and \"taylor\" (for now) when ``is_circular`` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    adaptive : bool, optional.\n        If True, an adaptative bandwidth is used. Only valid for 1D KDE.\n        Defaults to False.\n    quantiles : list\n        Quantiles in ascending order used to segment the KDE.\n        Use [.25, .5, .75] for quartiles. Defaults to None.\n    rotated : bool\n        Whether to rotate the 1D KDE plot 90 degrees.\n    contour : bool\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE.\n        Defaults to True.\n    hdi_probs : list\n        Plots highest density credibility regions for the provided probabilities for a 2D KDE.\n        Defaults to matplotlib chosen levels with no fixed probability associated.\n    fill_last : bool\n        If True fill the last contour of the 2D KDE plot. Defaults to False.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``. Not implemented for bokeh backend.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE. See :meth:`mpl:matplotlib.axes.Axes.plot`\n        or :meth:`bokeh:bokeh.plotting.Figure.line` for a description of accepted values.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use ``fill_kwargs={'alpha': 0}``\n        to disable fill). Ignored for 2D KDE. Passed to\n        :meth:`bokeh.plotting.Figure.patch`.\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if ``rug=False`` or for 2D KDE\n        Use ``space`` keyword (float) to control the position of the rugplot. The larger this number\n        the lower the rugplot. Passed to :class:`bokeh:bokeh.models.glyphs.Scatter`.\n    contour_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.contour`\n        to draw contour lines or :meth:`bokeh.plotting.Figure.patch`.\n        Ignored for 1D KDE.\n    contourf_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.contourf`\n        to draw filled contours. Ignored for 1D KDE.\n    pcolormesh_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.pcolormesh` or\n        :meth:`bokeh.plotting.Figure.image`.\n        Ignored for 1D KDE.\n    is_circular : {False, True, \"radians\", \"degrees\"}. Default False.\n        Select input type {\"radians\", \"degrees\"} for circular histogram or KDE plot. If True,\n        default input type is \"radians\". When this argument is present, it interprets ``values``\n        is a circular variable measured in radians and a circular KDE is used. Inputs in\n        \"degrees\" will undergo an internal conversion to radians.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    legend : bool\n        Add legend to the figure. By default True.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n    return_glyph : bool, optional\n        Internal argument to return glyphs for bokeh\n\n    Returns\n    -------\n    axes : matplotlib.Axes or bokeh.plotting.Figure\n        Object containing the kde plot\n    glyphs : list, optional\n        Bokeh glyphs present in plot.  Only provided if ``return_glyph`` is True.\n\n    See Also\n    --------\n    kde : One dimensional density estimation.\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n\n    Examples\n    --------\n    Plot default KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> mu_posterior = np.concatenate(non_centered.posterior[\"mu\"].values)\n        >>> tau_posterior = np.concatenate(non_centered.posterior[\"tau\"].values)\n        >>> az.plot_kde(mu_posterior)\n\n\n    Plot KDE with rugplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rug=True)\n\n    Plot KDE with adaptive bandwidth\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, adaptive=True)\n\n    Plot KDE with a different bandwidth estimator\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, bw=\"scott\")\n\n    Plot KDE with a bandwidth specified manually\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, bw=0.4)\n\n    Plot KDE for a circular variable\n\n    .. plot::\n        :context: close-figs\n\n        >>> rvs = np.random.vonmises(mu=np.pi, kappa=2, size=500)\n        >>> az.plot_kde(rvs, is_circular=True)\n\n\n    Plot a cumulative distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, cumulative=True)\n\n\n\n    Rotate plot 90 degrees\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rotated=True)\n\n\n    Plot 2d contour KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior)\n\n\n    Plot 2d contour KDE, without filling and contour lines using viridis cmap\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior,\n        ...             contour_kwargs={\"colors\":None, \"cmap\":plt.cm.viridis},\n        ...             contourf_kwargs={\"alpha\":0});\n\n    Plot 2d contour KDE, set the number of levels to 3.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(\n        ...     mu_posterior, values2=tau_posterior,\n        ...     contour_kwargs={\"levels\":3}, contourf_kwargs={\"levels\":3}\n        ... );\n\n    Plot 2d contour KDE with 30%, 60% and 90% HDI contours.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, hdi_probs=[0.3, 0.6, 0.9])\n\n    Plot 2d smooth KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, contour=False)\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_khat","page":"Plots","title":"ArviZ.plot_khat","text":"note: Note\nThis function is forwarded to Python's arviz.plot_khat. The docstring of that function is included below.\n\n    Plot Pareto tail indices for diagnosing convergence.\n\n    Parameters\n    ----------\n    khats : ELPDData containing Pareto shapes information or array of\n        Pareto tail indices.\n    color : str or array_like, optional\n        Colors of the scatter plot, if color is a str all dots will\n        have the same color, if it is the size of the observations,\n        each dot will have the specified color, otherwise, it will be\n        interpreted as a list of the dims to be used for the color\n        code. If Matplotlib c argument is passed, it will override\n        the color argument\n    xlabels : bool, optional\n        Use coords as xticklabels\n    show_hlines : bool, optional\n        Show the horizontal lines, by default at the values [0, 0.5, 0.7, 1].\n    show_bins : bool, optional\n        Show the percentage of khats falling in each bin, as delimited by hlines.\n    bin_format : str, optional\n        The string is used as formatting guide calling ``bin_format.format(count, pct)``.\n    threshold : float, optional\n        Show the labels of k values larger than threshold. Defaults to `None`,\n        no observations will be highlighted.\n    hover_label : bool, optional\n        Show the datapoint label when hovering over it with the mouse. Requires an interactive\n        backend.\n    hover_format : str, optional\n        String used to format the hover label via ``hover_format.format(idx, coord_label)``\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    coords : mapping, optional\n        Coordinates of points to plot. **All** values are used for computation, but only a\n        a subset can be plotted for convenience.\n    legend : bool, optional\n        Include a legend to the plot. Only taken into account when color argument is a dim name.\n    markersize : int, optional\n        markersize for scatter plot. Defaults to `None` in which case it will\n        be chosen based on autoscaling for figsize.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    hlines_kwargs : dictionary, optional\n        Additional keywords passed to\n        :meth:`matplotlib.axes.Axes.hlines`.\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n    kwargs :\n        Additional keywords passed to\n        :meth:`matplotlib.axes.Axes.scatter`.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    psislw : Pareto smoothed importance sampling (PSIS).\n\n    Examples\n    --------\n    Plot estimated pareto shape parameters showing how many fall in each category.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> radon = az.load_arviz_data(\"radon\")\n        >>> loo_radon = az.loo(radon, pointwise=True)\n        >>> az.plot_khat(loo_radon, show_bins=True)\n\n    Show xlabels\n\n    .. plot::\n        :context: close-figs\n\n        >>> centered_eight = az.load_arviz_data(\"centered_eight\")\n        >>> khats = az.loo(centered_eight, pointwise=True).pareto_k\n        >>> az.plot_khat(khats, xlabels=True, threshold=1)\n\n    Use custom color scheme\n\n    .. plot::\n        :context: close-figs\n\n        >>> counties = radon.posterior.County[radon.constant_data.county_idx].values\n        >>> colors = [\n        ...     \"blue\" if county[-1] in (\"A\", \"N\") else \"green\" for county in counties\n        ... ]\n        >>> az.plot_khat(loo_radon, color=colors)\n\n    Notes\n    -----\n    The Generalized Pareto distribution (GPD) may be used to diagnose\n    convergence rates for importance sampling.  GPD has parameters\n    offset, scale, and shape. The shape parameter is usually denoted\n    with ``k``. ``k`` also tells how many finite moments the\n    distribution has. The pre-asymptotic convergence rate of\n    importance sampling can be estimated based on the fractional\n    number of finite moments of the importance ratio distribution. GPD\n    is fitted to the largest importance ratios and the estimated shape\n    parameter ``k``, i.e., ``\\hat{k}`` can then be used as a diagnostic\n    (most importantly if ``\\hat{k} > 0.7``, then the convergence rate\n    is impractically low). See [1]_.\n\n    References\n    ----------\n    .. [1] Vehtari, A., Simpson, D., Gelman, A., Yao, Y., Gabry, J.,\n        2019. Pareto Smoothed Importance Sampling. arXiv:1507.02646 [stat].\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_loo_pit","page":"Plots","title":"ArviZ.plot_loo_pit","text":"Plot Leave-One-Out (LOO) probability integral transformation (PIT) predictive checks.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_loo_pit. The docstring of that function is included below.\n\n\n    Parameters\n    ----------\n    idata : InferenceData\n        :class:`arviz.InferenceData` object.\n    y : array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat : array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as y plus an\n        extra dimension at the end of size n_samples (chains and draws stacked). If str or\n        None, ``idata`` must contain the posterior predictive group. If None, ``y_hat`` is taken\n        equal to y, thus, y must be str too.\n    log_weights : array or DataArray\n        Smoothed log_weights. It must have the same shape as ``y_hat``\n    ecdf : bool, optional\n        Plot the difference between the LOO-PIT Empirical Cumulative Distribution Function\n        (ECDF) and the uniform CDF instead of LOO-PIT kde.\n        In this case, instead of overlaying uniform distributions, the beta ``hdi_prob``\n        around the theoretical uniform CDF is shown. This approximation only holds\n        for large S and ECDF values not very close to 0 nor 1. For more information, see\n        `Vehtari et al. (2019)`, `Appendix G <https://avehtari.github.io/rhat_ess/rhat_ess.html>`_.\n    ecdf_fill : bool, optional\n        Use :meth:`matplotlib.axes.Axes.fill_between` to mark the area\n        inside the credible interval. Otherwise, plot the\n        border lines.\n    n_unif : int, optional\n        Number of datasets to simulate and overlay from the uniform distribution.\n    use_hdi : bool, optional\n        Compute expected hdi values instead of overlaying the sampled uniform distributions.\n    hdi_prob : float, optional\n        Probability for the highest density interval. Works with ``use_hdi=True`` or ``ecdf=True``.\n    figsize : (float, float), optional\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize : int, optional\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    labeller : Labeller, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    color : str or array_like, optional\n        Color of the LOO-PIT estimated pdf plot. If ``plot_unif_kwargs`` has no \"color\" key,\n        a slightly lighter color than this argument will be used for the uniform kde lines.\n        This will ensure that LOO-PIT kde and uniform kde have different default colors.\n    legend : bool, optional\n        Show the legend of the figure.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    plot_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot`\n        for LOO-PIT line (kde or ECDF)\n    plot_unif_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot` for\n        overlaid uniform distributions or for beta credible interval\n        lines if ``ecdf=True``\n    hdi_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.axhspan`\n    fill_kwargs : dict, optional\n        Additional kwargs passed to :meth:`matplotlib.axes.Axes.fill_between`\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    plot_bpv : Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n    loo_pit : Compute leave one out (PSIS-LOO) probability integral transform (PIT) values.\n\n    References\n    ----------\n    * Gabry et al. (2017) see https://arxiv.org/abs/1709.01449\n    * https://mc-stan.org/bayesplot/reference/PPC-loo.html\n    * Gelman et al. BDA (2014) Section 6.3\n\n    Examples\n    --------\n    Plot LOO-PIT predictive checks overlaying the KDE of the LOO-PIT values to several\n    realizations of uniform variable sampling with the same number of observations.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"radon\")\n        >>> az.plot_loo_pit(idata=idata, y=\"y\")\n\n    Fill the area containing the 94% highest density interval of the difference between uniform\n    variables empirical CDF and the real uniform CDF. A LOO-PIT ECDF clearly outside of these\n    theoretical boundaries indicates that the observations and the posterior predictive\n    samples do not follow the same distribution.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_loo_pit(idata=idata, y=\"y\", ecdf=True)\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_mcse","page":"Plots","title":"ArviZ.plot_mcse","text":"Plot quantile or local Monte Carlo Standard Error.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_mcse. The docstring of that function is included below.\n\n\n    Parameters\n    ----------\n    idata : obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names : list of variable names, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars : {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        `pandas.filter`.\n    coords : dict, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    errorbar : bool, optional\n        Plot quantile value +/- mcse instead of plotting mcse.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    extra_methods : bool, optional\n        Plot mean and sd MCSE as horizontal lines. Only taken into account when\n        ``errorbar=False``.\n    rug : bool\n        Plot rug plot of values diverging or that reached the max tree depth.\n    rug_kind : bool\n        Variable in sample stats to use as rug mask. Must be a boolean variable.\n    n_points : int\n        Number of points for which to plot their quantile/local ess or number of subsets\n        in the evolution plot.\n    labeller : Labeller, optional\n        Class providing the method `make_label_vert` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    rug_kwargs : dict\n        kwargs passed to rug plot in\n        :meth:`mpl:matplotlib.axes.Axes.plot` or :class:`bokeh:bokeh.models.glyphs.Scatter`.\n    extra_kwargs : dict, optional\n        kwargs passed as extra method lines in\n        :meth:`mpl:matplotlib.axes.Axes.axhline` or :class:`bokeh:bokeh.models.Span`\n    text_kwargs : dict, optional\n        kwargs passed to :meth:`mpl:matplotlib.axes.Axes.annotate` for extra methods lines labels.\n        It accepts the additional key ``x`` to set ``xy=(text_kwargs[\"x\"], mcse)``.\n        text_kwargs are ignored for the bokeh plotting backend.\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :meth:`mpl:matplotlib.axes.Axes.hist` or\n        :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib depending on the value of `kind`.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    :func:`arviz.mcse`: Calculate Markov Chain Standard Error statistic.\n\n    References\n    ----------\n    * Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n\n    Examples\n    --------\n    Plot quantile Monte Carlo Standard Error.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"centered_eight\")\n        >>> coords = {\"school\": [\"Deerfield\", \"Lawrenceville\"]}\n        >>> az.plot_mcse(\n        ...     idata, var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_pair","page":"Plots","title":"ArviZ.plot_pair","text":"note: Note\nThis function is forwarded to Python's arviz.plot_pair. The docstring of that function is included below.\n\n    Plot a scatter, kde and/or hexbin matrix with (optional) marginals on the diagonal.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    group: str, optional\n        Specifies which InferenceData group should be plotted.  Defaults to 'posterior'.\n    var_names: list of variable names, optional\n        Variables to be plotted, if None all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n    marginals: bool, optional\n        If True pairplot will include marginal distributions for every variable\n    figsize: figure size tuple\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    kind : str or List[str]\n        Type of plot to display (scatter, kde and/or hexbin)\n    gridsize: int or (int, int), optional\n        Only works for ``kind=hexbin``. The number of hexagons in the x-direction.\n        The corresponding number of hexagons in the y-direction is chosen\n        such that the hexagons are approximately regular. Alternatively, gridsize\n        can be a tuple with two elements specifying the number of hexagons\n        in the x-direction and the y-direction.\n    divergences: Boolean\n        If True divergences will be plotted in a different color, only if group is either 'prior'\n        or 'posterior'.\n    colorbar: bool\n        If True a colorbar will be included as part of the plot (Defaults to False).\n        Only works when ``kind=hexbin``\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    divergences_kwargs: dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter` for divergences\n    scatter_kwargs:\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter` when using scatter kind\n    kde_kwargs: dict, optional\n        Additional keywords passed to :func:`arviz.plot_kde` when using kde kind\n    hexbin_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.hexbin` when\n        using hexbin kind\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    marginal_kwargs: dict, optional\n        Additional keywords passed to :func:`arviz.plot_dist`, modifying the\n        marginal distributions plotted in the diagonal.\n    point_estimate: str, optional\n        Select point estimate from 'mean', 'mode' or 'median'. The point estimate will be\n        plotted using a scatter marker and vertical/horizontal lines.\n    point_estimate_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.axvline`,\n        :meth:`matplotlib.axes.Axes.axhline` (matplotlib) or\n        :class:`bokeh:bokeh.models.Span` (bokeh)\n    point_estimate_marker_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter`\n        or :meth:`bokeh:bokeh.plotting.Figure.square` in point\n        estimate plot. Not available in bokeh\n    reference_values: dict, optional\n        Reference values for the plotted variables. The Reference values will be plotted\n        using a scatter marker\n    reference_values_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.circle` in reference values plot\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    Examples\n    --------\n    KDE Pair Plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> coords = {'school': ['Choate', 'Deerfield']}\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu', 'tau'],\n        >>>             kind='kde',\n        >>>             coords=coords,\n        >>>             divergences=True,\n        >>>             textsize=18)\n\n    Hexbin pair plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu'],\n        >>>             coords=coords,\n        >>>             textsize=18,\n        >>>             kind='hexbin')\n\n    Pair plot showing divergences and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        ...             var_names=['^t', 'mu'],\n        ...             filter_vars=\"regex\",\n        ...             coords=coords,\n        ...             divergences=True,\n        ...             textsize=18)\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_parallel","page":"Plots","title":"ArviZ.plot_parallel","text":"note: Note\nThis function is forwarded to Python's arviz.plot_parallel. The docstring of that function is included below.\n\n    Plot parallel coordinates plot showing posterior points with and without divergences.\n\n    Described by https://arxiv.org/abs/1709.01449\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names\n        Variables to be plotted, if `None` all variables are plotted. Can be used to change the\n        order of the plotted variables. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: mapping, optional\n        Coordinates of ``var_names`` to be plotted.\n        Passed to :meth:`xarray.Dataset.sel`.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    legend: bool\n        Flag for plotting legend (defaults to True)\n    colornd: valid matplotlib color\n        color for non-divergent points. Defaults to 'k'\n    colord: valid matplotlib color\n        color for divergent points. Defaults to 'C1'\n    shadend: float\n        Alpha blending value for non-divergent points, between 0 (invisible) and 1 (opaque).\n        Defaults to .025\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    norm_method: str\n        Method for normalizing the data. Methods include normal, minmax and rank.\n        Defaults to none.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes.\n        Defaults to value set in ``rcParams``.\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_pair : Plot a scatter, kde and/or hexbin matrix with (optional) marginals on the diagonal.\n    plot_trace : Plot distribution (histogram or kernel density estimates) and sampled values\n                 or rank plot\n\n    Examples\n    --------\n    Plot default parallel plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_parallel(data, var_names=[\"mu\", \"tau\"])\n\n\n    Plot parallel plot with normalization\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"normal\")\n\n    Plot parallel plot with minmax\n\n    .. plot::\n        :context: close-figs\n\n        >>> ax = az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"minmax\")\n        >>> ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n\n    Plot parallel plot with rank\n\n    .. plot::\n        :context: close-figs\n\n        >>> ax = az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"rank\")\n        >>> ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_posterior","page":"Plots","title":"ArviZ.plot_posterior","text":"Plot Posterior densities in the style of John K. Kruschke's book.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_posterior. The docstring of that function is included below.\n\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to the documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names\n        Variables to be plotted, two variables are required. Prefix the variables with ``~``\n        when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    transform: callable\n        Function to transform data (defaults to None i.e.the identity function)\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    hdi_prob: float, optional\n        Plots highest density interval for chosen percentage of density.\n        Use 'hide' to hide the highest density interval. Defaults to 0.94.\n    multimodal: bool\n        If true (default) it may compute more than one credible interval if the distribution is\n        multimodal and the modes are well separated.\n    skipna : bool\n        If true ignores nan values when computing the hdi and point estimates. Defaults to false.\n    round_to: int, optional\n        Controls formatting of floats. Defaults to 2 or the integer part, whichever is bigger.\n    point_estimate: Optional[str]\n        Plot point estimate per variable. Values should be 'mean', 'median', 'mode' or None.\n        Defaults to 'auto' i.e. it falls back to default set in rcParams.\n    group: str, optional\n        Specifies which InferenceData group should be plotted. Defaults to 'posterior'.\n    rope: tuple or dictionary of tuples\n        Lower and upper values of the Region Of Practical Equivalence. If a list is provided, its\n        length should match the number of variables.\n    ref_val: float or dictionary of floats\n        display the percentage below and above the values in ref_val. Must be None (default),\n        a constant, a list or a dictionary like see an example below. If a list is provided, its\n        length should match the number of variables.\n    rope_color: str, optional\n        Specifies the color of ROPE and displayed percentage within ROPE\n    ref_val_color: str, optional\n        Specifies the color of the displayed percentage\n    kind: str\n        Type of plot to display (kde or hist) For discrete variables this argument is ignored and\n        a histogram is always used. Defaults to rcParam ``plot.density_kind``\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when `circular` is False\n        and \"taylor\" (for now) when `circular` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is. Only works if `kind == kde`.\n    circular: bool, optional\n        If True, it interprets the values passed are from a circular variable measured in radians\n        and a circular KDE is used. Only valid for 1D KDE. Defaults to False.\n        Only works if `kind == kde`.\n    bins: integer or sequence or 'auto', optional\n        Controls the number of bins,accepts the same keywords :func:`matplotlib.pyplot.hist` does.\n        Only works if `kind == hist`. If None (default) it will use `auto` for continuous variables\n        and `range(xmin, xmax + 1)` for discrete variables.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`\n    show: bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :func:`matplotlib.pyplot.hist` or :func:`matplotlib.pyplot.plot` function\n        depending on the value of `kind`.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    plot_forest : Forest plot to compare HDI intervals from a number of distributions.\n\n    Examples\n    --------\n    Show a default kernel density plot following style of John Kruschke\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_posterior(data)\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'])\n\n    Plot Region of Practical Equivalence (rope) and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', '^the'], filter_vars=\"regex\", rope=(-1, 1))\n\n    Plot Region of Practical Equivalence for selected distributions\n\n    .. plot::\n        :context: close-figs\n\n        >>> rope = {'mu': [{'rope': (-2, 2)}], 'theta': [{'school': 'Choate', 'rope': (2, 4)}]}\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], rope=rope)\n\n    Using `coords` argument to plot only a subset of data\n\n    .. plot::\n        :context: close-figs\n\n        >>> coords = {\"school\": [\"Choate\",\"Phillips Exeter\"]}\n        >>> az.plot_posterior(data, var_names=[\"mu\", \"theta\"], coords=coords)\n\n    Add reference lines\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], ref_val=0)\n\n    Show point estimate of distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], point_estimate='mode')\n\n    Show reference values using variable names and coordinates\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, ref_val= {\"theta\": [{\"school\": \"Deerfield\", \"ref_val\": 4},\n        ...                                             {\"school\": \"Choate\", \"ref_val\": 3}]})\n\n    Show reference values using a list\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, ref_val=[1] + [5] * 8 + [1])\n\n\n    Plot posterior as a histogram\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'], kind='hist')\n\n    Change size of highest density interval\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'], hdi_prob=.75)\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_ppc","page":"Plots","title":"ArviZ.plot_ppc","text":"note: Note\nThis function is forwarded to Python's arviz.plot_ppc. The docstring of that function is included below.\n\n    Plot for posterior/prior predictive checks.\n\n    Parameters\n    ----------\n    data: az.InferenceData object\n        :class:`arviz.InferenceData` object containing the observed and posterior/prior\n        predictive data.\n    kind: str\n        Type of plot to display (\"kde\", \"cumulative\", or \"scatter\"). Defaults to `kde`.\n    alpha: float\n        Opacity of posterior/prior predictive density curves.\n        Defaults to 0.2 for ``kind = kde`` and cumulative, for scatter defaults to 0.7.\n    mean: bool\n        Whether or not to plot the mean posterior/prior predictive distribution.\n        Defaults to ``True``.\n    observed: bool, default True\n        Whether or not to plot the observed data.\n    observed: bool, default False\n        Whether or not to plot a rug plot for the observed data. Only valid if `observed` is\n        `True` and for kind `kde` or `cumulative`.\n    color: str\n        Valid matplotlib ``color``. Defaults to ``C0``.\n    color: list\n        List with valid matplotlib colors corresponding to the posterior/prior predictive\n        distribution, observed data and mean of the posterior/prior predictive distribution.\n        Defaults to [\"C0\", \"k\", \"C1\"].\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None, it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None, it will be\n        autoscaled based on ``figsize``.\n    data_pairs: dict\n        Dictionary containing relations between observed data and posterior/prior predictive data.\n        Dictionary structure:\n\n        - key = data var_name\n        - value = posterior/prior predictive var_name\n\n        For example, ``data_pairs = {'y' : 'y_hat'}``\n        If None, it will assume that the observed data and the posterior/prior\n        predictive data have the same variable name.\n    var_names: list of variable names\n        Variables to be plotted, if `None` all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. Defaults to including all coordinates for all\n        dimensions if None.\n    flatten: list\n        List of dimensions to flatten in ``observed_data``. Only flattens across the coordinates\n        specified in the ``coords`` argument. Defaults to flattening all of the dimensions.\n    flatten_pp: list\n        List of dimensions to flatten in posterior_predictive/prior_predictive. Only flattens\n        across the coordinates specified in the ``coords`` argument. Defaults to flattening all\n        of the dimensions. Dimensions should match flatten excluding dimensions for ``data_pairs``\n        parameters. If ``flatten`` is defined and ``flatten_pp`` is None, then\n        ``flatten_pp = flatten``.\n    num_pp_samples: int\n        The number of posterior/prior predictive samples to plot. For ``kind`` = 'scatter' and\n        ``animation = False`` if defaults to a maximum of 5 samples and will set jitter to 0.7.\n        unless defined. Otherwise it defaults to all provided samples.\n    random_seed: int\n        Random number generator seed passed to ``numpy.random.seed`` to allow\n        reproducibility of the plot. By default, no seed will be provided\n        and the plot will change each call if a random sample is specified\n        by ``num_pp_samples``.\n    jitter: float\n        If ``kind`` is \"scatter\", jitter will add random uniform noise to the height\n        of the ppc samples and observed data. By default 0.\n    animated: bool\n        Create an animation of one posterior/prior predictive sample per frame.\n        Defaults to ``False``. Only works with matploblib backend.\n        To run animations inside a notebook you have to use the `nbAgg` matplotlib's backend.\n        Try with `%matplotlib notebook` or  `%matplotlib  nbAgg`. You can switch back to the\n        default matplotlib's backend with `%matplotlib  inline` or `%matplotlib  auto`.\n        If switching back and forth between matplotlib's backend, you may need to run twice the cell\n        with the animation.\n        If you experience problems rendering the animation try setting\n        `animation_kwargs({'blit':False}`) or changing the matplotlib's backend (e.g. to TkAgg)\n        If you run the animation from a script write `ax, ani = az.plot_ppc(.)`\n    animation_kwargs : dict\n        Keywords passed to  :class:`matplotlib.animation.FuncAnimation`. Ignored with\n        matplotlib backend.\n    legend : bool\n        Add legend to figure. By default ``True``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    group: {\"prior\", \"posterior\"}, optional\n        Specifies which InferenceData group should be plotted. Defaults to 'posterior'.\n        Other value can be 'prior'.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_bpv: Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n    plot_lm: Posterior predictive and mean plots for regression-like data.\n    plot_ppc: plot for posterior/prior predictive checks.\n    plot_ts: Plot timeseries data.\n\n    Examples\n    --------\n    Plot the observed data KDE overlaid on posterior predictive KDEs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('radon')\n        >>> az.plot_ppc(data, data_pairs={\"y\":\"y\"})\n\n    Plot the overlay with empirical CDFs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='cumulative')\n\n    Use the ``coords`` and ``flatten`` parameters to plot selected variable dimensions\n    across multiple plots. We will now modify the dimension ``obs_id`` to contain\n    indicate the name of the county where the measure was taken. The change has to\n    be done on both ``posterior_predictive`` and ``observed_data`` groups, which is\n    why we will use :meth:`~arviz.InferenceData.map` to apply the same function to\n    both groups. Afterwards, we will select the counties to be plotted with the\n    ``coords`` arg.\n\n    .. plot::\n        :context: close-figs\n\n        >>> obs_county = data.posterior[\"County\"][data.constant_data[\"county_idx\"]]\n        >>> data = data.assign_coords(obs_id=obs_county, groups=\"observed_vars\")\n        >>> az.plot_ppc(data, coords={'obs_id': ['ANOKA', 'BELTRAMI']}, flatten=[])\n\n    Plot the overlay using a stacked scatter plot that is particularly useful\n    when the sample sizes are small.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='scatter', flatten=[],\n        >>>             coords={'obs_id': ['AITKIN', 'BELTRAMI']})\n\n    Plot random posterior predictive sub-samples.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, num_pp_samples=30, random_seed=7)\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_rank","page":"Plots","title":"ArviZ.plot_rank","text":"Plot rank order statistics of chains.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_rank. The docstring of that function is included below.\n\n\n    From the paper: Rank plots are histograms of the ranked posterior draws (ranked over all\n    chains) plotted separately for each chain.\n    If all of the chains are targeting the same posterior, we expect the ranks in each chain to be\n    uniform, whereas if one chain has a different location or scale parameter, this will be\n    reflected in the deviation from uniformity. If rank plots of all chains look similar, this\n    indicates good mixing of the chains.\n\n    This plot was introduced by Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter,\n    Paul-Christian Burkner (2019): Rank-normalization, folding, and localization: An improved R-hat\n    for assessing convergence of MCMC. arXiv preprint https://arxiv.org/abs/1903.08008\n\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of  :func:`arviz.convert_to_dataset` for details\n    var_names: string or list of variable names\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    transform: callable\n        Function to transform data (defaults to None i.e.the identity function)\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    bins: None or passed to np.histogram\n        Binning strategy used for histogram. By default uses twice the result of Sturges' formula.\n        See :func:`numpy.histogram` documentation for, other available arguments.\n    kind: string\n        If bars (defaults), ranks are represented as stacked histograms (one per chain). If vlines\n        ranks are represented as vertical lines above or below ``ref_line``.\n    colors: string or list of strings\n        List with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically choose a color per model from matplotlib's\n        cycle. If a single color is passed, e.g. 'k', 'C2' or 'red' this color will be used for all\n        models. Defaults to `cycle`.\n    ref_line: boolean\n        Whether to include a dashed line showing where a uniform distribution would lie\n    labels: bool\n        whether to plot or not the x and y labels, defaults to True\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    ref_line_kwargs : dict, optional\n        Reference line keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.axhline` or\n        :class:`bokeh:bokeh.models.Span`.\n    bar_kwargs : dict, optional\n        Bars keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.bar` or\n        :meth:`bokeh:bokeh.plotting.Figure.vbar`.\n    vlines_kwargs : dict, optional\n        Vlines keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.vlines` or\n        :meth:`bokeh:bokeh.plotting.Figure.multi_line`.\n    marker_vlines_kwargs : dict, optional\n        Marker for the vlines keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.circle`.\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_trace : Plot distribution (histogram or kernel density estimates) and\n                 sampled values or rank plot.\n\n    Examples\n    --------\n    Show a default rank plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_rank(data)\n\n    Recreate Figure 13 from the arxiv preprint\n\n    .. plot::\n        :context: close-figs\n\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_rank(data, var_names='tau')\n\n    Use vlines to compare results for centered vs noncentered models\n\n    .. plot::\n        :context: close-figs\n\n        >>> import matplotlib.pyplot as plt\n        >>> centered_data = az.load_arviz_data('centered_eight')\n        >>> noncentered_data = az.load_arviz_data('non_centered_eight')\n        >>> _, ax = plt.subplots(1, 2, figsize=(12, 3))\n        >>> az.plot_rank(centered_data, var_names=\"mu\", kind='vlines', ax=ax[0])\n        >>> az.plot_rank(noncentered_data, var_names=\"mu\", kind='vlines', ax=ax[1])\n\n    Change the aesthetics using kwargs\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_rank(noncentered_data, var_names=\"mu\", kind=\"vlines\",\n        >>>              vlines_kwargs={'lw':0}, marker_vlines_kwargs={'lw':3});\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_separation","page":"Plots","title":"ArviZ.plot_separation","text":"Separation plot for binary outcome models.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_separation. The docstring of that function is included below.\n\n\n    Model predictions are sorted and plotted using a color code according to\n    the observed data.\n\n    Parameters\n    ----------\n    idata : InferenceData\n        :class:`arviz.InferenceData` object.\n    y : array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat : array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as ``y``. If str or\n        None, ``idata`` must contain the posterior predictive group.\n    y_hat_line : bool, optional\n        Plot the sorted ``y_hat`` predictions.\n    expected_events : bool, optional\n        Plot the total number of expected events.\n    figsize : figure size tuple, optional\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int, optional\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    color : str, optional\n        Color to assign to the positive class. The negative class will be plotted using the\n        same color and an `alpha=0.3` transparency.\n    legend : bool, optional\n        Show the legend of the figure.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    plot_kwargs : dict, optional\n        Additional keywords passed to :meth:`mpl:matplotlib.axes.Axes.bar` or\n        :meth:`bokeh:bokeh.plotting.Figure.vbar` for separation plot.\n    y_hat_line_kwargs : dict, optional\n        Additional keywords passed to ax.plot for ``y_hat`` line.\n    exp_events_kwargs : dict, optional\n        Additional keywords passed to ax.scatter for ``expected_events`` marker.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_ppc : Plot for posterior/prior predictive checks.\n\n    References\n    ----------\n    .. [1] Greenhill, B. *et al.*, The Separation Plot: A New Visual Method\n       for Evaluating the Fit of Binary Models, *American Journal of\n       Political Science*, (2011) see https://doi.org/10.1111/j.1540-5907.2011.00525.x\n\n    Examples\n    --------\n    Separation plot for a logistic regression model.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data('classification10d')\n        >>> az.plot_separation(idata=idata, y='outcome', y_hat='outcome', figsize=(8, 1))\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_trace","page":"Plots","title":"ArviZ.plot_trace","text":"Plot distribution (histogram or kernel density estimates) and sampled values or rank plot.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_trace. The docstring of that function is included below.\n\n\n    If `divergences` data is available in `sample_stats`, will plot the location of divergences as\n    dashed vertical lines.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: str or list of str, optional\n        One or more variables to be plotted. Prefix the variables by ``~`` when you want\n        to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: dict of {str: slice or array_like}, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    divergences: {\"bottom\", \"top\", None}, optional\n        Plot location of divergences on the traceplots.\n    kind: {\"trace\", \"rank_bars\", \"rank_vlines\"}, optional\n        Choose between plotting sampled values per iteration and rank plots.\n    transform: callable, optional\n        Function to transform data (defaults to None i.e.the identity function)\n    figsize: tuple of (float, float), optional\n        If None, size is (12, variables * 2)\n    rug: bool, optional\n        If True adds a rugplot of samples. Defaults to False. Ignored for 2D KDE.\n        Only affects continuous variables.\n    lines: list of tuple of (str, dict, array_like), optional\n        List of (var_name, {'coord': selection}, [line, positions]) to be overplotted as\n        vertical lines on the density and horizontal lines on the trace.\n    circ_var_names : str or list of str, optional\n        List of circular variables to account for when plotting KDE.\n    circ_var_units : str\n        Whether the variables in ``circ_var_names`` are in \"degrees\" or \"radians\".\n    compact: bool, optional\n        Plot multidimensional variables in a single plot.\n    compact_prop: str or dict {str: array_like}, optional\n         Defines the property name and the property values to distinguish different\n        dimensions with compact=True.\n        When compact=True it defaults to color, it is\n        ignored otherwise.\n    combined: bool, optional\n        Flag for combining multiple chains into a single line. If False (default), chains will be\n        plotted separately.\n    chain_prop: str or dict {str: array_like}, optional\n        Defines the property name and the property values to distinguish different chains.\n        If compact=True it defaults to linestyle,\n        otherwise it uses the color to distinguish\n        different chains.\n    legend: bool, optional\n        Add a legend to the figure with the chain color code.\n    plot_kwargs, fill_kwargs, rug_kwargs, hist_kwargs: dict, optional\n        Extra keyword arguments passed to :func:`arviz.plot_dist`. Only affects continuous\n        variables.\n    trace_kwargs: dict, optional\n        Extra keyword arguments passed to :meth:`matplotlib.axes.Axes.plot`\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    rank_kwargs : dict, optional\n        Extra keyword arguments passed to :func:`arviz.plot_rank`\n    axes: axes, optional\n        Matplotlib axes or bokeh figures.\n    backend: {\"matplotlib\", \"bokeh\"}, optional\n        Select plotting backend.\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in rcParams.\n    backend_kwargs: dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_rank : Plot rank order statistics of chains.\n\n    Examples\n    --------\n    Plot a subset variables and select them with partial naming\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('non_centered_eight')\n        >>> coords = {'school': ['Choate', 'Lawrenceville']}\n        >>> az.plot_trace(data, var_names=('theta'), filter_vars=\"like\", coords=coords)\n\n    Show all dimensions of multidimensional variables in the same plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(data, compact=True)\n\n    Display a rank plot instead of trace\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(data, var_names=[\"mu\", \"tau\"], kind=\"rank_bars\")\n\n    Combine all chains into one distribution and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(\n        >>>     data, var_names=('^theta'), filter_vars=\"regex\", coords=coords, combined=True\n        >>> )\n\n\n    Plot reference lines against distribution and trace\n\n    .. plot::\n        :context: close-figs\n\n        >>> lines = (('theta_t',{'school': \"Choate\"}, [-1]),)\n        >>> az.plot_trace(data, var_names=('theta_t', 'theta'), coords=coords, lines=lines)\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/plots/#ArviZ.plot_violin","page":"Plots","title":"ArviZ.plot_violin","text":"Plot posterior of traces as violin plot.\n\nnote: Note\nThis function is forwarded to Python's arviz.plot_violin. The docstring of that function is included below.\n\n\n    Notes\n    -----\n    If multiple chains are provided for a variable they will be combined\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names, optional\n        Variables to be plotted, if None all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    transform: callable\n        Function to transform data (defaults to None i.e. the identity function).\n    quartiles: bool, optional\n        Flag for plotting the interquartile range, in addition to the ``hdi_prob`` * 100%\n        intervals. Defaults to ``True``.\n    rug: bool\n        If ``True`` adds a jittered rugplot. Defaults to ``False``.\n    side : {\"both\", \"left\", \"right\"}, default \"both\"\n        If ``both``, both sides of the violin plot are rendered. If ``left`` or ``right``, only\n        the respective side is rendered. By separately plotting left and right halfs with\n        different data, split violin plots can be achieved.\n    hdi_prob: float, optional\n        Plots highest posterior density interval for chosen percentage of density.\n        Defaults to 0.94.\n    shade: float\n        Alpha blending value for the shaded area under the curve, between 0\n        (no shade) and 1 (opaque). Defaults to 0.\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``circular`` is ``False``\n        and \"taylor\" (for now) when ``circular`` is ``True``.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    circular: bool, optional.\n        If ``True``, it interprets `values` is a circular variable measured in radians\n        and a circular KDE is used. Defaults to ``False``.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: int\n        Text size of the point_estimates, axis ticks, and highest density interval. If None it will\n        be autoscaled based on ``figsize``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    sharex: bool\n        Defaults to ``True``, violinplots share a common x-axis scale.\n    sharey: bool\n        Defaults to ``True``, violinplots share a common y-axis scale.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    shade_kwargs: dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.fill_between`, or\n        :meth:`matplotlib.axes.Axes.barh` to control the shade.\n    rug_kwargs: dict\n        Keywords passed to the rug plot. If true only the right half side of the violin will be\n        plotted.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_forest: Forest plot to compare HDI intervals from a number of distributions.\n\n    Examples\n    --------\n    Show a default violin plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_violin(data)\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/#api","page":"API Overview","title":"API Overview","text":"","category":"section"},{"location":"api/","page":"API Overview","title":"API Overview","text":"Pages = [\"data.md\", \"dataset.md\", \"diagnostics.md\", \"inference_data.md\", \"plots.md\", \"stats.md\"]\nDepth = 1","category":"page"},{"location":"api/stats/#stats-api","page":"Stats","title":"Stats","text":"","category":"section"},{"location":"api/stats/","page":"Stats","title":"Stats","text":"Pages = [\"stats.md\"]","category":"page"},{"location":"api/stats/#General-statistics","page":"Stats","title":"General statistics","text":"","category":"section"},{"location":"api/stats/","page":"Stats","title":"Stats","text":"hdi\nArviZ.summary\nsummarystats\nr2_score","category":"page"},{"location":"api/stats/#ArviZ.ArviZStats.hdi","page":"Stats","title":"ArviZ.ArviZStats.hdi","text":"note: Note\nThis function is forwarded to Python's arviz.hdi. The docstring of that function is included below.\n\n    Calculate highest density interval (HDI) of array for given probability.\n\n    The HDI is the minimum width Bayesian credible interval (BCI).\n\n    Parameters\n    ----------\n    ary: obj\n        object containing posterior samples.\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    hdi_prob: float, optional\n        Prob for which the highest density interval will be computed. Defaults to\n        ``stats.hdi_prob`` rcParam.\n    circular: bool, optional\n        Whether to compute the hdi taking into account `x` is a circular variable\n        (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n        Only works if multimodal is False.\n    multimodal: bool, optional\n        If true it may compute more than one hdi if the distribution is multimodal and the\n        modes are well separated.\n    skipna: bool, optional\n        If true ignores nan values when computing the hdi. Defaults to false.\n    group: str, optional\n        Specifies which InferenceData group should be used to calculate hdi.\n        Defaults to 'posterior'\n    var_names: list, optional\n        Names of variables to include in the hdi report. Prefix the variables by ``~``\n        when you want to exclude them from the report: `[\"~beta\"]` instead of `[\"beta\"]`\n        (see :func:`arviz.summary` for more details).\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: mapping, optional\n        Specifies the subset over to calculate hdi.\n    max_modes: int, optional\n        Specifies the maximum number of modes for multimodal case.\n    dask_kwargs : dict, optional\n        Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.\n    kwargs: dict, optional\n        Additional keywords passed to :func:`~arviz.wrap_xarray_ufunc`.\n\n    Returns\n    -------\n    np.ndarray or xarray.Dataset, depending upon input\n        lower(s) and upper(s) values of the interval(s).\n\n    See Also\n    --------\n    plot_hdi : Plot highest density intervals for regression data.\n    xarray.Dataset.quantile : Calculate quantiles of array for given probabilities.\n\n    Examples\n    --------\n    Calculate the HDI of a Normal random variable:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: import numpy as np\n           ...: data = np.random.normal(size=2000)\n           ...: az.hdi(data, hdi_prob=.68)\n\n    Calculate the HDI of a dataset:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data('centered_eight')\n           ...: az.hdi(data)\n\n    We can also calculate the HDI of some of the variables of dataset:\n\n    .. ipython::\n\n        In [1]: az.hdi(data, var_names=[\"mu\", \"theta\"])\n\n    By default, ``hdi`` is calculated over the ``chain`` and ``draw`` dimensions. We can use the\n    ``input_core_dims`` argument of :func:`~arviz.wrap_xarray_ufunc` to change this. In this example\n    we calculate the HDI also over the ``school`` dimension:\n\n    .. ipython::\n\n        In [1]: az.hdi(data, var_names=\"theta\", input_core_dims = [[\"chain\",\"draw\", \"school\"]])\n\n    We can also calculate the hdi over a particular selection:\n\n    .. ipython::\n\n        In [1]: az.hdi(data, coords={\"chain\":[0, 1, 3]}, input_core_dims = [[\"draw\"]])\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/stats/#ArviZ.ArviZStats.summary","page":"Stats","title":"ArviZ.ArviZStats.summary","text":"summary(\n    data; group = :posterior, coords dims, kwargs...,\n) -> Union{Dataset,DataFrames.DataFrame}\n\nCompute summary statistics on any object that can be passed to convert_to_dataset.\n\nKeywords\n\ncoords: Map from named dimension to named indices.\ndims: Map from variable name to names of its dimensions.\nkwargs: Keyword arguments passed to summarystats.\n\n\n\n\n\n","category":"function"},{"location":"api/stats/#StatsBase.summarystats","page":"Stats","title":"StatsBase.summarystats","text":"summarystats(\n    data::InferenceData;\n    group = :posterior,\n    kwargs...,\n) -> Union{Dataset,DataFrames.DataFrame}\nsummarystats(data::Dataset; kwargs...) -> Union{Dataset,DataFrames.DataFrame}\n\nCompute summary statistics on data.\n\nArguments\n\ndata::Union{Dataset,InferenceData}: The data on which to compute summary statistics. If   data is an InferenceData, only the dataset corresponding to group is used.\n\nKeywords\n\nvar_names: Collection of names of variables as Symbols to include in summary\ninclude_circ::Bool=false: Whether to include circular statistics\ndigits::Int: Number of decimals used to round results. If not provided, numbers are not   rounded.\nstat_funcs::Union{Dict{String,Function},Vector{Function}}=nothing: A vector of functions   or a dict of functions with function names as keys used to calculate statistics. By   default, the mean, standard deviation, simulation standard error, and highest posterior   density intervals are included.   The functions will be given one argument, the samples for a variable as an array, The   functions should operate on an array, returning a single number. For example,   Statistics.mean, or Statistics.var would both work.\nextend::Bool=true: If true, use the statistics returned by stat_funcs in addition   to, rather than in place of, the default statistics. This is only meaningful when   stat_funcs is not nothing.\nhdi_prob::Real=0.94: HDI interval to compute. This is only meaningful when stat_funcs   is nothing.\nskipna::Bool=false: If true, ignores NaN values when computing the summary   statistics. It does not affect the behaviour of the functions passed to stat_funcs.\n\nReturns\n\nDataFrames.DataFrame: Summary statistics for each variable. Default statistics are:\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat (only computed for traces with 2 or more chains)\n\nExamples\n\nusing ArviZ\nidata = load_example_data(\"centered_eight\")\nsummarystats(idata; var_names=(:mu, :tau))\n\nOther statistics can be calculated by passing a list of functions or a dictionary with key, function pairs:\n\nusing Statistics\nfunction median_sd(x)\n    med = median(x)\n    sd = sqrt(mean((x .- med).^2))\n    return sd\nend\n\nfunc_dict = Dict(\n    \"std\" => x -> std(x; corrected = false),\n    \"median_std\" => median_sd,\n    \"5%\" => x -> quantile(x, 0.05),\n    \"median\" => median,\n    \"95%\" => x -> quantile(x, 0.95),\n)\n\nsummarystats(idata; var_names = (:mu, :tau), stat_funcs = func_dict, extend = false)\n\n\n\n","category":"function"},{"location":"api/stats/#ArviZ.ArviZStats.r2_score","page":"Stats","title":"ArviZ.ArviZStats.r2_score","text":"R² for Bayesian regression models. Only valid for linear models.\n\nnote: Note\nThis function is forwarded to Python's arviz.r2_score. The docstring of that function is included below.\n\n\n    Parameters\n    ----------\n    y_true: array-like of shape = (n_outputs,)\n        Ground truth (correct) target values.\n    y_pred: array-like of shape = (n_posterior_samples, n_outputs)\n        Estimated target values.\n\n    Returns\n    -------\n    Pandas Series with the following indices:\n    r2: Bayesian R²\n    r2_std: standard deviation of the Bayesian R².\n\n    See Also\n    --------\n    plot_lm : Posterior predictive and mean plots for regression-like data.\n\n    Examples\n    --------\n    Calculate R² for Bayesian regression models :\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data('regression1d')\n           ...: y_true = data.observed_data[\"y\"].values\n           ...: y_pred = data.posterior_predictive.stack(sample=(\"chain\", \"draw\"))[\"y\"].values.T\n           ...: az.r2_score(y_true, y_pred)\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/stats/#Pareto-smoothed-importance-sampling","page":"Stats","title":"Pareto-smoothed importance sampling","text":"","category":"section"},{"location":"api/stats/","page":"Stats","title":"Stats","text":"PSIS.PSISResult\nPSIS.psis\nPSIS.psis!","category":"page"},{"location":"api/stats/#PSIS.PSISResult","page":"Stats","title":"PSIS.PSISResult","text":"PSISResult\n\nResult of Pareto-smoothed importance sampling (PSIS) using psis.\n\nProperties\n\nlog_weights: un-normalized Pareto-smoothed log weights\nweights: normalized Pareto-smoothed weights (allocates a copy)\npareto_shape: Pareto k=ξ shape parameter\nnparams: number of parameters in log_weights\nndraws: number of draws in log_weights\nnchains: number of chains in log_weights\nreff: the ratio of the effective sample size of the unsmoothed importance ratios and the actual sample size.\ness: estimated effective sample size of estimate of mean using smoothed importance samples (see ess_is)\ntail_length: length of the upper tail of log_weights that was smoothed\ntail_dist: the generalized Pareto distribution that was fit to the tail of log_weights. Note that the tail weights are scaled to have a maximum of 1, so tail_dist * exp(maximum(log_ratios)) is the corresponding fit directly to the tail of log_ratios.\nnormalized::Bool:indicates whether log_weights are log-normalized along the sample dimensions.\n\nDiagnostic\n\nThe pareto_shape parameter k=ξ of the generalized Pareto distribution tail_dist can be used to diagnose reliability and convergence of estimates using the importance weights [VehtariSimpson2021].\n\nif k  frac13, importance sampling is stable, and importance sampling (IS) and PSIS both are reliable.\nif k  frac12, then the importance ratio distributon has finite variance, and the central limit theorem holds. As k approaches the upper bound, IS becomes less reliable, while PSIS still works well but with a higher RMSE.\nif frac12  k  07, then the variance is infinite, and IS can behave quite poorly. However, PSIS works well in this regime.\nif 07  k  1, then it quickly becomes impractical to collect enough importance weights to reliably compute estimates, and importance sampling is not recommended.\nif k  1, then neither the variance nor the mean of the raw importance ratios exists. The convergence rate is close to zero, and bias can be large with practical sample sizes.\n\nSee PSISPlots.paretoshapeplot for a diagnostic plot.\n\n[VehtariSimpson2021]: Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling. arXiv:1507.02646v7 [stat.CO]\n\n\n\n\n\n","category":"type"},{"location":"api/stats/#PSIS.psis","page":"Stats","title":"PSIS.psis","text":"psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult\n\nCompute Pareto smoothed importance sampling (PSIS) log weights [VehtariSimpson2021].\n\nWhile psis computes smoothed log weights out-of-place, psis! smooths them in-place.\n\nArguments\n\nlog_ratios: an array of logarithms of importance ratios, with size (draws, [chains, [parameters...]]), where chains>1 would be used when chains are generated using Markov chain Monte Carlo.\nreff::Union{Real,AbstractArray}: the ratio(s) of effective sample size of log_ratios and the actual sample size reff = ess/(draws * chains), used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size (parameters...,) to match log_ratios.\n\nKeywords\n\nwarn=true: If true, warning messages are delivered\nnormalize=true: If true, the log-weights will be log-normalized so that exp.(log_weights) sums to 1 along the sample dimensions.\n\nReturns\n\nresult: a PSISResult object containing the results of the Pareto-smoothing.\n\nA warning is raised if the Pareto shape parameter k  07. See PSISResult for details and PSISPlots.paretoshapeplot for a diagnostic plot.\n\n[VehtariSimpson2021]: Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling. arXiv:1507.02646v7 [stat.CO]\n\n\n\n\n\n","category":"function"},{"location":"api/stats/#PSIS.psis!","page":"Stats","title":"PSIS.psis!","text":"psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult\n\nCompute Pareto smoothed importance sampling (PSIS) log weights [VehtariSimpson2021].\n\nWhile psis computes smoothed log weights out-of-place, psis! smooths them in-place.\n\nArguments\n\nlog_ratios: an array of logarithms of importance ratios, with size (draws, [chains, [parameters...]]), where chains>1 would be used when chains are generated using Markov chain Monte Carlo.\nreff::Union{Real,AbstractArray}: the ratio(s) of effective sample size of log_ratios and the actual sample size reff = ess/(draws * chains), used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size (parameters...,) to match log_ratios.\n\nKeywords\n\nwarn=true: If true, warning messages are delivered\nnormalize=true: If true, the log-weights will be log-normalized so that exp.(log_weights) sums to 1 along the sample dimensions.\n\nReturns\n\nresult: a PSISResult object containing the results of the Pareto-smoothing.\n\nA warning is raised if the Pareto shape parameter k  07. See PSISResult for details and PSISPlots.paretoshapeplot for a diagnostic plot.\n\n[VehtariSimpson2021]: Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling. arXiv:1507.02646v7 [stat.CO]\n\n\n\n\n\n","category":"function"},{"location":"api/stats/#Model-assessment-and-selection","page":"Stats","title":"Model assessment and selection","text":"","category":"section"},{"location":"api/stats/#LOO-and-WAIC","page":"Stats","title":"LOO and WAIC","text":"","category":"section"},{"location":"api/stats/","page":"Stats","title":"Stats","text":"AbstractELPDResult\nPSISLOOResult\nWAICResult\nelpd_estimates\ninformation_criterion\nloo\nwaic","category":"page"},{"location":"api/stats/#ArviZ.ArviZStats.AbstractELPDResult","page":"Stats","title":"ArviZ.ArviZStats.AbstractELPDResult","text":"abstract type AbstractELPDResult\n\nAn abstract type representing the result of an ELPD computation.\n\nEvery subtype stores estimates of both the expected log predictive density (elpd) and the effective number of parameters p, as well as standard errors and pointwise estimates of each, from which other relevant estimates can be computed.\n\nSubtypes implement the following functions:\n\nelpd_estimates\ninformation_criterion\n\n\n\n\n\n","category":"type"},{"location":"api/stats/#ArviZ.ArviZStats.PSISLOOResult","page":"Stats","title":"ArviZ.ArviZStats.PSISLOOResult","text":"Results of Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO).\n\nSee also: loo, AbstractELPDResult\n\nestimates: Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p)\npointwise: Pointwise estimates\npsis_result: Pareto-smoothed importance sampling (PSIS) results\n\n\n\n\n\n","category":"type"},{"location":"api/stats/#ArviZ.ArviZStats.WAICResult","page":"Stats","title":"ArviZ.ArviZStats.WAICResult","text":"Results of computing the widely applicable information criterion (WAIC).\n\nSee also: waic, AbstractELPDResult\n\nestimates: Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p)\npointwise: Pointwise estimates\n\n\n\n\n\n","category":"type"},{"location":"api/stats/#ArviZ.ArviZStats.elpd_estimates","page":"Stats","title":"ArviZ.ArviZStats.elpd_estimates","text":"elpd_estimates(result::AbstractELPDResult; pointwise=false) -> (; elpd, elpd_mcse, lpd)\n\nReturn the (E)LPD estimates from the result.\n\n\n\n\n\n","category":"function"},{"location":"api/stats/#ArviZ.ArviZStats.information_criterion","page":"Stats","title":"ArviZ.ArviZStats.information_criterion","text":"information_criterion(elpd, scale::Symbol)\n\nCompute the information criterion for the given scale from the elpd estimate.\n\nscale must be one of (:deviance, :log, :negative_log).\n\nSee also: effective_number_of_parameters, loo, waic\n\n\n\n\n\ninformation_criterion(result::AbstractELPDResult, scale::Symbol; pointwise=false)\n\nCompute information criterion for the given scale from the existing ELPD result.\n\nscale must be one of (:deviance, :log, :negative_log).\n\nIf pointwise=true, then pointwise estimates are returned.\n\n\n\n\n\n","category":"function"},{"location":"api/stats/#ArviZ.ArviZStats.loo","page":"Stats","title":"ArviZ.ArviZStats.loo","text":"loo(log_likelihood; reff=nothing, kwargs...) -> PSISLOOResult{<:NamedTuple,<:NamedTuple}\n\nCompute the Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO). [Vehtari2017][LOOFAQ]\n\nlog_likelihood must be an array of log-likelihood values with shape (chains, draws[, params...]).\n\nKeywords\n\nreff::Union{Real,AbstractArray{<:Real}}: The relative effective sample size(s) of the likelihood values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using ess.\nkwargs: Remaining keywords are forwarded to psis.\n\nSee also: PSISLOOResult, waic\n\n[Vehtari2017]: Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi: 10.1007/s11222-016-9696-4 arXiv: 1507.04544\n\n[LOOFAQ]: Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html\n\n\n\n\n\nloo(data::Dataset; [var_name::Symbol,] kwargs...) -> PSISLOOResult{<:NamedTuple,<:Dataset}\nloo(data::InferenceData; [var_name::Symbol,] kwargs...) -> PSISLOOResult{<:NamedTuple,<:Dataset}\n\nCompute PSIS-LOO from log-likelihood values in data.\n\nIf more than one log-likelihood variable is present, then var_name must be provided.\n\n\n\n\n\n","category":"function"},{"location":"api/stats/#ArviZ.ArviZStats.waic","page":"Stats","title":"ArviZ.ArviZStats.waic","text":"waic(log_likelihood::AbstractArray) -> WAICResult{<:NamedTuple,<:NamedTuple}\n\nCompute the widely applicable information criterion (WAIC).[Watanabe2010][Vehtari2017][LOOFAQ]\n\nlog_likelihood must be an array of log-likelihood values with shape (chains, draws[, params...]).\n\nSee also: WAICResult, loo\n\n[Watanabe2010]: Watanabe, S. Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. 11(116):3571−3594, 2010. https://jmlr.csail.mit.edu/papers/v11/watanabe10a.html\n\n[Vehtari2017]: Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi: 10.1007/s11222-016-9696-4 arXiv: 1507.04544\n\n[LOOFAQ]: Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html\n\n\n\n\n\nwaic(data::Dataset; [var_name::Symbol]) -> WAICResult{<:NamedTuple,<:Dataset}\nwaic(data::InferenceData; [var_name::Symbol]) -> WAICResult{<:NamedTuple,<:Dataset}\n\nCompute WAIC from log-likelihood values in data.\n\nIf more than one log-likelihood variable is present, then var_name must be provided.\n\n\n\n\n\n","category":"function"},{"location":"api/stats/#Others","page":"Stats","title":"Others","text":"","category":"section"},{"location":"api/stats/","page":"Stats","title":"Stats","text":"compare\nloo_pit","category":"page"},{"location":"api/stats/#ArviZ.ArviZStats.compare","page":"Stats","title":"ArviZ.ArviZStats.compare","text":"Compare models based on  their expected log pointwise predictive density (ELPD).\n\nnote: Note\nThis function is forwarded to Python's arviz.compare. The docstring of that function is included below.\n\n\n    The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out\n    cross-validation (LOO) or using the widely applicable information criterion (WAIC).\n    We recommend loo. Read more theory here - in a paper by some of the\n    leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353\n\n    Parameters\n    ----------\n    compare_dict: dict of {str: InferenceData or ELPDData}\n        A dictionary of model names and :class:`arviz.InferenceData` or ``ELPDData``.\n    ic: str, optional\n        Method to estimate the ELPD, available options are \"loo\" or \"waic\". Defaults to\n        ``rcParams[\"stats.information_criterion\"]``.\n    method: str, optional\n        Method used to estimate the weights for each model. Available options are:\n\n        - 'stacking' : stacking of predictive distributions.\n        - 'BB-pseudo-BMA' : pseudo-Bayesian Model averaging using Akaike-type\n          weighting. The weights are stabilized using the Bayesian bootstrap.\n        - 'pseudo-BMA': pseudo-Bayesian Model averaging using Akaike-type\n          weighting, without Bootstrap stabilization (not recommended).\n\n        For more information read https://arxiv.org/abs/1704.02030\n    b_samples: int, optional default = 1000\n        Number of samples taken by the Bayesian bootstrap estimation.\n        Only useful when method = 'BB-pseudo-BMA'.\n        Defaults to ``rcParams[\"stats.ic_compare_method\"]``.\n    alpha: float, optional\n        The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only\n        useful when method = 'BB-pseudo-BMA'. When alpha=1 (default), the distribution is uniform\n        on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1.\n    seed: int or np.random.RandomState instance, optional\n        If int or RandomState, use it for seeding Bayesian bootstrap. Only\n        useful when method = 'BB-pseudo-BMA'. Default None the global\n        :mod:`numpy.random` state is used.\n    scale: str, optional\n        Output scale for IC. Available options are:\n\n        - `log` : (default) log-score (after Vehtari et al. (2017))\n        - `negative_log` : -1 * (log-score)\n        - `deviance` : -2 * (log-score)\n\n        A higher log-score (or a lower deviance) indicates a model with better predictive\n        accuracy.\n    var_name: str, optional\n        If there is more than a single observed variable in the ``InferenceData``, which\n        should be used as the basis for comparison.\n\n    Returns\n    -------\n    A DataFrame, ordered from best to worst model (measured by the ELPD).\n    The index reflects the key with which the models are passed to this function. The columns are:\n    rank: The rank-order of the models. 0 is the best.\n    elpd: ELPD estimated either using (PSIS-LOO-CV `elpd_loo` or WAIC `elpd_waic`).\n        Higher ELPD indicates higher out-of-sample predictive fit (\"better\" model).\n        If `scale` is `deviance` or `negative_log` smaller values indicates\n        higher out-of-sample predictive fit (\"better\" model).\n    pIC: Estimated effective number of parameters.\n    elpd_diff: The difference in ELPD between two models.\n        If more than two models are compared, the difference is computed relative to the\n        top-ranked model, that always has a elpd_diff of 0.\n    weight: Relative weight for each model.\n        This can be loosely interpreted as the probability of each model (among the compared model)\n        given the data. By default the uncertainty in the weights estimation is considered using\n        Bayesian bootstrap.\n    SE: Standard error of the ELPD estimate.\n        If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap.\n    dSE: Standard error of the difference in ELPD between each model and the top-ranked model.\n        It's always 0 for the top-ranked model.\n    warning: A value of 1 indicates that the computation of the ELPD may not be reliable.\n        This could be indication of WAIC/LOO starting to fail see\n        http://arxiv.org/abs/1507.04544 for details.\n    scale: Scale used for the ELPD.\n\n    Examples\n    --------\n    Compare the centered and non centered models of the eight school problem:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data1 = az.load_arviz_data(\"non_centered_eight\")\n           ...: data2 = az.load_arviz_data(\"centered_eight\")\n           ...: compare_dict = {\"non centered\": data1, \"centered\": data2}\n           ...: az.compare(compare_dict)\n\n    Compare the models using PSIS-LOO-CV, returning the ELPD in log scale and calculating the\n    weights using the stacking method.\n\n    .. ipython::\n\n        In [1]: az.compare(compare_dict, ic=\"loo\", method=\"stacking\", scale=\"log\")\n\n    See Also\n    --------\n    loo :\n        Compute the ELPD using the Pareto smoothed importance sampling Leave-one-out\n        cross-validation method.\n    waic : Compute the ELPD using the widely applicable information criterion.\n    plot_compare : Summary plot for model comparison.\n\n    References\n    ----------\n    .. [1] Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using\n        leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017)\n        see https://doi.org/10.1007/s11222-016-9696-4\n\n    \n\n\n\n\n\n","category":"function"},{"location":"api/stats/#ArviZ.ArviZStats.loo_pit","page":"Stats","title":"ArviZ.ArviZStats.loo_pit","text":"Compute leave one out (PSIS-LOO) probability integral transform (PIT) values.\n\nnote: Note\nThis function is forwarded to Python's arviz.loo_pit. The docstring of that function is included below.\n\n\n    Parameters\n    ----------\n    idata: InferenceData\n        :class:`arviz.InferenceData` object.\n    y: array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat: array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as y plus an\n        extra dimension at the end of size n_samples (chains and draws stacked). If str or\n        None, ``idata`` must contain the posterior predictive group. If None, y_hat is taken\n        equal to y, thus, y must be str too.\n    log_weights: array or DataArray\n        Smoothed log_weights. It must have the same shape as ``y_hat``\n    dask_kwargs : dict, optional\n        Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.\n\n    Returns\n    -------\n    loo_pit: array or DataArray\n        Value of the LOO-PIT at each observed data point.\n\n    See Also\n    --------\n    plot_loo_pit : Plot Leave-One-Out probability integral transformation (PIT) predictive checks.\n    loo : Compute Pareto-smoothed importance sampling leave-one-out\n          cross-validation (PSIS-LOO-CV).\n    plot_elpd : Plot pointwise elpd differences between two or more models.\n    plot_khat : Plot Pareto tail indices for diagnosing convergence.\n\n    Examples\n    --------\n    Calculate LOO-PIT values using as test quantity the observed values themselves.\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data(\"centered_eight\")\n           ...: az.loo_pit(idata=data, y=\"obs\")\n\n    Calculate LOO-PIT values using as test quantity the square of the difference between\n    each observation and `mu`. Both ``y`` and ``y_hat`` inputs will be array-like,\n    but ``idata`` will still be passed in order to calculate the ``log_weights`` from\n    there.\n\n    .. ipython::\n\n        In [1]: T = data.observed_data.obs - data.posterior.mu.median(dim=(\"chain\", \"draw\"))\n           ...: T_hat = data.posterior_predictive.obs - data.posterior.mu\n           ...: T_hat = T_hat.stack(__sample__=(\"chain\", \"draw\"))\n           ...: az.loo_pit(idata=data, y=T**2, y_hat=T_hat**2)\n\n    \n\n\n\n\n\n","category":"function"},{"location":"working_with_inference_data/#working-with-inference-data","page":"Working with InferenceData","title":"Working with InferenceData","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"using ArviZ, DimensionalData, Statistics","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Here we present a collection of common manipulations you can use while working with InferenceData.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Let's load one of ArviZ's example datasets. posterior, posterior_predictive, etc are the groups stored in idata, and they are stored as Datasets. In this HTML view, you can click a group name to expand a summary of the group.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"idata = load_example_data(\"centered_eight\")","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"info: Info\nDatasets are DimensionalData.AbstractDimStacks and can be used identically.   The variables a Dataset contains are called \"layers\", and dimensions of the same name that appear in more than one layer within a Dataset must have the same indices.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"InferenceData behaves like a NamedTuple and can be used similarly. Note that unlike a NamedTuple, the groups always appear in a specific order.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"length(idata) # number of groups","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"keys(idata) # group names","category":"page"},{"location":"working_with_inference_data/#Get-the-dataset-corresponding-to-a-single-group","page":"Working with InferenceData","title":"Get the dataset corresponding to a single group","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Group datasets can be accessed both as properties or as indexed items.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"post = idata.posterior","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"post is the dataset itself, so this is a non-allocating operation.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"idata[:posterior] === post","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"InferenceData supports a more advanced indexing syntax, which we'll see later.","category":"page"},{"location":"working_with_inference_data/#Getting-a-new-InferenceData-with-a-subset-of-groups","page":"Working with InferenceData","title":"Getting a new InferenceData with a subset of groups","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"We can index by a collection of group names to get a new InferenceData with just those groups. This is also non-allocating.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"idata_sub = idata[(:posterior, :posterior_predictive)]","category":"page"},{"location":"working_with_inference_data/#Adding-groups-to-an-InferenceData","page":"Working with InferenceData","title":"Adding groups to an InferenceData","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"InferenceData is immutable, so to add or replace groups we use merge to create a new object.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"merge(idata_sub, idata[(:observed_data, :prior)])","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"We can also use Base.setindex to out-of-place add or replace a single group.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Base.setindex(idata_sub, idata.prior, :prior)","category":"page"},{"location":"working_with_inference_data/#Add-a-new-variable","page":"Working with InferenceData","title":"Add a new variable","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Dataset is also immutable. So while the values within the underlying data arrays can be mutated, layers cannot be added or removed from Datasets, and groups cannot be added/removed from InferenceData.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Instead, we do this out-of-place also using merge.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"merge(post, (log_tau=log.(post[:tau]),))","category":"page"},{"location":"working_with_inference_data/#Obtain-an-array-for-a-given-parameter","page":"Working with InferenceData","title":"Obtain an array for a given parameter","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Let’s say we want to get the values for mu as an array. Parameters can be accessed with either property or index syntax.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"post.tau","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"post[:tau] === post.tau","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"To remove the dimensions, just use parent to retrieve the underlying array.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"parent(post.tau)","category":"page"},{"location":"working_with_inference_data/#Get-the-dimension-lengths","page":"Working with InferenceData","title":"Get the dimension lengths","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Let’s check how many groups are in our hierarchical model.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"size(idata.observed_data, :school)","category":"page"},{"location":"working_with_inference_data/#Get-coordinate/index-values","page":"Working with InferenceData","title":"Get coordinate/index values","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"What are the names of the groups in our hierarchical model? You can access them from the coordinate name school in this case.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"DimensionalData.index(idata.observed_data, :school)","category":"page"},{"location":"working_with_inference_data/#Get-a-subset-of-chains","page":"Working with InferenceData","title":"Get a subset of chains","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Let’s keep only chain 0 here. For the subset to take effect on all relevant InferenceData groups – posterior, sample_stats, log_likelihood, and posterior_predictive – we will index InferenceData instead of Dataset.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Here we use DimensionalData's At selector. Its other selectors are also supported.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"idata[chain=At(0)]","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Note that in this case, prior only has a chain of 0. If it also had the other chains, we could have passed chain=At([0, 2]) to subset by chains 0 and 2.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"warning: Warning\nIf we used idata[chain=[0, 2]] without the At selector, this is equivalent to idata[chain=DimensionalData.index(idata.posterior, :chain)[0, 2]], that is, [0, 2] indexes an array of dimension indices, which here would error.   But if we had requested idata[chain=[1, 2]] we would not hit an error, but we would index the wrong chains.   So it's important to always use a selector to index by values of dimension indices.","category":"page"},{"location":"working_with_inference_data/#Remove-the-first-n-draws-(burn-in)","page":"Working with InferenceData","title":"Remove the first n draws (burn-in)","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Let’s say we want to remove the first 100 draws from all the chains and all InferenceData groups with draws. To do this we use the .. syntax from IntervalSets.jl, which is exported by DimensionalData.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"idata[draw=100 .. Inf]","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"If you check the object you will see that the groups posterior, posterior_predictive, prior, and sample_stats have 400 draws compared to idata, which has 500. The group observed_data has not been affected because it does not have the draw dimension.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Alternatively, you can change a subset of groups by combining indexing styles with merge. Here we use this to build a new InferenceData where we have discarded the first 100 draws only from posterior.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"merge(idata, idata[(:posterior,), draw=100 .. Inf])","category":"page"},{"location":"working_with_inference_data/#Compute-posterior-mean-values-along-draw-and-chain-dimensions","page":"Working with InferenceData","title":"Compute posterior mean values along draw and chain dimensions","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"To compute the mean value of the posterior samples, do the following:","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"mean(post)","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"This computes the mean along all dimensions, discarding all dimensions and returning the result as a NamedTuple. This may be what you wanted for mu and tau, which have only two dimensions (chain and draw), but maybe not what you expected for theta, which has one more dimension school.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"You can specify along which dimension you want to compute the mean (or other functions), which instead returns a Dataset.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"mean(post; dims=(:chain, :draw))","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"The singleton dimensions of chain and draw now contain meaningless indices, so you may want to discard them, which you can do with dropdims.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"dropdims(mean(post; dims=(:chain, :draw)); dims=(:chain, :draw))","category":"page"},{"location":"working_with_inference_data/#Renaming-a-dimension","page":"Working with InferenceData","title":"Renaming a dimension","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"We can rename a dimension in a Dataset using DimensionalData's set method:","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"theta_bis = set(post.theta; school=:school_bis)","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"We can use this, for example, to broadcast functions across multiple arrays, automatically matching up shared dimensions, using DimensionalData.broadcast_dims.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"theta_school_diff = broadcast_dims(-, post.theta, theta_bis)","category":"page"},{"location":"working_with_inference_data/#Compute-and-store-posterior-pushforward-quantities","page":"Working with InferenceData","title":"Compute and store posterior pushforward quantities","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"We use “posterior pushfoward quantities” to refer to quantities that are not variables in the posterior but deterministic computations using posterior variables.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"You can compute these pushforward operations and store them as a new variable in a copy of the posterior group.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Here we'll create a new InferenceData with theta_school_diff in the posterior:","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"idata_new = Base.setindex(idata, merge(post, (; theta_school_diff)), :posterior)","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Once you have these pushforward quantities in an InferenceData, you’ll then be able to plot them with ArviZ functions, calculate stats and diagnostics on them, or save and share the InferenceData object with the pushforward quantities included.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Here we compute the mcse of theta_school_diff:","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"mcse(idata_new.posterior).theta_school_diff","category":"page"},{"location":"working_with_inference_data/#Advanced-subsetting","page":"Working with InferenceData","title":"Advanced subsetting","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"To select the value corresponding to the difference between the Choate and Deerfield schools do:","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"school_idx = [\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"]\nschool_bis_idx = [\"Deerfield\", \"Choate\", \"Lawrenceville\"]\ntheta_school_diff[school=At(school_idx), school_bis=At(school_bis_idx)]","category":"page"},{"location":"working_with_inference_data/#Add-new-chains-using-cat","page":"Working with InferenceData","title":"Add new chains using cat","text":"","category":"section"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"Suppose after checking the mcse and realizing you need more samples, you rerun the model with two chains and obtain an idata_rerun object.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"idata_rerun = InferenceData(; posterior=set(post[chain=At([0, 1])]; chain=[4, 5]))","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"You can combine the two using cat.","category":"page"},{"location":"working_with_inference_data/","page":"Working with InferenceData","title":"Working with InferenceData","text":"cat(idata[[:posterior]], idata_rerun; dims=:chain)","category":"page"},{"location":"mpl_examples/#Matplotlib-Example-Gallery","page":"Matplotlib","title":"Matplotlib Example Gallery","text":"","category":"section"},{"location":"mpl_examples/#Autocorrelation-Plot","page":"Matplotlib","title":"Autocorrelation Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_autocorr(data; var_names=[:tau, :mu])\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_autocorr","category":"page"},{"location":"mpl_examples/#Bayesian-P-Value-Posterior-Plot","page":"Matplotlib","title":"Bayesian P-Value Posterior Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nplot_bpv(data)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_bpv","category":"page"},{"location":"mpl_examples/#Bayesian-P-Value-with-Median-T-Statistic-Posterior-Plot","page":"Matplotlib","title":"Bayesian P-Value with Median T Statistic Posterior Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nplot_bpv(data; kind=:t_stat, t_stat=\"0.5\")\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_bpv","category":"page"},{"location":"mpl_examples/#Compare-Plot","page":"Matplotlib","title":"Compare Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nmodel_compare = compare(\n    Dict(\n        \"Centered 8 schools\" => load_example_data(\"centered_eight\"),\n        \"Non-centered 8 schools\" => load_example_data(\"non_centered_eight\"),\n    ),\n)\nplot_compare(model_compare; figsize=(12, 4))\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See compare, plot_compare","category":"page"},{"location":"mpl_examples/#Density-Plot","page":"Matplotlib","title":"Density Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered_data = load_example_data(\"centered_eight\")\nnon_centered_data = load_example_data(\"non_centered_eight\")\nplot_density(\n    [centered_data, non_centered_data];\n    data_labels=[\"Centered\", \"Non Centered\"],\n    var_names=[:theta],\n    shade=0.1,\n)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_density","category":"page"},{"location":"mpl_examples/#Dist-Plot","page":"Matplotlib","title":"Dist Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using Random\nusing Distributions\nusing PyPlot\nfigure() #hide\nusing ArviZ\n\nRandom.seed!(308)\n\nArviZ.use_style(\"arviz-darkgrid\")\n\na = rand(Poisson(4), 1000)\nb = rand(Normal(0, 1), 1000)\n_, ax = plt.subplots(1, 2; figsize=(10, 4))\nplot_dist(a; color=\"C1\", label=\"Poisson\", ax=ax[1])\nplot_dist(b; color=\"C2\", label=\"Gaussian\", ax=ax[2])\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_dist","category":"page"},{"location":"mpl_examples/#ELPD-Plot","page":"Matplotlib","title":"ELPD Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nd1 = load_example_data(\"centered_eight\")\nd2 = load_example_data(\"non_centered_eight\")\nplot_elpd(Dict(\"Centered eight\" => d1, \"Non centered eight\" => d2); xlabels=true)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_elpd","category":"page"},{"location":"mpl_examples/#Energy-Plot","page":"Matplotlib","title":"Energy Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_energy(data; figsize=(12, 8))\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_energy","category":"page"},{"location":"mpl_examples/#ESS-Evolution-Plot","page":"Matplotlib","title":"ESS Evolution Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nplot_ess(idata; var_names=[:b], kind=:evolution)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_ess","category":"page"},{"location":"mpl_examples/#ESS-Local-Plot","page":"Matplotlib","title":"ESS Local Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"non_centered_eight\")\nplot_ess(idata; var_names=[:mu], kind=:local, marker=\"_\", ms=20, mew=2, rug=true)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_ess","category":"page"},{"location":"mpl_examples/#ESS-Quantile-Plot","page":"Matplotlib","title":"ESS Quantile Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nplot_ess(idata; var_names=[:sigma], kind=:quantile, color=\"C4\")\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_ess","category":"page"},{"location":"mpl_examples/#Forest-Plot","page":"Matplotlib","title":"Forest Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered_data = load_example_data(\"centered_eight\")\nnon_centered_data = load_example_data(\"non_centered_eight\")\nplot_forest(\n    [centered_data, non_centered_data];\n    model_names=[\"Centered\", \"Non Centered\"],\n    var_names=[:mu],\n)\ntitle(\"Estimated theta for eight schools model\")\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_forest","category":"page"},{"location":"mpl_examples/#Ridge-Plot","page":"Matplotlib","title":"Ridge Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nrugby_data = load_example_data(\"rugby\")\nplot_forest(\n    rugby_data;\n    kind=:ridgeplot,\n    var_names=[:defs],\n    linewidth=4,\n    combined=true,\n    ridgeplot_overlap=1.5,\n    colors=:blue,\n    figsize=(9, 4),\n)\ntitle(\"Relative defensive strength\\nof Six Nation rugby teams\")\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_forest","category":"page"},{"location":"mpl_examples/#Plot-HDI","page":"Matplotlib","title":"Plot HDI","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using Random\nusing PyPlot\nfigure() #hide\nusing ArviZ\n\nRandom.seed!(308)\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nx_data = randn(100)\ny_data = 2 .+ x_data .* 0.5\ny_data_rep = 0.5 .* randn(200, 100) .+ transpose(y_data)\nplot(x_data, y_data; color=\"C6\")\nplot_hdi(x_data, y_data_rep; color=:k, plot_kwargs=Dict(:ls => \"--\"))\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_hdi","category":"page"},{"location":"mpl_examples/#Joint-Plot","page":"Matplotlib","title":"Joint Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_pair(\n    data;\n    var_names=[:theta],\n    coords=Dict(:school => [\"Choate\", \"Phillips Andover\"]),\n    kind=:hexbin,\n    marginals=true,\n    figsize=(10, 10),\n)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_pair","category":"page"},{"location":"mpl_examples/#KDE-Plot","page":"Matplotlib","title":"KDE Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\n\n## Combine different posterior draws from different chains\nobs = data.posterior_predictive.obs\nsize_obs = size(obs)\ny_hat = reshape(obs, prod(size_obs[1:2]), size_obs[3:end]...)\n\nplot_kde(\n    y_hat;\n    label=\"Estimated Effect\\n of SAT Prep\",\n    rug=true,\n    plot_kwargs=Dict(:linewidth => 2, :color => :black),\n    rug_kwargs=Dict(:color => :black),\n)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_kde","category":"page"},{"location":"mpl_examples/#d-KDE","page":"Matplotlib","title":"2d KDE","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using Random\nusing PyPlot\nfigure() #hide\nusing ArviZ\n\nRandom.seed!(308)\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nplot_kde(rand(100), rand(100))\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_kde","category":"page"},{"location":"mpl_examples/#KDE-Quantiles-Plot","page":"Matplotlib","title":"KDE Quantiles Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using Random\nusing Distributions\nusing PyPlot\nfigure() #hide\nusing ArviZ\n\nRandom.seed!(308)\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndist = rand(Beta(rand(Uniform(0.5, 10)), 5), 1000)\nplot_kde(dist; quantiles=[0.25, 0.5, 0.75])\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_kde","category":"page"},{"location":"mpl_examples/#Pareto-Shape-Plot","page":"Matplotlib","title":"Pareto Shape Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nloo_data = loo(idata)\nplot_khat(loo_data; show_bins=true)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See loo, plot_khat","category":"page"},{"location":"mpl_examples/#LOO-PIT-ECDF-Plot","page":"Matplotlib","title":"LOO-PIT ECDF Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\n\nplot_loo_pit(idata; y=:y, ecdf=true, color=:maroon)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See psis, plot_loo_pit","category":"page"},{"location":"mpl_examples/#LOO-PIT-Overlay-Plot","page":"Matplotlib","title":"LOO-PIT Overlay Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"non_centered_eight\")\nplot_loo_pit(; idata, y=:obs, color=:indigo)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_loo_pit","category":"page"},{"location":"mpl_examples/#Quantile-Monte-Carlo-Standard-Error-Plot","page":"Matplotlib","title":"Quantile Monte Carlo Standard Error Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_mcse(data; var_names=[:tau, :mu], rug=true, extra_methods=true)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_mcse","category":"page"},{"location":"mpl_examples/#Quantile-MCSE-Errobar-Plot","page":"Matplotlib","title":"Quantile MCSE Errobar Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"radon\")\nplot_mcse(data; var_names=[:sigma_a], color=\"C4\", errorbar=true)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_mcse","category":"page"},{"location":"mpl_examples/#Pair-Plot","page":"Matplotlib","title":"Pair Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered; var_names=[:theta, :mu, :tau], coords, divergences=true, textsize=22\n)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_pair","category":"page"},{"location":"mpl_examples/#Hexbin-Pair-Plot","page":"Matplotlib","title":"Hexbin Pair Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[:theta, :mu, :tau],\n    kind=:hexbin,\n    coords,\n    colorbar=true,\n    divergences=true,\n)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_pair","category":"page"},{"location":"mpl_examples/#KDE-Pair-Plot","page":"Matplotlib","title":"KDE Pair Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[:theta, :mu, :tau],\n    kind=:kde,\n    coords,\n    divergences=true,\n    textsize=22,\n)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_pair","category":"page"},{"location":"mpl_examples/#Point-Estimate-Pair-Plot","page":"Matplotlib","title":"Point Estimate Pair Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[:mu, :theta],\n    kind=[:scatter, :kde],\n    kde_kwargs=Dict(:fill_last => false),\n    marginals=true,\n    coords,\n    point_estimate=:median,\n    figsize=(10, 8),\n)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_pair","category":"page"},{"location":"mpl_examples/#Parallel-Plot","page":"Matplotlib","title":"Parallel Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nax = plot_parallel(data; var_names=[:theta, :tau, :mu])\nax.set_xticklabels(ax.get_xticklabels(); rotation=70)\ndraw()\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_parallel","category":"page"},{"location":"mpl_examples/#Posterior-Plot","page":"Matplotlib","title":"Posterior Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\"])\nplot_posterior(data; var_names=[:mu, :theta], coords, rope=(-1, 1))\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_posterior","category":"page"},{"location":"mpl_examples/#Posterior-Predictive-Check-Plot","page":"Matplotlib","title":"Posterior Predictive Check Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_ppc(data; data_pairs=Dict(:obs => :obs), alpha=0.03, figsize=(12, 6), textsize=14)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_ppc","category":"page"},{"location":"mpl_examples/#Posterior-Predictive-Check-Cumulative-Plot","page":"Matplotlib","title":"Posterior Predictive Check Cumulative Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_ppc(data; alpha=0.3, kind=:cumulative, figsize=(12, 6), textsize=14)\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_ppc","category":"page"},{"location":"mpl_examples/#Rank-Plot","page":"Matplotlib","title":"Rank Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_rank(data; var_names=[:tau, :mu])\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_rank","category":"page"},{"location":"mpl_examples/#Separation-Plot","page":"Matplotlib","title":"Separation Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"classification10d\")\nplot_separation(data; y=:outcome, y_hat=:outcome, figsize=(8, 1))\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_separation","category":"page"},{"location":"mpl_examples/#Trace-Plot","page":"Matplotlib","title":"Trace Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_trace(data; var_names=[:tau, :mu])\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_trace","category":"page"},{"location":"mpl_examples/#Violin-Plot","page":"Matplotlib","title":"Violin Plot","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_violin(data; var_names=[:mu, :tau])\n\ngcf()","category":"page"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"See plot_violin","category":"page"},{"location":"mpl_examples/#Styles","page":"Matplotlib","title":"Styles","text":"","category":"section"},{"location":"mpl_examples/","page":"Matplotlib","title":"Matplotlib","text":"using PyPlot\nfigure() #hide\nusing PyCall\nusing Distributions\nusing ArviZ\n\nx = range(0, 1; length=100)\ndist = pdf.(Beta(2, 5), x)\n\nstyle_list = [\n    \"default\",\n    [\"default\", \"arviz-colors\"],\n    \"arviz-darkgrid\",\n    \"arviz-whitegrid\",\n    \"arviz-white\",\n]\n\nfig = figure(; figsize=(12, 12))\nfor (idx, style) in enumerate(style_list)\n    @pywith plt.style.context(style) begin\n        ax = fig.add_subplot(3, 2, idx; label=idx)\n        for i in 0:9\n            ax.plot(x, dist .- i, \"C$i\"; label=\"C$i\")\n        end\n        ax.set_title(style)\n        ax.set_xlabel(\"x\")\n        ax.set_ylabel(\"f(x)\"; rotation=0, labelpad=15)\n        ax.legend(; bbox_to_anchor=(1, 1))\n        draw()\n    end\nend\ntight_layout()\n\ngcf()","category":"page"},{"location":"api/inference_data/#inferencedata-api","page":"InferenceData","title":"InferenceData","text":"","category":"section"},{"location":"api/inference_data/","page":"InferenceData","title":"InferenceData","text":"Pages = [\"inference_data.md\"]","category":"page"},{"location":"api/inference_data/#Type-definition","page":"InferenceData","title":"Type definition","text":"","category":"section"},{"location":"api/inference_data/","page":"InferenceData","title":"InferenceData","text":"InferenceData","category":"page"},{"location":"api/inference_data/#InferenceObjects.InferenceData","page":"InferenceData","title":"InferenceObjects.InferenceData","text":"InferenceData{group_names,group_types}\n\nContainer for inference data storage using DimensionalData.\n\nThis object implements the InferenceData schema.\n\nInternally, groups are stored in a NamedTuple, which can be accessed using parent(::InferenceData).\n\nConstructors\n\nInferenceData(groups::NamedTuple)\nInferenceData(; groups...)\n\nConstruct an inference data from either a NamedTuple or keyword arguments of groups.\n\nGroups must be Dataset objects.\n\nInstead of directly creating an InferenceData, use the exported from_xyz functions or convert_to_inference_data.\n\n\n\n\n\n","category":"type"},{"location":"api/inference_data/#Property-interface","page":"InferenceData","title":"Property interface","text":"","category":"section"},{"location":"api/inference_data/","page":"InferenceData","title":"InferenceData","text":"getproperty\npropertynames","category":"page"},{"location":"api/inference_data/#Base.getproperty","page":"InferenceData","title":"Base.getproperty","text":"getproperty(data::InferenceData, name::Symbol) -> Dataset\n\nGet group with the specified name.\n\n\n\n\n\n","category":"function"},{"location":"api/inference_data/#Base.propertynames","page":"InferenceData","title":"Base.propertynames","text":"propertynames(data::InferenceData) -> Tuple{Symbol}\n\nGet names of groups\n\n\n\n\n\n","category":"function"},{"location":"api/inference_data/#Indexing-interface","page":"InferenceData","title":"Indexing interface","text":"","category":"section"},{"location":"api/inference_data/","page":"InferenceData","title":"InferenceData","text":"getindex\nBase.setindex","category":"page"},{"location":"api/inference_data/#Base.getindex","page":"InferenceData","title":"Base.getindex","text":"Base.getindex(data::InferenceData, groups::Symbol; coords...) -> Dataset\nBase.getindex(data::InferenceData, groups; coords...) -> InferenceData\n\nReturn a new InferenceData containing the specified groups sliced to the specified coords.\n\ncoords specifies a dimension name mapping to an index, a DimensionalData.Selector, or an IntervalSets.AbstractInterval.\n\nIf one or more groups lack the specified dimension, a warning is raised but can be ignored. All groups that contain the dimension must also contain the specified indices, or an exception will be raised.\n\nExamples\n\nSelect data from all groups for just the specified id values.\n\njulia> using InferenceObjects, DimensionalData\n\njulia> idata = from_namedtuple(\n           (θ=randn(4, 100, 4), τ=randn(4, 100));\n           prior=(θ=randn(4, 100, 4), τ=randn(4, 100)),\n           observed_data=(y=randn(4),),\n           dims=(θ=[:id], y=[:id]),\n           coords=(id=[\"a\", \"b\", \"c\", \"d\"],),\n       )\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b, c, d] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×4)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\"\n\njulia> idata_sel = idata[id=At([\"a\", \"b\"])]\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata_sel.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×2)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\"\n\nSelect data from just the posterior, returning a Dataset if the indices index more than one element from any of the variables:\n\njulia> idata[:observed_data, id=At([\"a\"])]\nDataset with dimensions:\n  Dim{:id} Categorical String[a] ForwardOrdered\nand 1 layer:\n  :y Float64 dims: Dim{:id} (1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:19:25.982\"\n\nNote that if a single index is provided, the behavior is still to slice so that the dimension is preserved.\n\n\n\n\n\n","category":"function"},{"location":"api/inference_data/#Base.setindex","page":"InferenceData","title":"Base.setindex","text":"Base.setindex(data::InferenceData, group::Dataset, name::Symbol) -> InferenceData\n\nCreate a new InferenceData containing the group with the specified name.\n\nIf a group with name is already in data, it is replaced.\n\n\n\n\n\n","category":"function"},{"location":"api/inference_data/#Iteration-interface","page":"InferenceData","title":"Iteration interface","text":"","category":"section"},{"location":"api/inference_data/","page":"InferenceData","title":"InferenceData","text":"InferenceData also implements the same iteration interface as its underlying NamedTuple. That is, iterating over an InferenceData iterates over its groups.","category":"page"},{"location":"api/inference_data/#General-conversion","page":"InferenceData","title":"General conversion","text":"","category":"section"},{"location":"api/inference_data/","page":"InferenceData","title":"InferenceData","text":"convert_to_inference_data\nfrom_dict\nfrom_namedtuple","category":"page"},{"location":"api/inference_data/#InferenceObjects.convert_to_inference_data","page":"InferenceData","title":"InferenceObjects.convert_to_inference_data","text":"convert_to_inference_data(obj; group, kwargs...) -> InferenceData\n\nConvert a supported object to an InferenceData object.\n\nIf obj converts to a single dataset, group specifies which dataset in the resulting InferenceData that is.\n\nSee convert_to_dataset\n\nArguments\n\nobj can be many objects. Basic supported types are:\nInferenceData: return unchanged\nDataset/DimensionalData.AbstractDimStack: add to InferenceData as the only group\nNamedTuple/AbstractDict: create a Dataset as the only group\nAbstractArray{<:Real}: create a Dataset as the only group, given an arbitrary name, if the name is not set\n\nMore specific types may be documented separately.\n\nKeywords\n\ngroup::Symbol = :posterior: If obj converts to a single dataset, assign the resulting dataset to this group.\ndims: a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are:\nSymbol: dimension name\nType{<:DimensionsionalData.Dimension}: dimension type\nDimensionsionalData.Dimension: dimension, potentially with indices\nNothing: no dimension name provided, dimension name is automatically generated\ncoords: a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in dims are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated.\nkwargs: remaining keywords forwarded to converter functions\n\n\n\n\n\n","category":"function"},{"location":"api/inference_data/#InferenceObjects.from_dict","page":"InferenceData","title":"InferenceObjects.from_dict","text":"from_dict(posterior::AbstractDict; kwargs...) -> InferenceData\n\nConvert a Dict to an InferenceData.\n\nArguments\n\nposterior: The data to be converted. Its strings must be Symbol or AbstractString, and its values must be arrays.\n\nKeywords\n\nposterior_predictive::Any=nothing: Draws from the posterior predictive distribution\nsample_stats::Any=nothing: Statistics of the posterior sampling process\npredictions::Any=nothing: Out-of-sample predictions for the posterior.\nprior::Dict=nothing: Draws from the prior\nprior_predictive::Any=nothing: Draws from the prior predictive distribution\nsample_stats_prior::Any=nothing: Statistics of the prior sampling process\nobserved_data::NamedTuple: Observed data on which the posterior is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values.\nconstant_data::NamedTuple: Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values.\npredictions_constant_data::NamedTuple: Constants relevant to the model predictions (i.e. new x values in a linear regression).\nlog_likelihood: Pointwise log-likelihood for the data. It is recommended to use this argument as a NamedTuple whose keys are observed variable names and whose values are log likelihood arrays.\nlibrary: Name of library that generated the draws\ncoords: Map from named dimension to named indices\ndims: Map from variable name to names of its dimensions\n\nReturns\n\nInferenceData: The data with groups corresponding to the provided data\n\nExamples\n\nusing InferenceObjects\nnchains = 2\nndraws = 100\n\ndata = Dict(\n    :x => rand(ndraws, nchains),\n    :y => randn(2, ndraws, nchains),\n    :z => randn(3, 2, ndraws, nchains),\n)\nidata = from_dict(data)\n\n\n\n\n\n","category":"function"},{"location":"api/inference_data/#InferenceObjects.from_namedtuple","page":"InferenceData","title":"InferenceObjects.from_namedtuple","text":"from_namedtuple(posterior::NamedTuple; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{Vector{<:NamedTuple}}; kwargs...) -> InferenceData\nfrom_namedtuple(\n    posterior::NamedTuple,\n    sample_stats::Any,\n    posterior_predictive::Any,\n    predictions::Any,\n    log_likelihood::Any;\n    kwargs...\n) -> InferenceData\n\nConvert a NamedTuple or container of NamedTuples to an InferenceData.\n\nIf containers are passed, they are flattened into a single NamedTuple with array elements whose first dimensions correspond to the dimensions of the containers.\n\nArguments\n\nposterior: The data to be converted. It may be of the following types:\n::NamedTuple: The keys are the variable names and the values are arrays with dimensions (ndraws, nchains[, sizes...]).\n::Vector{Vector{<:NamedTuple}}: A vector of length nchains whose elements have length ndraws.\n\nKeywords\n\nposterior_predictive::Any=nothing: Draws from the posterior predictive distribution\nsample_stats::Any=nothing: Statistics of the posterior sampling process\npredictions::Any=nothing: Out-of-sample predictions for the posterior.\nprior=nothing: Draws from the prior. Accepts the same types as posterior.\nprior_predictive::Any=nothing: Draws from the prior predictive distribution\nsample_stats_prior::Any=nothing: Statistics of the prior sampling process\nobserved_data::NamedTuple: Observed data on which the posterior is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values.\nconstant_data::NamedTuple: Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values.\npredictions_constant_data::NamedTuple: Constants relevant to the model predictions (i.e. new x values in a linear regression).\nlog_likelihood: Pointwise log-likelihood for the data. It is recommended to use this argument as a NamedTuple whose keys are observed variable names and whose values are log likelihood arrays.\nlibrary: Name of library that generated the draws\ncoords: Map from named dimension to named indices\ndims: Map from variable name to names of its dimensions\n\nReturns\n\nInferenceData: The data with groups corresponding to the provided data\n\nnote: Note\nIf a NamedTuple is provided for observed_data, constant_data, or predictionsconstantdata`, any non-array values (e.g. integers) are converted to 0-dimensional arrays.\n\nExamples\n\nusing InferenceObjects\nnchains = 2\nndraws = 100\n\ndata1 = (\n    x=rand(ndraws, nchains), y=randn(ndraws, nchains, 2), z=randn(ndraws, nchains, 3, 2)\n)\nidata1 = from_namedtuple(data1)\n\ndata2 = [[(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws] for _ in 1:nchains];\nidata2 = from_namedtuple(data2)\n\n\n\n\n\n","category":"function"},{"location":"api/inference_data/#General-functions","page":"InferenceData","title":"General functions","text":"","category":"section"},{"location":"api/inference_data/","page":"InferenceData","title":"InferenceData","text":"cat\nmerge","category":"page"},{"location":"api/inference_data/#Base.cat","page":"InferenceData","title":"Base.cat","text":"cat(data::InferenceData...; [groups=keys(data[1]),] dims) -> InferenceData\n\nConcatenate InferenceData objects along the specified dimension dims.\n\nOnly the groups in groups are concatenated. Remaining groups are merged into the new InferenceData object.\n\nExamples\n\nHere is how we can concatenate all groups of two InferenceData objects along the existing chain dimension:\n\njulia> coords = (; a_dim=[\"x\", \"y\", \"z\"]);\n\njulia> dims = dims=(; a=[:a_dim]);\n\njulia> data = Dict(:a => randn(100, 4, 3), :b => randn(100, 4));\n\njulia> idata = from_dict(data; coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1 = cat(idata, idata; dims=:chain)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim} (100×8×3)\n  :b Float64 dims: Dim{:draw}, Dim{:chain} (100×8)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\"\n\nAlternatively, we can concatenate along a new run dimension, which will be created.\n\njulia> idata_cat2 = cat(idata, idata; dims=:run)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat2.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\"\n\nWe can also concatenate only a subset of groups and merge the rest, which is useful when some groups are present only in some of the InferenceData objects or will be identical in all of them:\n\njulia> observed_data = Dict(:y => randn(10));\n\njulia> idata2 = from_dict(data; observed_data=observed_data, coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3 = cat(idata, idata2; groups=(:posterior,), dims=:run)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\"\n\njulia> idata_cat3.observed_data\nDataset with dimensions: Dim{:y_dim_1}\nand 1 layer:\n  :y Float64 dims: Dim{:y_dim_1} (10)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-02-17T15:11:00.59\"\n\n\n\n\n\n","category":"function"},{"location":"api/inference_data/#Base.merge","page":"InferenceData","title":"Base.merge","text":"merge(data::InferenceData...) -> InferenceData\n\nMerge InferenceData objects.\n\nThe result contains all groups in data and others. If a group appears more than once, the one that occurs last is kept.\n\nSee also: cat\n\nExamples\n\nHere we merge an InferenceData containing only a posterior group with one containing only a prior group to create a new one containing both groups.\n\njulia> idata1 = from_dict(Dict(:a => randn(100, 4, 3), :b => randn(100, 4)))\nInferenceData with groups:\n  > posterior\n\njulia> idata2 = from_dict(; prior=Dict(:a => randn(100, 1, 3), :c => randn(100, 1)))\nInferenceData with groups:\n  > prior\n\njulia> idata_merged = merge(idata1, idata2)\nInferenceData with groups:\n  > posterior\n  > prior\n\n\n\n\n\n","category":"function"},{"location":"quickstart/#ArviZ.jl-Quickstart","page":"Quickstart","title":"ArviZ.jl Quickstart","text":"","category":"section"},{"location":"quickstart/#Set-up","page":"Quickstart","title":"Set-up","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Here we add the necessary packages for this notebook and load a few we will use throughout.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using ArviZ, Distributions, LinearAlgebra, PyPlot, Random, StanSample, Turing","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"/home/runner/.julia/conda/3/x86_64/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"# ArviZ ships with style sheets!\nArviZ.use_style(\"arviz-darkgrid\")","category":"page"},{"location":"quickstart/#Get-started-with-plotting","page":"Quickstart","title":"Get started with plotting","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"ArviZ.jl is designed to be used with libraries like Stan, Turing.jl, and Soss.jl but works fine with raw arrays.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"rng1 = Random.MersenneTwister(37772);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"plot_posterior(randn(rng1, 100_000));","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"(Image: )","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Plotting a dictionary of arrays, ArviZ.jl will interpret each key as the name of a different random variable. Each row of an array is treated as an independent series of draws from the variable, called a chain. Below, we have 10 chains of 50 draws each for four different distributions.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"s = (50, 10)\nplot_forest((\n    normal=randn(rng1, s),\n    gumbel=rand(rng1, Gumbel(), s),\n    student_t=rand(rng1, TDist(6), s),\n    exponential=rand(rng1, Exponential(), s),\n),);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"(Image: )","category":"page"},{"location":"quickstart/#Plotting-with-MCMCChains.jl’s-Chains-objects-produced-by-Turing.jl","page":"Quickstart","title":"Plotting with MCMCChains.jl’s Chains objects produced by Turing.jl","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"ArviZ is designed to work well with high dimensional, labelled data. Consider the eight schools model, which roughly tries to measure the effectiveness of SAT classes at eight different schools. To show off ArviZ’s labelling, I give the schools the names of a different eight schools.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"This model is small enough to write down, is hierarchical, and uses labelling. Additionally, a centered parameterization causes divergences (which are interesting for illustration).","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"First we create our data and set some sampling parameters.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"J = 8\ny = [28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0]\nσ = [15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0]\nschools = [\n    \"Choate\",\n    \"Deerfield\",\n    \"Phillips Andover\",\n    \"Phillips Exeter\",\n    \"Hotchkiss\",\n    \"Lawrenceville\",\n    \"St. Paul's\",\n    \"Mt. Hermon\",\n]\nndraws = 1_000\nndraws_warmup = 1_000\nnchains = 4;","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Now we write and run the model using Turing:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Turing.@model function model_turing(y, σ, J=length(y))\n    μ ~ Normal(0, 5)\n    τ ~ truncated(Cauchy(0, 5), 0, Inf)\n    θ ~ filldist(Normal(μ, τ), J)\n    for i in 1:J\n        y[i] ~ Normal(θ[i], σ[i])\n    end\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"model_turing (generic function with 3 methods)","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"rng2 = Random.MersenneTwister(16653);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"param_mod_turing = model_turing(y, σ)\nsampler = NUTS(ndraws_warmup, 0.8)\n\nturing_chns = Turing.sample(\n    rng2, model_turing(y, σ), sampler, MCMCThreads(), ndraws, nchains\n);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"┌ Info: Found initial step size\n└   ϵ = 1.6\n┌ Info: Found initial step size\n└   ϵ = 0.8\n┌ Info: Found initial step size\n└   ϵ = 0.4\n┌ Info: Found initial step size\n└   ϵ = 0.8","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Most ArviZ functions work fine with Chains objects from Turing:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"plot_autocorr(turing_chns; var_names=(:μ, :τ));","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"(Image: )","category":"page"},{"location":"quickstart/#Convert-to-InferenceData","page":"Quickstart","title":"Convert to InferenceData","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"For much more powerful querying, analysis and plotting, we can use built-in ArviZ utilities to convert Chains objects to multidimensional data structures with named dimensions and indices. Note that for such dimensions, the information is not contained in Chains, so we need to provide it.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"ArviZ is built to work with InferenceData, and the more groups it has access to, the more powerful analyses it can perform.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"idata_turing_post = from_mcmcchains(\n    turing_chns;\n    coords=(; school=schools),\n    dims=NamedTuple(k => (:school,) for k in (:y, :σ, :θ)),\n    library=\"Turing\",\n)","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"<div>InferenceData<details>\n<summary>posterior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :μ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :τ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :θ Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n&#10;with metadata Dict{String, Any} with 2 entries:\n  \"created_at\" => \"2023-07-06T22:38:11.324\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>sample_stats</summary>\n<pre><code>Dataset with dimensions: Dim{:draw}, Dim{:chain}\nand 12 layers:\n  :energy           Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :n_steps          Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :diverging        Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :max_energy_error Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :energy_error     Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :is_accept        Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :log_density      Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :tree_depth       Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size        Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :acceptance_rate  Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :lp               Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size_nom    Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n&#10;with metadata Dict{String, Any} with 2 entries:\n  \"created_at\" => \"2023-07-06T22:38:11.208\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n</div>","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Each group is a Dataset, a DimensionalData.AbstractDimStack that can be used identically to a DimensionalData.Dimstack. We can view a summary of the dataset.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"idata_turing_post.posterior","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :μ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :τ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :θ Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n\nwith metadata Dict{String, Any} with 2 entries:\n  \"created_at\"        => \"2023-07-06T22:38:11.324\"\n  \"inference_library\" => \"Turing\"","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Here is a plot of the trace. Note the intelligent labels.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"plot_trace(idata_turing_post);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"(Image: )","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"We can also generate summary stats…","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"summarystats(idata_turing_post)","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"…and examine the energy distribution of the Hamiltonian sampler.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"plot_energy(idata_turing_post);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"(Image: )","category":"page"},{"location":"quickstart/#Additional-information-in-Turing.jl","page":"Quickstart","title":"Additional information in Turing.jl","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"With a few more steps, we can use Turing to compute additional useful groups to add to the InferenceData.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"To sample from the prior, one simply calls sample but with the Prior sampler:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"prior = Turing.sample(rng2, param_mod_turing, Prior(), ndraws);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"To draw from the prior and posterior predictive distributions we can instantiate a “predictive model”, i.e. a Turing model but with the observations set to missing, and then calling predict on the predictive model and the previously drawn samples:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"# Instantiate the predictive model\nparam_mod_predict = model_turing(similar(y, Missing), σ)\n# and then sample!\nprior_predictive = Turing.predict(rng2, param_mod_predict, prior)\nposterior_predictive = Turing.predict(rng2, param_mod_predict, turing_chns);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"And to extract the pointwise log-likelihoods, which is useful if you want to compute metrics such as loo,","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"log_likelihood = let\n    log_likelihood = Turing.pointwise_loglikelihoods(\n        param_mod_turing, MCMCChains.get_sections(turing_chns, :parameters)\n    )\n    # Ensure the ordering of the loglikelihoods matches the ordering of `posterior_predictive`\n    ynames = string.(keys(posterior_predictive))\n    log_likelihood_y = getindex.(Ref(log_likelihood), ynames)\n    (; y=cat(log_likelihood_y...; dims=3))\nend;","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"This can then be included in the from_mcmcchains call from above:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"idata_turing = from_mcmcchains(\n    turing_chns;\n    posterior_predictive,\n    log_likelihood,\n    prior,\n    prior_predictive,\n    observed_data=(; y),\n    coords=(; school=schools),\n    dims=NamedTuple(k => (:school,) for k in (:y, :σ, :θ)),\n    library=Turing,\n)","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"<div>InferenceData<details>\n<summary>posterior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :μ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :τ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :θ Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n&#10;with metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:38.105\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>posterior_predictive</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n&#10;with metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:37.15\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>log_likelihood</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n&#10;with metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:37.884\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>sample_stats</summary>\n<pre><code>Dataset with dimensions: Dim{:draw}, Dim{:chain}\nand 12 layers:\n  :energy           Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :n_steps          Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :diverging        Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :max_energy_error Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :energy_error     Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :is_accept        Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :log_density      Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :tree_depth       Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size        Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :acceptance_rate  Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :lp               Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size_nom    Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n&#10;with metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:38.104\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>prior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :μ Float64 dims: Dim{:draw}, Dim{:chain} (1000×1)\n  :τ Float64 dims: Dim{:draw}, Dim{:chain} (1000×1)\n  :θ Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×1×8)\n&#10;with metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:39.089\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>prior_predictive</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×1×8)\n&#10;with metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:38.755\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>sample_stats_prior</summary>\n<pre><code>Dataset with dimensions: Dim{:draw}, Dim{:chain}\nand 1 layer:\n  :lp Float64 dims: Dim{:draw}, Dim{:chain} (1000×1)\n&#10;with metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:38.943\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>observed_data</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:school} (8)\n&#10;with metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:39.44\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n</div>","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Then we can for example compute the expected leave-one-out (LOO) predictive density, which is an estimate of the out-of-distribution predictive fit of the model:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"loo(idata_turing) # higher ELPD is better","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"PSISLOOResult with estimates\n       Estimate    SE \n elpd       -31   1.5\n    p       0.9  0.35\n\nand PSISResult with 1000 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  5 (62.5%)  723\n  (0.5, 0.7]  okay  3 (37.5%)  390","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"If the model is well-calibrated, i.e. it replicates the true generative process well, the CDF of the pointwise LOO values should be similarly distributed to a uniform distribution. This can be inspected visually:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"plot_loo_pit(idata_turing; y=:y, ecdf=true);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"(Image: )","category":"page"},{"location":"quickstart/#Plotting-with-Stan.jl-outputs","page":"Quickstart","title":"Plotting with Stan.jl outputs","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"StanSample.jl comes with built-in support for producing InferenceData outputs.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Here is the same centered eight schools model in Stan:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"schools_code = \"\"\"\ndata {\n    int<lower=0> J;\n    real y[J];\n    real<lower=0> sigma[J];\n}\n\nparameters {\n    real mu;\n    real<lower=0> tau;\n    real theta[J];\n}\n\nmodel {\n    mu ~ normal(0, 5);\n    tau ~ cauchy(0, 5);\n    theta ~ normal(mu, tau);\n    y ~ normal(theta, sigma);\n}\n\ngenerated quantities {\n    vector[J] log_lik;\n    vector[J] y_hat;\n    for (j in 1:J) {\n        log_lik[j] = normal_lpdf(y[j] | theta[j], sigma[j]);\n        y_hat[j] = normal_rng(theta[j], sigma[j]);\n    }\n}\n\"\"\"\n\nschools_data = Dict(\"J\" => J, \"y\" => y, \"sigma\" => σ)\nidata_stan = mktempdir() do path\n    stan_model = SampleModel(\"schools\", schools_code, path)\n    _ = stan_sample(\n        stan_model;\n        data=schools_data,\n        num_chains=nchains,\n        num_warmups=ndraws_warmup,\n        num_samples=ndraws,\n        seed=28983,\n        summary=false,\n    )\n    return StanSample.inferencedata(\n        stan_model;\n        posterior_predictive_var=:y_hat,\n        observed_data=(; y),\n        log_likelihood_var=:log_lik,\n        coords=(; school=schools),\n        dims=NamedTuple(\n            k => (:school,) for k in (:y, :sigma, :theta, :log_lik, :y_hat)\n        ),\n    )\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"[ Info: /tmp/jl_qZcpA8/schools.stan updated.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"<div>InferenceData<details>\n<summary>posterior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :theta Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n&#10;with metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:25.626\"</code></pre>\n</details>\n<details>\n<summary>posterior_predictive</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y_hat Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n&#10;with metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:24.595\"</code></pre>\n</details>\n<details>\n<summary>log_likelihood</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :log_lik Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n&#10;with metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:25.378\"</code></pre>\n</details>\n<details>\n<summary>sample_stats</summary>\n<pre><code>Dataset with dimensions: Dim{:draw}, Dim{:chain}\nand 7 layers:\n  :tree_depth      Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :energy          Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :diverging       Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :acceptance_rate Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :n_steps         Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :lp              Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size       Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n&#10;with metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:24.951\"</code></pre>\n</details>\n<details>\n<summary>observed_data</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:school} (8)\n&#10;with metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:25.789\"</code></pre>\n</details>\n</div>","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"plot_density(idata_stan; var_names=(:mu, :tau));","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"(Image: )","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Here is a plot showing where the Hamiltonian sampler had divergences:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"plot_pair(\n    idata_stan;\n    coords=Dict(:school => [\"Choate\", \"Deerfield\", \"Phillips Andover\"]),\n    divergences=true,\n);","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"(Image: )","category":"page"},{"location":"quickstart/#Environment","page":"Quickstart","title":"Environment","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using Pkg\nPkg.status()","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Status `~/work/ArviZ.jl/ArviZ.jl/docs/Project.toml`\n  [cbdf2221] AlgebraOfGraphics v0.6.16\n  [131c737c] ArviZ v0.9.0-DEV `~/work/ArviZ.jl/ArviZ.jl`\n  [13f3f980] CairoMakie v0.10.6\n  [992eb4ea] CondaPkg v0.2.18\n  [a93c6f00] DataFrames v1.5.0\n  [0703355e] DimensionalData v0.24.12\n  [31c24e10] Distributions v0.25.98\n  [e30172f5] Documenter v0.27.25\n⌅ [f6006082] EvoTrees v0.14.11\n  [7073ff75] IJulia v1.24.2\n  [c7f686f2] MCMCChains v6.0.3\n  [be115224] MCMCDiagnosticTools v0.3.4\n  [a7f614a8] MLJBase v0.21.11\n  [614be32b] MLJIteration v0.5.1\n  [438e738f] PyCall v1.96.1\n  [d330b81b] PyPlot v2.11.1\n  [754583d1] SampleChains v0.5.1\n  [c1514b29] StanSample v7.4.1\n⌅ [fce5fe82] Turing v0.24.4\n  [f43a241f] Downloads v1.6.0\n  [37e2e46d] LinearAlgebra\n  [10745b16] Statistics v1.9.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"versioninfo()","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Julia Version 1.9.2\nCommit e4ee485e909 (2023-07-05 09:39 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 2 × Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, skylake-avx512)\n  Threads: 3 on 2 virtual cores\nEnvironment:\n  JULIA_CMDSTAN_HOME = /home/runner/work/ArviZ.jl/ArviZ.jl/.cmdstan//cmdstan-2.25.0/\n  JULIA_IMAGE_THREADS = 1\n  JULIA_NUM_THREADS = 2","category":"page"},{"location":"api/data/#data-api","page":"Data","title":"Data","text":"","category":"section"},{"location":"api/data/","page":"Data","title":"Data","text":"Pages = [\"data.md\"]","category":"page"},{"location":"api/data/#Inference-library-converters","page":"Data","title":"Inference library converters","text":"","category":"section"},{"location":"api/data/","page":"Data","title":"Data","text":"from_mcmcchains\nfrom_samplechains","category":"page"},{"location":"api/data/#ArviZ.from_mcmcchains","page":"Data","title":"ArviZ.from_mcmcchains","text":"from_mcmcchains(posterior::MCMCChains.Chains; kwargs...) -> InferenceData\nfrom_mcmcchains(; kwargs...) -> InferenceData\nfrom_mcmcchains(\n    posterior::MCMCChains.Chains,\n    posterior_predictive,\n    predictions,\n    log_likelihood;\n    kwargs...\n) -> InferenceData\n\nConvert data in an MCMCChains.Chains format into an InferenceData.\n\nAny keyword argument below without an an explicitly annotated type above is allowed, so long as it can be passed to convert_to_inference_data.\n\nArguments\n\nposterior::MCMCChains.Chains: Draws from the posterior\n\nKeywords\n\nposterior_predictive::Any=nothing: Draws from the posterior predictive distribution or   name(s) of predictive variables in posterior\npredictions: Out-of-sample predictions for the posterior.\nprior: Draws from the prior\nprior_predictive: Draws from the prior predictive distribution or name(s) of predictive   variables in prior\nobserved_data: Observed data on which the posterior is conditional. It should only   contain data which is modeled as a random variable. Keys are parameter names and values.\nconstant_data: Model constants, data included in the model that are not modeled as   random variables. Keys are parameter names.\npredictions_constant_data: Constants relevant to the model predictions (i.e. new x   values in a linear regression).\nlog_likelihood: Pointwise log-likelihood for the data. It is recommended to use this   argument as a named tuple whose keys are observed variable names and whose values are log   likelihood arrays. Alternatively, provide the name of variable in posterior containing   log likelihoods.\nlibrary=MCMCChains: Name of library that generated the chains\ncoords: Map from named dimension to named indices\ndims: Map from variable name to names of its dimensions\neltypes: Map from variable names to eltypes. This is primarily used to assign discrete   eltypes to discrete variables that were stored in Chains as floats.\n\nReturns\n\nInferenceData: The data with groups corresponding to the provided data\n\n\n\n","category":"function"},{"location":"api/data/#ArviZ.from_samplechains","page":"Data","title":"ArviZ.from_samplechains","text":"from_samplechains(\n    posterior=nothing;\n    prior=nothing,\n    library=SampleChains,\n    kwargs...,\n) -> InferenceData\n\nConvert SampleChains samples to an InferenceData.\n\nEither posterior or prior may be a SampleChains.AbstractChain or SampleChains.MultiChain object.\n\nFor descriptions of remaining kwargs, see from_namedtuple.\n\n\n\n\n\n","category":"function"},{"location":"api/data/#IO-/-Conversion","page":"Data","title":"IO / Conversion","text":"","category":"section"},{"location":"api/data/","page":"Data","title":"Data","text":"from_netcdf\nto_netcdf","category":"page"},{"location":"api/data/#InferenceObjectsNetCDF.from_netcdf","page":"Data","title":"InferenceObjectsNetCDF.from_netcdf","text":"from_netcdf(path::AbstractString; kwargs...) -> InferenceData\n\nLoad an InferenceData from an unopened NetCDF file.\n\nRemaining kwargs are passed to NCDatasets.NCDataset. This method loads data eagerly. To instead load data lazily, pass an opened NCDataset to from_netcdf.\n\nExamples\n\njulia> idata = from_netcdf(\"centered_eight.nc\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data\n\nfrom_netcdf(ds::NCDatasets.NCDataset; load_mode) -> InferenceData\n\nLoad an InferenceData from an opened NetCDF file.\n\nload_mode defaults to :lazy, which avoids reading variables into memory. Operations on these arrays will be slow. load_mode can also be :eager, which copies all variables into memory. It is then safe to close ds. If load_mode is :lazy and ds is closed after constructing InferenceData, using the variable arrays will have undefined behavior.\n\nExamples\n\nHere is how we might load an InferenceData from an InferenceData lazily from a web-hosted NetCDF file.\n\njulia> using HTTP, NCDatasets\n\njulia> resp = HTTP.get(\"https://github.com/arviz-devs/arviz_example_data/blob/main/data/centered_eight.nc?raw=true\");\n\njulia> ds = NCDataset(\"centered_eight\", \"r\"; memory = resp.body);\n\njulia> idata = from_netcdf(ds)\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data\n\njulia> idata_copy = copy(idata); # disconnect from the loaded dataset\n\njulia> close(ds);\n\n\n\n\n\n","category":"function"},{"location":"api/data/#InferenceObjectsNetCDF.to_netcdf","page":"Data","title":"InferenceObjectsNetCDF.to_netcdf","text":"to_netcdf(data, dest::AbstractString; group::Symbol=:posterior, kwargs...)\nto_netcdf(data, dest::NCDatasets.NCDataset; group::Symbol=:posterior)\n\nWrite data to a NetCDF file.\n\ndata is any type that can be converted to an InferenceData using convert_to_inference_data. If not an InferenceData, then group specifies which group the data represents.\n\ndest specifies either the path to the NetCDF file or an opened NetCDF file. If dest is a path, remaining kwargs are passed to NCDatasets.NCDataset.\n\nExamples\n\njulia> using NCDatasets\n\njulia> idata = from_namedtuple((; x = randn(4, 100, 3), z = randn(4, 100)))\nInferenceData with groups:\n  > posterior\n\njulia> to_netcdf(idata, \"data.nc\")\n\"data.nc\"\n\n\n\n\n\n","category":"function"},{"location":"api/data/#Example-data","page":"Data","title":"Example data","text":"","category":"section"},{"location":"api/data/","page":"Data","title":"Data","text":"describe_example_data\nload_example_data","category":"page"},{"location":"api/data/#ArviZExampleData.describe_example_data","page":"Data","title":"ArviZExampleData.describe_example_data","text":"describe_example_data() -> String\n\nReturn a string containing descriptions of all available datasets.\n\nExamples\n\njulia> describe_example_data(\"radon\") |> println\nradon\n=====\n\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n\nThis example uses an EPA study of radon levels in houses in Minnesota to construct a model with a hierarchy over households within a county. The model includes estimates (gamma) for contextual effects of the uranium per household.\n\nSee Gelman and Hill (2006) for details on the example, or https://docs.pymc.io/notebooks/multilevel_modeling.html by Chris Fonnesbeck for details on this implementation.\n\nremote: http://ndownloader.figshare.com/files/24067472\n\n\n\n\n\n","category":"function"},{"location":"api/data/#ArviZExampleData.load_example_data","page":"Data","title":"ArviZExampleData.load_example_data","text":"load_example_data(name; kwargs...) -> InferenceObjects.InferenceData\nload_example_data() -> Dict{String,AbstractFileMetadata}\n\nLoad a local or remote pre-made dataset.\n\nkwargs are forwarded to InferenceObjects.from_netcdf.\n\nPass no parameters to get a Dict listing all available datasets.\n\nData files are handled by DataDeps.jl. A file is downloaded only when it is requested and then cached for future use.\n\nExamples\n\njulia> keys(load_example_data())\nKeySet for a OrderedCollections.OrderedDict{String, ArviZExampleData.AbstractFileMetadata} with 9 entries. Keys:\n  \"centered_eight\"\n  \"non_centered_eight\"\n  \"radon\"\n  \"rugby\"\n  \"regression1d\"\n  \"regression10d\"\n  \"classification1d\"\n  \"classification10d\"\n  \"glycan_torsion_angles\"\n\njulia> load_example_data(\"centered_eight\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > log_likelihood\n  > sample_stats\n  > prior\n  > prior_predictive\n  > observed_data\n  > constant_data\n\n\n\n\n\n","category":"function"},{"location":"api/diagnostics/#diagnostics-api","page":"Diagnostics","title":"Diagnostics","text":"","category":"section"},{"location":"api/diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"Pages = [\"diagnostics.md\"]","category":"page"},{"location":"api/diagnostics/#bfmi","page":"Diagnostics","title":"Bayesian fraction of missing information","text":"","category":"section"},{"location":"api/diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"MCMCDiagnosticTools.bfmi","category":"page"},{"location":"api/diagnostics/#MCMCDiagnosticTools.bfmi","page":"Diagnostics","title":"MCMCDiagnosticTools.bfmi","text":"bfmi(energy::AbstractVector{<:Real}) -> Real\nbfmi(energy::AbstractMatrix{<:Real}; dims::Int=1) -> AbstractVector{<:Real}\n\nCalculate the estimated Bayesian fraction of missing information (BFMI).\n\nWhen sampling with Hamiltonian Monte Carlo (HMC), BFMI quantifies how well momentum resampling matches the marginal energy distribution.\n\nThe current advice is that values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may change. A BFMI value below the threshold often indicates poor adaptation of sampling parameters or that the target distribution has heavy tails that were not well explored by the Markov chain.\n\nFor more information, see Section 6.1 of [Betancourt2018] or [Betancourt2016] for a complete account.\n\nenergy is either a vector of Hamiltonian energies of draws or a matrix of energies of draws for multiple chains. dims indicates the dimension in energy that contains the draws. The default dims=1 assumes energy has the shape draws or (draws, chains). If a different shape is provided, dims must be set accordingly.\n\nIf energy is a vector, a single BFMI value is returned. Otherwise, a vector of BFMI values for each chain is returned.\n\n[Betancourt2018]: Betancourt M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo. arXiv:1701.02434v2 [stat.ME]\n\n[Betancourt2016]: Betancourt M. (2016). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo. arXiv:1604.00695v1 [stat.ME]\n\n\n\n\n\n","category":"function"},{"location":"api/diagnostics/#ess_rhat","page":"Diagnostics","title":"Effective sample size and widehatR diagnostic","text":"","category":"section"},{"location":"api/diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"MCMCDiagnosticTools.ess\nMCMCDiagnosticTools.rhat\nMCMCDiagnosticTools.ess_rhat","category":"page"},{"location":"api/diagnostics/#MCMCDiagnosticTools.ess","page":"Diagnostics","title":"MCMCDiagnosticTools.ess","text":"ess(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind=:bulk,\n    relative::Bool=false,\n    autocov_method=AutocovMethod(),\n    split_chains::Int=2,\n    maxlag::Int=250,\n    kwargs...\n)\n\nEstimate the effective sample size (ESS) of the samples of shape (draws, [chains[, parameters...]]) with the autocov_method.\n\nOptionally, the kind of ESS estimate to be computed can be specified (see below). Some kinds accept additional kwargs.\n\nIf relative is true, the relative ESS is returned, i.e. ess / (draws * chains).\n\nsplit_chains indicates the number of chains each chain is split into. When split_chains > 1, then the diagnostics check for within-chain convergence. When d = mod(draws, split_chains) > 0, i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first d splits within each chain. There must be at least 3 draws in each chain after splitting.\n\nmaxlag indicates the maximum lag for which autocovariance is computed and must be greater than 0.\n\nFor a given estimand, it is recommended that the ESS is at least 100 * chains and that widehatR  101.[VehtariGelman2021]\n\nSee also: AutocovMethod, FFTAutocovMethod, BDAAutocovMethod, rhat, ess_rhat, mcse\n\nKinds of ESS estimates\n\nIf kind isa a Symbol, it may take one of the following values:\n\n:bulk: basic ESS computed on rank-normalized draws. This kind diagnoses poor convergence   in the bulk of the distribution due to trends or different locations of the chains.\n:tail: minimum of the quantile-ESS for the symmetric quantiles where   tail_prob=0.1 is the probability in the tails. This kind diagnoses poor convergence in   the tails of the distribution. If this kind is chosen, kwargs may contain a   tail_prob keyword.\n:basic: basic ESS, equivalent to specifying kind=Statistics.mean.\n\nnote: Note\nWhile Bulk-ESS is conceptually related to basic ESS, it is well-defined even if the chains do not have finite variance.[VehtariGelman2021] For each parameter, rank-normalization proceeds by first ranking the inputs using \"tied ranking\" and then transforming the ranks to normal quantiles so that the result is standard normally distributed. This transform is monotonic.\n\nOtherwise, kind specifies one of the following estimators, whose ESS is to be estimated:\n\nStatistics.mean\nStatistics.median\nStatistics.std\nStatsBase.mad\nBase.Fix2(Statistics.quantile, p::Real)\n\n[VehtariGelman2021]: Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved widehat R for assessing convergence of MCMC. Bayesian Analysis. doi: 10.1214/20-BA1221 arXiv: 1903.08008\n\n\n\n\n\ness(data::InferenceData; kwargs...) -> Dataset\ness(data::Dataset; kwargs...) -> Dataset\n\nCalculate the effective sample size (ESS) for each parameter in the data.\n\n\n\n\n\n","category":"function"},{"location":"api/diagnostics/#MCMCDiagnosticTools.rhat","page":"Diagnostics","title":"MCMCDiagnosticTools.rhat","text":"rhat(samples::AbstractArray{Union{Real,Missing}}; kind::Symbol=:rank, split_chains=2)\n\nCompute the widehatR diagnostics for each parameter in samples of shape (draws, [chains[, parameters...]]).[VehtariGelman2021]\n\nkind indicates the kind of widehatR to compute (see below).\n\nsplit_chains indicates the number of chains each chain is split into. When split_chains > 1, then the diagnostics check for within-chain convergence. When d = mod(draws, split_chains) > 0, i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first d splits within each chain.\n\nSee also ess, ess_rhat, rstar\n\nKinds of widehatR\n\nThe following kinds are supported:\n\n:rank: maximum of widehatR with kind=:bulk and kind=:tail.\n:bulk: basic widehatR computed on rank-normalized draws. This kind diagnoses   poor convergence in the bulk of the distribution due to trends or different locations of   the chains.\n:tail: widehatR computed on draws folded around the median and then   rank-normalized. This kind diagnoses poor convergence in the tails of the distribution   due to different scales of the chains.\n:basic: Classic widehatR.\n\n[VehtariGelman2021]: Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved widehat R for assessing convergence of MCMC. Bayesian Analysis. doi: 10.1214/20-BA1221 arXiv: 1903.08008\n\n\n\n\n\nrhat(data::InferenceData; kwargs...) -> Dataset\nrhat(data::Dataset; kwargs...) -> Dataset\n\nCalculate the widehatR diagnostic for each parameter in the data.\n\n\n\n\n\n","category":"function"},{"location":"api/diagnostics/#MCMCDiagnosticTools.ess_rhat","page":"Diagnostics","title":"MCMCDiagnosticTools.ess_rhat","text":"ess_rhat(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind::Symbol=:rank,\n    kwargs...,\n) -> NamedTuple{(:ess, :rhat)}\n\nEstimate the effective sample size and widehatR of the samples of shape (draws, [chains[, parameters...]]).\n\nWhen both ESS and widehatR are needed, this method is often more efficient than calling ess and rhat separately.\n\nSee rhat for a description of supported kinds and ess for a description of kwargs.\n\n\n\n\n\ness_rhat(data::InferenceData; kwargs...) -> Dataset\ness_rhat(data::Dataset; kwargs...) -> Dataset\n\nCalculate the effective sample size (ESS) and widehatR diagnostic for each parameter in the data.\n\n\n\n\n\n","category":"function"},{"location":"api/diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"The following autocovariance methods are supported:","category":"page"},{"location":"api/diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"MCMCDiagnosticTools.AutocovMethod\nMCMCDiagnosticTools.FFTAutocovMethod\nMCMCDiagnosticTools.BDAAutocovMethod","category":"page"},{"location":"api/diagnostics/#MCMCDiagnosticTools.AutocovMethod","page":"Diagnostics","title":"MCMCDiagnosticTools.AutocovMethod","text":"AutocovMethod <: AbstractAutocovMethod\n\nThe AutocovMethod uses a standard algorithm for estimating the mean autocovariance of MCMC chains.\n\nIt is is based on the discussion by [VehtariGelman2021] and uses the biased estimator of the autocovariance, as discussed by [Geyer1992].\n\n[VehtariGelman2021]: Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved widehat R for assessing convergence of MCMC. Bayesian Analysis. doi: 10.1214/20-BA1221 arXiv: 1903.08008\n\n[Geyer1992]: Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483.\n\n\n\n\n\n","category":"type"},{"location":"api/diagnostics/#MCMCDiagnosticTools.FFTAutocovMethod","page":"Diagnostics","title":"MCMCDiagnosticTools.FFTAutocovMethod","text":"FFTAutocovMethod <: AbstractAutocovMethod\n\nThe FFTAutocovMethod uses a standard algorithm for estimating the mean autocovariance of MCMC chains.\n\nThe algorithm is the same as the one of AutocovMethod but this method uses fast Fourier transforms (FFTs) for estimating the autocorrelation.\n\ninfo: Info\nTo be able to use this method, you have to load a package that implements the AbstractFFTs.jl interface such as FFTW.jl or FastTransforms.jl.\n\n\n\n\n\n","category":"type"},{"location":"api/diagnostics/#MCMCDiagnosticTools.BDAAutocovMethod","page":"Diagnostics","title":"MCMCDiagnosticTools.BDAAutocovMethod","text":"BDAAutocovMethod <: AbstractAutocovMethod\n\nThe BDAAutocovMethod uses a standard algorithm for estimating the mean autocovariance of MCMC chains.\n\nIt is is based on the discussion by [VehtariGelman2021]. and uses the variogram estimator of the autocorrelation function discussed by [BDA3].\n\n[VehtariGelman2021]: Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved widehat R for assessing convergence of MCMC. Bayesian Analysis. doi: 10.1214/20-BA1221 arXiv: 1903.08008\n\n[BDA3]: Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. CRC press.\n\n\n\n\n\n","category":"type"},{"location":"api/diagnostics/#mcse","page":"Diagnostics","title":"Monte Carlo standard error","text":"","category":"section"},{"location":"api/diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"MCMCDiagnosticTools.mcse","category":"page"},{"location":"api/diagnostics/#MCMCDiagnosticTools.mcse","page":"Diagnostics","title":"MCMCDiagnosticTools.mcse","text":"mcse(samples::AbstractArray{<:Union{Missing,Real}}; kind=Statistics.mean, kwargs...)\n\nEstimate the Monte Carlo standard errors (MCSE) of the estimator kind applied to samples of shape (draws, [chains[, parameters...]]).\n\nSee also: ess\n\nKinds of MCSE estimates\n\nThe estimator whose MCSE should be estimated is specified with kind. kind must accept a vector of the same eltype as samples and return a real estimate.\n\nFor the following estimators, the effective sample size ess and an estimate of the asymptotic variance are used to compute the MCSE, and kwargs are forwarded to ess:\n\nStatistics.mean\nStatistics.median\nStatistics.std\nBase.Fix2(Statistics.quantile, p::Real)\n\nFor other estimators, the subsampling bootstrap method (SBM)[FlegalJones2011][Flegal2012] is used as a fallback, and the only accepted kwargs are batch_size, which indicates the size of the overlapping batches used to estimate the MCSE, defaulting to floor(Int, sqrt(draws * chains)). Note that SBM tends to underestimate the MCSE, especially for highly autocorrelated chains. One should verify that autocorrelation is low by checking the bulk- and tail-ESS values.\n\n[FlegalJones2011]: Flegal JM, Jones GL. (2011) Implementing MCMC: estimating with confidence.                 Handbook of Markov Chain Monte Carlo. pp. 175-97.                 pdf\n\n[Flegal2012]: Flegal JM. (2012) Applicability of subsampling bootstrap methods in Markov chain Monte Carlo.            Monte Carlo and Quasi-Monte Carlo Methods 2010. pp. 363-72.            doi: 10.1007/978-3-642-27440-4_18\n\n\n\n\n\ness(data::InferenceData; kwargs...) -> Dataset\ness(data::Dataset; kwargs...) -> Dataset\n\nCalculate the Monte Carlo standard error (MCSE) for each parameter in the data.\n\n\n\n\n\n","category":"function"},{"location":"api/diagnostics/#rstar","page":"Diagnostics","title":"R^* diagnostic","text":"","category":"section"},{"location":"api/diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"MCMCDiagnosticTools.rstar","category":"page"},{"location":"api/diagnostics/#MCMCDiagnosticTools.rstar","page":"Diagnostics","title":"MCMCDiagnosticTools.rstar","text":"rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples,\n    chain_indices::AbstractVector{Int};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n)\n\nCompute the R^* convergence statistic of the table samples with the classifier.\n\nsamples must be either an AbstractMatrix, an AbstractVector, or a table (i.e. implements the Tables.jl interface) whose rows are draws and whose columns are parameters.\n\nchain_indices indicates the chain ids of each row of samples.\n\nThis method supports ragged chains, i.e. chains of nonequal lengths.\n\n\n\n\n\nrstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples::AbstractArray{<:Real};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n)\n\nCompute the R^* convergence statistic of the samples with the classifier.\n\nsamples is an array of draws with the shape (draws, [chains[, parameters...]]).`\n\nThis implementation is an adaption of algorithms 1 and 2 described by Lambert and Vehtari.\n\nThe classifier has to be a supervised classifier of the MLJ framework (see the MLJ documentation for a list of supported models). It is trained with a subset of the samples from each chain. Each chain is split into split_chains separate chains to additionally check for within-chain convergence. The training of the classifier can be inspected by adjusting the verbosity level.\n\nIf the classifier is deterministic, i.e., if it predicts a class, the value of the R^* statistic is returned (algorithm 1). If the classifier is probabilistic, i.e., if it outputs probabilities of classes, the scaled Poisson-binomial distribution of the R^* statistic is returned (algorithm 2).\n\nnote: Note\nThe correctness of the statistic depends on the convergence of the classifier used internally in the statistic.\n\nExamples\n\njulia> using MLJBase, MLJIteration, EvoTrees, Statistics\n\njulia> samples = fill(4.0, 100, 3, 2);\n\nOne can compute the distribution of the R^* statistic (algorithm 2) with a probabilistic classifier. For instance, we can use a gradient-boosted trees model with nrounds = 100 sequentially stacked trees and learning rate eta = 0.05:\n\njulia> model = EvoTreeClassifier(; nrounds=100, eta=0.05);\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0\n\nNote, however, that it is recommended to determine nrounds based on early-stopping. With the MLJ framework, this can be achieved in the following way (see the MLJ documentation for additional explanations):\n\njulia> model = IteratedModel(;\n           model=EvoTreeClassifier(; eta=0.05),\n           iteration_parameter=:nrounds,\n           resampling=Holdout(),\n           measures=log_loss,\n           controls=[Step(5), Patience(2), NumberLimit(100)],\n           retrain=true,\n       );\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0\n\nFor deterministic classifiers, a single R^* statistic (algorithm 1) is returned. Deterministic classifiers can also be derived from probabilistic classifiers by e.g. predicting the mode. In MLJ this corresponds to a pipeline of models.\n\njulia> evotree_deterministic = Pipeline(model; operation=predict_mode);\n\njulia> value = rstar(evotree_deterministic, samples);\n\njulia> round(value; digits=2)\n1.0\n\nReferences\n\nLambert, B., & Vehtari, A. (2020). R^*: A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers.\n\n\n\n\n\nrstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    data::Union{InferenceData,Dataset};\n    kwargs...,\n)\n\nCalculate the R^* diagnostic for the data.\n\n\n\n\n\n","category":"function"},{"location":"#arvizjl","page":"Home","title":"ArviZ.jl: Exploratory analysis of Bayesian models in Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ArviZ.jl is a Julia package for exploratory analysis of Bayesian models. It is codeveloped with the Python package ArviZ and in some cases temporarily relies on the Python package for functionality.","category":"page"},{"location":"#installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install ArviZ.jl, we first need to install Python ArviZ. To use with the default Python environment, first install Python ArviZ. From the Julia REPL, type ] to enter the Pkg REPL mode and run","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add ArviZ","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install ArviZ.jl with its Python dependencies in Julia's private conda environment, in the console run","category":"page"},{"location":"","page":"Home","title":"Home","text":"PYTHON=\"\" julia -e 'using Pkg; Pkg.add(\"PyCall\"); Pkg.build(\"PyCall\"); Pkg.add(\"ArviZ\")'","category":"page"},{"location":"","page":"Home","title":"Home","text":"For specifying other Python versions, see the PyCall documentation.","category":"page"},{"location":"#usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See the Quickstart for example usage and the API Overview for description of functions.","category":"page"},{"location":"#extendingarviz","page":"Home","title":"Extending ArviZ.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use a custom data type with ArviZ.jl, simply overload convert_to_inference_data to convert your input(s) to an InferenceData.","category":"page"},{"location":"#knownissues","page":"Home","title":"Known Issues","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ArviZ.jl uses PyCall.jl to wrap ArviZ. At the moment, Julia segfaults if Numba is imported, which ArviZ does if it is available. For the moment, the workaround is to specify a Python version that doesn't have Numba installed. See this issue for more details.","category":"page"}]
}
