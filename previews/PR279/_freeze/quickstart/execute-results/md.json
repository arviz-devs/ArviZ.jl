{
  "hash": "c3d3b41487d0e709ad243516d2baf63b",
  "result": {
    "markdown": "---\ntitle: \"ArviZ.jl Quickstart\"\n---\n\n## Set-up\n\nHere we add the necessary packages for this notebook and load a few we will use throughout.\n\n\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing ArviZ, Distributions, LinearAlgebra, PyPlot, Random, StanSample, Turing\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/runner/.julia/conda/3/x86_64/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\n# ArviZ ships with style sheets!\nArviZ.use_style(\"arviz-darkgrid\")\n```\n:::\n\n\n## Get started with plotting\n\nArviZ.jl is designed to be used with libraries like [Stan](https://github.com/StanJulia/Stan.jl), [Turing.jl](https://turinglang.org), and [Soss.jl](https://github.com/cscherrer/Soss.jl) but works fine with raw arrays.\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nrng1 = Random.MersenneTwister(37772);\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nplot_posterior(randn(rng1, 100_000));\n```\n\n::: {.cell-output .cell-output-display}\n![](quickstart_files/figure-commonmark/cell-6-output-1.png){}\n:::\n:::\n\n\nPlotting a dictionary of arrays, ArviZ.jl will interpret each key as the name of a different random variable.\nEach row of an array is treated as an independent series of draws from the variable, called a _chain_.\nBelow, we have 10 chains of 50 draws each for four different distributions.\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\ns = (50, 10)\nplot_forest((\n    normal=randn(rng1, s),\n    gumbel=rand(rng1, Gumbel(), s),\n    student_t=rand(rng1, TDist(6), s),\n    exponential=rand(rng1, Exponential(), s),\n),);\n```\n\n::: {.cell-output .cell-output-display}\n![](quickstart_files/figure-commonmark/cell-7-output-1.png){}\n:::\n:::\n\n\n## Plotting with MCMCChains.jl's `Chains` objects produced by Turing.jl\n\nArviZ is designed to work well with high dimensional, labelled data.\nConsider the [eight schools model](https://statmodeling.stat.columbia.edu/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/), which roughly tries to measure the effectiveness of SAT classes at eight different schools.\nTo show off ArviZ's labelling, I give the schools the names of [a different eight schools](https://en.wikipedia.org/wiki/Eight_Schools_Association).\n\nThis model is small enough to write down, is hierarchical, and uses labelling.\nAdditionally, a centered parameterization causes [divergences](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html) (which are interesting for illustration).\n\nFirst we create our data and set some sampling parameters.\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\nJ = 8\ny = [28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0]\nσ = [15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0]\nschools = [\n    \"Choate\",\n    \"Deerfield\",\n    \"Phillips Andover\",\n    \"Phillips Exeter\",\n    \"Hotchkiss\",\n    \"Lawrenceville\",\n    \"St. Paul's\",\n    \"Mt. Hermon\",\n]\nndraws = 1_000\nndraws_warmup = 1_000\nnchains = 4;\n```\n:::\n\n\nNow we write and run the model using Turing:\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\nTuring.@model function model_turing(y, σ, J=length(y))\n    μ ~ Normal(0, 5)\n    τ ~ truncated(Cauchy(0, 5), 0, Inf)\n    θ ~ filldist(Normal(μ, τ), J)\n    for i in 1:J\n        y[i] ~ Normal(θ[i], σ[i])\n    end\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nmodel_turing (generic function with 3 methods)\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nrng2 = Random.MersenneTwister(16653);\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nparam_mod_turing = model_turing(y, σ)\nsampler = NUTS(ndraws_warmup, 0.8)\n\nturing_chns = Turing.sample(\n    rng2, model_turing(y, σ), sampler, MCMCThreads(), ndraws, nchains\n);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Info: Found initial step size\n└   ϵ = 1.6\n┌ Info: Found initial step size\n└   ϵ = 0.8\n┌ Info: Found initial step size\n└   ϵ = 0.4\n┌ Info: Found initial step size\n└   ϵ = 0.8\n```\n:::\n:::\n\n\nMost ArviZ functions work fine with `Chains` objects from Turing:\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\nplot_autocorr(turing_chns; var_names=(:μ, :τ));\n```\n\n::: {.cell-output .cell-output-display}\n![](quickstart_files/figure-commonmark/cell-12-output-1.png){}\n:::\n:::\n\n\n### Convert to `InferenceData`\n\nFor much more powerful querying, analysis and plotting, we can use built-in ArviZ utilities to convert `Chains` objects to multidimensional data structures with named dimensions and indices.\nNote that for such dimensions, the information is not contained in `Chains`, so we need to provide it.\n\nArviZ is built to work with [`InferenceData`](@ref), and the more *groups* it has access to, the more powerful analyses it can perform.\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\nidata_turing_post = from_mcmcchains(\n    turing_chns;\n    coords=(; school=schools),\n    dims=NamedTuple(k => (:school,) for k in (:y, :σ, :θ)),\n    library=\"Turing\",\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>InferenceData<details>\n<summary>posterior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :μ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :τ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :θ Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n\nwith metadata Dict{String, Any} with 2 entries:\n  \"created_at\" => \"2023-07-06T22:38:11.324\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>sample_stats</summary>\n<pre><code>Dataset with dimensions: Dim{:draw}, Dim{:chain}\nand 12 layers:\n  :energy           Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :n_steps          Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :diverging        Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :max_energy_error Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :energy_error     Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :is_accept        Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :log_density      Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :tree_depth       Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size        Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :acceptance_rate  Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :lp               Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size_nom    Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n\nwith metadata Dict{String, Any} with 2 entries:\n  \"created_at\" => \"2023-07-06T22:38:11.208\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n</div>\n```\n:::\n:::\n\n\nEach group is a [`Dataset`](@ref), a `DimensionalData.AbstractDimStack` that can be used identically to a [`DimensionalData.Dimstack`](https://rafaqz.github.io/DimensionalData.jl/stable/api/#DimensionalData.DimStack).\nWe can view a summary of the dataset.\n\n::: {.cell execution_count=13}\n``` {.julia .cell-code}\nidata_turing_post.posterior\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nDataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :μ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :τ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :θ Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n\nwith metadata Dict{String, Any} with 2 entries:\n  \"created_at\"        => \"2023-07-06T22:38:11.324\"\n  \"inference_library\" => \"Turing\"\n```\n:::\n:::\n\n\nHere is a plot of the trace. Note the intelligent labels.\n\n::: {.cell execution_count=14}\n``` {.julia .cell-code}\nplot_trace(idata_turing_post);\n```\n\n::: {.cell-output .cell-output-display}\n![](quickstart_files/figure-commonmark/cell-15-output-1.png){}\n:::\n:::\n\n\nWe can also generate summary stats...\n\n::: {.cell execution_count=15}\n``` {.julia .cell-code}\nsummarystats(idata_turing_post)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n\\begin{tabular}{r|ccccccccc}\n\t& variable & mean & sd & hdi\\_3\\% & hdi\\_97\\% & mcse\\_mean & mcse\\_sd & ess\\_bulk & \\\\\n\t\\hline\n\t& String & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n\t\\hline\n\t1 & μ & 4.174 & 3.329 & -1.892 & 10.232 & 0.12 & 0.085 & 777.0 & $\\dots$ \\\\\n\t2 & τ & 4.094 & 3.201 & 0.572 & 9.281 & 0.149 & 0.106 & 304.0 & $\\dots$ \\\\\n\t3 & θ[Choate] & 6.32 & 5.839 & -4.134 & 17.191 & 0.175 & 0.124 & 1090.0 & $\\dots$ \\\\\n\t4 & θ[Deerfield] & 4.78 & 4.799 & -4.265 & 14.092 & 0.126 & 0.094 & 1359.0 & $\\dots$ \\\\\n\t5 & θ[Phillips Andover] & 3.622 & 5.593 & -7.236 & 13.951 & 0.141 & 0.105 & 1435.0 & $\\dots$ \\\\\n\t6 & θ[Phillips Exeter] & 4.728 & 5.032 & -5.11 & 14.005 & 0.129 & 0.093 & 1404.0 & $\\dots$ \\\\\n\t7 & θ[Hotchkiss] & 3.366 & 4.86 & -5.999 & 12.05 & 0.138 & 0.101 & 1199.0 & $\\dots$ \\\\\n\t8 & θ[Lawrenceville] & 3.803 & 4.963 & -5.521 & 13.226 & 0.136 & 0.096 & 1282.0 & $\\dots$ \\\\\n\t9 & θ[St. Paul's] & 6.429 & 5.342 & -2.704 & 17.22 & 0.156 & 0.111 & 1120.0 & $\\dots$ \\\\\n\t10 & θ[Mt. Hermon] & 4.664 & 5.567 & -5.24 & 15.691 & 0.157 & 0.111 & 1190.0 & $\\dots$ \\\\\n\\end{tabular}\n\n:::\n:::\n\n\n...and examine the energy distribution of the Hamiltonian sampler.\n\n::: {.cell execution_count=16}\n``` {.julia .cell-code}\nplot_energy(idata_turing_post);\n```\n\n::: {.cell-output .cell-output-display}\n![](quickstart_files/figure-commonmark/cell-17-output-1.png){}\n:::\n:::\n\n\n### Additional information in Turing.jl\n\nWith a few more steps, we can use Turing to compute additional useful groups to add to the `InferenceData`.\n\nTo sample from the prior, one simply calls `sample` but with the `Prior` sampler:\n\n::: {.cell execution_count=17}\n``` {.julia .cell-code}\nprior = Turing.sample(rng2, param_mod_turing, Prior(), ndraws);\n```\n:::\n\n\nTo draw from the prior and posterior predictive distributions we can instantiate a \"predictive model\", i.e. a Turing model but with the observations set to `missing`, and then calling `predict` on the predictive model and the previously drawn samples:\n\n::: {.cell execution_count=18}\n``` {.julia .cell-code}\n# Instantiate the predictive model\nparam_mod_predict = model_turing(similar(y, Missing), σ)\n# and then sample!\nprior_predictive = Turing.predict(rng2, param_mod_predict, prior)\nposterior_predictive = Turing.predict(rng2, param_mod_predict, turing_chns);\n```\n:::\n\n\nAnd to extract the pointwise log-likelihoods, which is useful if you want to compute metrics such as [`loo`](@ref),\n\n::: {.cell execution_count=19}\n``` {.julia .cell-code}\nlog_likelihood = let\n    log_likelihood = Turing.pointwise_loglikelihoods(\n        param_mod_turing, MCMCChains.get_sections(turing_chns, :parameters)\n    )\n    # Ensure the ordering of the loglikelihoods matches the ordering of `posterior_predictive`\n    ynames = string.(keys(posterior_predictive))\n    log_likelihood_y = getindex.(Ref(log_likelihood), ynames)\n    (; y=cat(log_likelihood_y...; dims=3))\nend;\n```\n:::\n\n\nThis can then be included in the [`from_mcmcchains`](@ref) call from above:\n\n::: {.cell execution_count=20}\n``` {.julia .cell-code}\nidata_turing = from_mcmcchains(\n    turing_chns;\n    posterior_predictive,\n    log_likelihood,\n    prior,\n    prior_predictive,\n    observed_data=(; y),\n    coords=(; school=schools),\n    dims=NamedTuple(k => (:school,) for k in (:y, :σ, :θ)),\n    library=Turing,\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```{=html}\n<div>InferenceData<details>\n<summary>posterior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :μ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :τ Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :θ Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:38.105\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>posterior_predictive</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:37.15\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>log_likelihood</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:37.884\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>sample_stats</summary>\n<pre><code>Dataset with dimensions: Dim{:draw}, Dim{:chain}\nand 12 layers:\n  :energy           Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :n_steps          Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :diverging        Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :max_energy_error Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :energy_error     Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :is_accept        Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :log_density      Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :tree_depth       Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size        Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :acceptance_rate  Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :lp               Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size_nom    Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:38.104\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>prior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :μ Float64 dims: Dim{:draw}, Dim{:chain} (1000×1)\n  :τ Float64 dims: Dim{:draw}, Dim{:chain} (1000×1)\n  :θ Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×1×8)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:39.089\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>prior_predictive</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×1×8)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:38.755\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>sample_stats_prior</summary>\n<pre><code>Dataset with dimensions: Dim{:draw}, Dim{:chain}\nand 1 layer:\n  :lp Float64 dims: Dim{:draw}, Dim{:chain} (1000×1)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:38.943\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n<details>\n<summary>observed_data</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" => \"2023-07-06T22:38:39.44\"\n  \"inference_library_version\" => \"0.24.4\"\n  \"inference_library\" => \"Turing\"</code></pre>\n</details>\n</div>\n```\n:::\n:::\n\n\nThen we can for example compute the expected *leave-one-out (LOO)* predictive density, which is an estimate of the out-of-distribution predictive fit of the model:\n\n::: {.cell execution_count=21}\n``` {.julia .cell-code}\nloo(idata_turing) # higher ELPD is better\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nPSISLOOResult with estimates\n       Estimate    SE \n elpd       -31   1.5\n    p       0.9  0.35\n\nand PSISResult with 1000 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  5 (62.5%)  723\n  (0.5, 0.7]  okay  3 (37.5%)  390\n```\n:::\n:::\n\n\nIf the model is well-calibrated, i.e. it replicates the true generative process well, the CDF of the pointwise LOO values should be similarly distributed to a uniform distribution.\nThis can be inspected visually:\n\n::: {.cell execution_count=22}\n``` {.julia .cell-code}\nplot_loo_pit(idata_turing; y=:y, ecdf=true);\n```\n\n::: {.cell-output .cell-output-display}\n![](quickstart_files/figure-commonmark/cell-23-output-1.png){}\n:::\n:::\n\n\n## Plotting with Stan.jl outputs\n\nStanSample.jl comes with built-in support for producing `InferenceData` outputs.\n\nHere is the same centered eight schools model in Stan:\n\n::: {.cell execution_count=23}\n``` {.julia .cell-code}\nschools_code = \"\"\"\ndata {\n    int<lower=0> J;\n    real y[J];\n    real<lower=0> sigma[J];\n}\n\nparameters {\n    real mu;\n    real<lower=0> tau;\n    real theta[J];\n}\n\nmodel {\n    mu ~ normal(0, 5);\n    tau ~ cauchy(0, 5);\n    theta ~ normal(mu, tau);\n    y ~ normal(theta, sigma);\n}\n\ngenerated quantities {\n    vector[J] log_lik;\n    vector[J] y_hat;\n    for (j in 1:J) {\n        log_lik[j] = normal_lpdf(y[j] | theta[j], sigma[j]);\n        y_hat[j] = normal_rng(theta[j], sigma[j]);\n    }\n}\n\"\"\"\n\nschools_data = Dict(\"J\" => J, \"y\" => y, \"sigma\" => σ)\nidata_stan = mktempdir() do path\n    stan_model = SampleModel(\"schools\", schools_code, path)\n    _ = stan_sample(\n        stan_model;\n        data=schools_data,\n        num_chains=nchains,\n        num_warmups=ndraws_warmup,\n        num_samples=ndraws,\n        seed=28983,\n        summary=false,\n    )\n    return StanSample.inferencedata(\n        stan_model;\n        posterior_predictive_var=:y_hat,\n        observed_data=(; y),\n        log_likelihood_var=:log_lik,\n        coords=(; school=schools),\n        dims=NamedTuple(\n            k => (:school,) for k in (:y, :sigma, :theta, :log_lik, :y_hat)\n        ),\n    )\nend\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[ Info: /tmp/jl_qZcpA8/schools.stan updated.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>InferenceData<details>\n<summary>posterior</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :theta Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:25.626\"</code></pre>\n</details>\n<details>\n<summary>posterior_predictive</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y_hat Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:24.595\"</code></pre>\n</details>\n<details>\n<summary>log_likelihood</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:draw},\n  Dim{:chain},\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :log_lik Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:school} (1000×4×8)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:25.378\"</code></pre>\n</details>\n<details>\n<summary>sample_stats</summary>\n<pre><code>Dataset with dimensions: Dim{:draw}, Dim{:chain}\nand 7 layers:\n  :tree_depth      Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :energy          Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :diverging       Bool dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :acceptance_rate Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :n_steps         Int64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :lp              Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n  :step_size       Float64 dims: Dim{:draw}, Dim{:chain} (1000×4)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:24.951\"</code></pre>\n</details>\n<details>\n<summary>observed_data</summary>\n<pre><code>Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :y Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-07-06T22:39:25.789\"</code></pre>\n</details>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=24}\n``` {.julia .cell-code}\nplot_density(idata_stan; var_names=(:mu, :tau));\n```\n\n::: {.cell-output .cell-output-display}\n![](quickstart_files/figure-commonmark/cell-25-output-1.png){}\n:::\n:::\n\n\nHere is a plot showing where the Hamiltonian sampler had divergences:\n\n::: {.cell execution_count=25}\n``` {.julia .cell-code}\nplot_pair(\n    idata_stan;\n    coords=Dict(:school => [\"Choate\", \"Deerfield\", \"Phillips Andover\"]),\n    divergences=true,\n);\n```\n\n::: {.cell-output .cell-output-display}\n![](quickstart_files/figure-commonmark/cell-26-output-1.png){}\n:::\n:::\n\n\n## Environment\n\n::: {.cell execution_count=26}\n``` {.julia .cell-code}\nusing Pkg\nPkg.status()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStatus `~/work/ArviZ.jl/ArviZ.jl/docs/Project.toml`\n  [cbdf2221] AlgebraOfGraphics v0.6.16\n  [131c737c] ArviZ v0.9.0-DEV `~/work/ArviZ.jl/ArviZ.jl`\n  [13f3f980] CairoMakie v0.10.6\n  [992eb4ea] CondaPkg v0.2.18\n  [a93c6f00] DataFrames v1.5.0\n  [0703355e] DimensionalData v0.24.12\n  [31c24e10] Distributions v0.25.98\n  [e30172f5] Documenter v0.27.25\n⌅ [f6006082] EvoTrees v0.14.11\n  [7073ff75] IJulia v1.24.2\n  [c7f686f2] MCMCChains v6.0.3\n  [be115224] MCMCDiagnosticTools v0.3.4\n  [a7f614a8] MLJBase v0.21.11\n  [614be32b] MLJIteration v0.5.1\n  [438e738f] PyCall v1.96.1\n  [d330b81b] PyPlot v2.11.1\n  [754583d1] SampleChains v0.5.1\n  [c1514b29] StanSample v7.4.1\n⌅ [fce5fe82] Turing v0.24.4\n  [f43a241f] Downloads v1.6.0\n  [37e2e46d] LinearAlgebra\n  [10745b16] Statistics v1.9.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n```\n:::\n:::\n\n\n::: {.cell execution_count=27}\n``` {.julia .cell-code}\nversioninfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJulia Version 1.9.2\nCommit e4ee485e909 (2023-07-05 09:39 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 2 × Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, skylake-avx512)\n  Threads: 3 on 2 virtual cores\nEnvironment:\n  JULIA_CMDSTAN_HOME = /home/runner/work/ArviZ.jl/ArviZ.jl/.cmdstan//cmdstan-2.25.0/\n  JULIA_IMAGE_THREADS = 1\n  JULIA_NUM_THREADS = 2\n```\n:::\n:::\n\n\n",
    "supporting": [
      "quickstart_files"
    ],
    "filters": []
  }
}